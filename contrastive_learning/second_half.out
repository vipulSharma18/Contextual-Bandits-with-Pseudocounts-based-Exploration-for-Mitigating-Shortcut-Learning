## SLURM PROLOG ###############################################################
##    Job ID : 1988458
##  Job Name : second_half
##  Nodelist : gpu1402
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning
##   Job Started : Sat May 11 12:10:15 AM EDT 2024
###############################################################################
==========================================================================
Running experiment for setting 0.8_1
==========================================================================
Running for seed 1 of experiment 0.8_1
wandb: Currently logged in as: vipul. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_001025-a344n2pe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-rain-310
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/a344n2pe
/users/vsharm44/.conda/envs/dl_project/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Batch 0, train loss:8.552826881408691, Elapsed time for epoch : 0.017428823312123618
Epoch 0, Batch 10, train loss:3.12107515335083, Elapsed time for epoch : 0.1426769534746806
Epoch 0, Batch 20, train loss:2.6773674488067627, Elapsed time for epoch : 0.26703025499979655
Epoch 0, Batch 30, train loss:2.4125542640686035, Elapsed time for epoch : 0.39000725746154785
Epoch 0, Batch 40, train loss:2.149223804473877, Elapsed time for epoch : 0.5142092148462931
Epoch 0, Batch 50, train loss:2.0407238006591797, Elapsed time for epoch : 0.6352184891700745
Epoch 0, Batch 60, train loss:1.9001915454864502, Elapsed time for epoch : 0.7565159440040589
Epoch 0, Batch 70, train loss:1.9667513370513916, Elapsed time for epoch : 0.87915514310201
Epoch 0, Batch 80, train loss:1.765973687171936, Elapsed time for epoch : 1.001383090019226
Epoch 0, Batch 90, train loss:1.7038229703903198, Elapsed time for epoch : 1.1230950395266215
Epoch 0, Batch 100, train loss:1.709254503250122, Elapsed time for epoch : 1.2450815161069235
Epoch 0, Batch 110, train loss:1.5245072841644287, Elapsed time for epoch : 1.367836598555247
Batch 0, val loss:3.21181321144104
Batch 10, val loss:5.64308500289917
Batch 20, val loss:4.529299259185791
Batch 30, val loss:5.170571327209473
Epoch 0, Train Loss:3.1661625271258145, Val loss:3.3339531355433993
Epoch 1, Batch 0, train loss:1.6201677322387695, Elapsed time for epoch : 0.011516765753428141
Epoch 1, Batch 10, train loss:1.554982304573059, Elapsed time for epoch : 0.12658057610193887
Epoch 1, Batch 20, train loss:1.4135915040969849, Elapsed time for epoch : 0.2419490655263265
Epoch 1, Batch 30, train loss:1.3944355249404907, Elapsed time for epoch : 0.3576825181643168
Epoch 1, Batch 40, train loss:1.4157421588897705, Elapsed time for epoch : 0.4727992018063863
Epoch 1, Batch 50, train loss:1.4690449237823486, Elapsed time for epoch : 0.5886775890986125
Epoch 1, Batch 60, train loss:1.3224468231201172, Elapsed time for epoch : 0.7041111389795939
Epoch 1, Batch 70, train loss:1.3589171171188354, Elapsed time for epoch : 0.8203332185745239
Epoch 1, Batch 80, train loss:1.2880085706710815, Elapsed time for epoch : 0.9363638957341512
Epoch 1, Batch 90, train loss:1.406941533088684, Elapsed time for epoch : 1.0516908288002014
Epoch 1, Batch 100, train loss:1.0429761409759521, Elapsed time for epoch : 1.167247756322225
Epoch 1, Batch 110, train loss:1.259617567062378, Elapsed time for epoch : 1.2833339810371398
Batch 0, val loss:2.2565455436706543
Batch 10, val loss:6.921954154968262
Batch 20, val loss:2.7392616271972656
Batch 30, val loss:2.4656214714050293
Epoch 1, Train Loss:1.3881733210190483, Val loss:6.180936336517334
Epoch 2, Batch 0, train loss:1.1916574239730835, Elapsed time for epoch : 0.011570584774017335
Epoch 2, Batch 10, train loss:1.2351815700531006, Elapsed time for epoch : 0.12721286614735922
Epoch 2, Batch 20, train loss:1.183985710144043, Elapsed time for epoch : 0.2428528070449829
Epoch 2, Batch 30, train loss:1.1094070672988892, Elapsed time for epoch : 0.35885416666666664
Epoch 2, Batch 40, train loss:1.1456429958343506, Elapsed time for epoch : 0.4748744209607442
Epoch 2, Batch 50, train loss:1.198148488998413, Elapsed time for epoch : 0.5908702452977498
Epoch 2, Batch 60, train loss:1.0319526195526123, Elapsed time for epoch : 0.7066888093948365
Epoch 2, Batch 70, train loss:1.0187557935714722, Elapsed time for epoch : 0.8228257338205973
Epoch 2, Batch 80, train loss:1.2247917652130127, Elapsed time for epoch : 0.9385842561721802
Epoch 2, Batch 90, train loss:1.0989723205566406, Elapsed time for epoch : 1.0547566374142965
Epoch 2, Batch 100, train loss:1.0689162015914917, Elapsed time for epoch : 1.1706021229426067
Epoch 2, Batch 110, train loss:1.0679970979690552, Elapsed time for epoch : 1.286509601275126
Batch 0, val loss:22.517465591430664
Batch 10, val loss:3.4835188388824463
Batch 20, val loss:2.730771780014038
Batch 30, val loss:2.4606404304504395
Epoch 2, Train Loss:1.1211424407751664, Val loss:10.94334296716584
Epoch 3, Batch 0, train loss:1.0983387231826782, Elapsed time for epoch : 0.01161498228708903
Epoch 3, Batch 10, train loss:1.042340636253357, Elapsed time for epoch : 0.1275003472963969
Epoch 3, Batch 20, train loss:1.0182297229766846, Elapsed time for epoch : 0.24325015544891357
Epoch 3, Batch 30, train loss:1.0260558128356934, Elapsed time for epoch : 0.3593153198560079
Epoch 3, Batch 40, train loss:1.0344841480255127, Elapsed time for epoch : 0.4754445234934489
Epoch 3, Batch 50, train loss:0.8001105189323425, Elapsed time for epoch : 0.590989871819814
Epoch 3, Batch 60, train loss:0.9646849036216736, Elapsed time for epoch : 0.706996484597524
Epoch 3, Batch 70, train loss:0.9849231839179993, Elapsed time for epoch : 0.8228402972221375
Epoch 3, Batch 80, train loss:1.0299787521362305, Elapsed time for epoch : 0.9387333830197652
Epoch 3, Batch 90, train loss:1.0230118036270142, Elapsed time for epoch : 1.0546884497006734
Epoch 3, Batch 100, train loss:0.7860875129699707, Elapsed time for epoch : 1.1705214738845826
Epoch 3, Batch 110, train loss:0.8033939003944397, Elapsed time for epoch : 1.286418088277181
Batch 0, val loss:6.346395969390869
Batch 10, val loss:6.538214683532715
Batch 20, val loss:2.187405824661255
Batch 30, val loss:4.469576358795166
Epoch 3, Train Loss:0.9998316469399825, Val loss:8.039728664689594
Epoch 4, Batch 0, train loss:0.9914650321006775, Elapsed time for epoch : 0.011671837170918782
Epoch 4, Batch 10, train loss:0.800117015838623, Elapsed time for epoch : 0.12759333451588947
Epoch 4, Batch 20, train loss:0.8435159921646118, Elapsed time for epoch : 0.2435439387957255
Epoch 4, Batch 30, train loss:0.8810386061668396, Elapsed time for epoch : 0.3596162478129069
Epoch 4, Batch 40, train loss:0.9400702714920044, Elapsed time for epoch : 0.4757238229115804
Epoch 4, Batch 50, train loss:0.9885932803153992, Elapsed time for epoch : 0.5915831247965495
Epoch 4, Batch 60, train loss:0.9600507020950317, Elapsed time for epoch : 0.7075375954310099
Epoch 4, Batch 70, train loss:0.9837784767150879, Elapsed time for epoch : 0.8233271241188049
Epoch 4, Batch 80, train loss:0.9316459894180298, Elapsed time for epoch : 0.9394982020060222
Epoch 4, Batch 90, train loss:0.9426449537277222, Elapsed time for epoch : 1.0554121534029643
Epoch 4, Batch 100, train loss:0.9223838448524475, Elapsed time for epoch : 1.1712071975072225
Epoch 4, Batch 110, train loss:0.9524491429328918, Elapsed time for epoch : 1.2874351501464845
Batch 0, val loss:3.6308979988098145
Batch 10, val loss:3.1232123374938965
Batch 20, val loss:1.970996618270874
Batch 30, val loss:2.5972392559051514
Epoch 4, Train Loss:0.9258323306622712, Val loss:6.747061683071984
wandb: - 0.099 MB of 0.099 MB uploadedwandb: \ 0.146 MB of 0.161 MB uploaded (0.004 MB deduped)wandb: | 0.146 MB of 0.161 MB uploaded (0.004 MB deduped)wandb: / 0.161 MB of 0.161 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 2.3%             
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.92583
wandb:         Val Loss 6.74706
wandb:      train_batch 110
wandb: train_batch_loss 0.95245
wandb:        val_batch 30
wandb:   val_batch_loss 2.59724
wandb: 
wandb: üöÄ View run devout-rain-310 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/a344n2pe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_001025-a344n2pe/logs
Seed completed execution! 1 0.8_1
------------------------------------------------------------------
Running for seed 42 of experiment 0.8_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_001818-stxergrl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-cherry-312
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/stxergrl
Epoch 0, Batch 0, train loss:8.552826881408691, Elapsed time for epoch : 0.012953841686248779
Epoch 0, Batch 10, train loss:3.12107515335083, Elapsed time for epoch : 0.12830456892649333
Epoch 0, Batch 20, train loss:2.6773674488067627, Elapsed time for epoch : 0.2437222401301066
Epoch 0, Batch 30, train loss:2.4125542640686035, Elapsed time for epoch : 0.3591658711433411
Epoch 0, Batch 40, train loss:2.149223804473877, Elapsed time for epoch : 0.47527626355489094
Epoch 0, Batch 50, train loss:2.0407238006591797, Elapsed time for epoch : 0.5907871683438619
Epoch 0, Batch 60, train loss:1.9001915454864502, Elapsed time for epoch : 0.7062876264254252
Epoch 0, Batch 70, train loss:1.9667513370513916, Elapsed time for epoch : 0.8221937775611877
Epoch 0, Batch 80, train loss:1.765973687171936, Elapsed time for epoch : 0.9379358649253845
Epoch 0, Batch 90, train loss:1.7038229703903198, Elapsed time for epoch : 1.0539747516314188
Epoch 0, Batch 100, train loss:1.709254503250122, Elapsed time for epoch : 1.1698973337809244
Epoch 0, Batch 110, train loss:1.5245072841644287, Elapsed time for epoch : 1.2857105533281963
Batch 0, val loss:3.21181321144104
Batch 10, val loss:5.64308500289917
Batch 20, val loss:4.529299259185791
Batch 30, val loss:5.170571327209473
Epoch 0, Train Loss:3.1661625271258145, Val loss:3.3339531355433993
Epoch 1, Batch 0, train loss:1.6201677322387695, Elapsed time for epoch : 0.011617871125539143
Epoch 1, Batch 10, train loss:1.554982304573059, Elapsed time for epoch : 0.12748301426569622
Epoch 1, Batch 20, train loss:1.4135915040969849, Elapsed time for epoch : 0.24336777925491332
Epoch 1, Batch 30, train loss:1.3944355249404907, Elapsed time for epoch : 0.3590802431106567
Epoch 1, Batch 40, train loss:1.4157421588897705, Elapsed time for epoch : 0.4748866200447083
Epoch 1, Batch 50, train loss:1.4690449237823486, Elapsed time for epoch : 0.590691073735555
Epoch 1, Batch 60, train loss:1.3224468231201172, Elapsed time for epoch : 0.7063064972559611
Epoch 1, Batch 70, train loss:1.3589171171188354, Elapsed time for epoch : 0.8221123218536377
Epoch 1, Batch 80, train loss:1.2880085706710815, Elapsed time for epoch : 0.9376931548118591
Epoch 1, Batch 90, train loss:1.406941533088684, Elapsed time for epoch : 1.0539028724034627
Epoch 1, Batch 100, train loss:1.0429761409759521, Elapsed time for epoch : 1.1697813669840496
Epoch 1, Batch 110, train loss:1.259617567062378, Elapsed time for epoch : 1.2855351010958354
Batch 0, val loss:2.2565455436706543
Batch 10, val loss:6.921954154968262
Batch 20, val loss:2.7392616271972656
Batch 30, val loss:2.4656214714050293
Epoch 1, Train Loss:1.3881733210190483, Val loss:6.180936336517334
Epoch 2, Batch 0, train loss:1.1916574239730835, Elapsed time for epoch : 0.011612399419148763
Epoch 2, Batch 10, train loss:1.2351815700531006, Elapsed time for epoch : 0.1275067965189616
Epoch 2, Batch 20, train loss:1.183985710144043, Elapsed time for epoch : 0.24372612635294597
Epoch 2, Batch 30, train loss:1.1094070672988892, Elapsed time for epoch : 0.35967793464660647
Epoch 2, Batch 40, train loss:1.1456429958343506, Elapsed time for epoch : 0.4758265773455302
Epoch 2, Batch 50, train loss:1.198148488998413, Elapsed time for epoch : 0.5917171955108642
Epoch 2, Batch 60, train loss:1.0319526195526123, Elapsed time for epoch : 0.7077538291613261
Epoch 2, Batch 70, train loss:1.0187557935714722, Elapsed time for epoch : 0.823678453763326
Epoch 2, Batch 80, train loss:1.2247917652130127, Elapsed time for epoch : 0.9399525880813598
Epoch 2, Batch 90, train loss:1.0989723205566406, Elapsed time for epoch : 1.0559348146120706
Epoch 2, Batch 100, train loss:1.0689162015914917, Elapsed time for epoch : 1.1722423275311789
Epoch 2, Batch 110, train loss:1.0679970979690552, Elapsed time for epoch : 1.2880687912305195
Batch 0, val loss:22.517465591430664
Batch 10, val loss:3.4835188388824463
Batch 20, val loss:2.730771780014038
Batch 30, val loss:2.4606404304504395
Epoch 2, Train Loss:1.1211424407751664, Val loss:10.94334296716584
Epoch 3, Batch 0, train loss:1.0983387231826782, Elapsed time for epoch : 0.011619369188944498
Epoch 3, Batch 10, train loss:1.042340636253357, Elapsed time for epoch : 0.12771092255910238
Epoch 3, Batch 20, train loss:1.0182297229766846, Elapsed time for epoch : 0.24380404949188234
Epoch 3, Batch 30, train loss:1.0260558128356934, Elapsed time for epoch : 0.3597746729850769
Epoch 3, Batch 40, train loss:1.0344841480255127, Elapsed time for epoch : 0.4756125887235006
Epoch 3, Batch 50, train loss:0.8001105189323425, Elapsed time for epoch : 0.591761056582133
Epoch 3, Batch 60, train loss:0.9646849036216736, Elapsed time for epoch : 0.7077035864194234
Epoch 3, Batch 70, train loss:0.9849231839179993, Elapsed time for epoch : 0.8236038406689962
Epoch 3, Batch 80, train loss:1.0299787521362305, Elapsed time for epoch : 0.9394442280133565
Epoch 3, Batch 90, train loss:1.0230118036270142, Elapsed time for epoch : 1.0554661353429158
Epoch 3, Batch 100, train loss:0.7860875129699707, Elapsed time for epoch : 1.1710156122843425
Epoch 3, Batch 110, train loss:0.8033939003944397, Elapsed time for epoch : 1.2870051344235738
Batch 0, val loss:6.346395969390869
Batch 10, val loss:6.538214683532715
Batch 20, val loss:2.187405824661255
Batch 30, val loss:4.469576358795166
Epoch 3, Train Loss:0.9998316469399825, Val loss:8.039728664689594
Epoch 4, Batch 0, train loss:0.9914650321006775, Elapsed time for epoch : 0.011596882343292236
Epoch 4, Batch 10, train loss:0.800117015838623, Elapsed time for epoch : 0.1275087594985962
Epoch 4, Batch 20, train loss:0.8435159921646118, Elapsed time for epoch : 0.24329216480255128
Epoch 4, Batch 30, train loss:0.8810386061668396, Elapsed time for epoch : 0.35913537740707396
Epoch 4, Batch 40, train loss:0.9400702714920044, Elapsed time for epoch : 0.4750052253405253
Epoch 4, Batch 50, train loss:0.9885932803153992, Elapsed time for epoch : 0.5911239226659138
Epoch 4, Batch 60, train loss:0.9600507020950317, Elapsed time for epoch : 0.7072116374969483
Epoch 4, Batch 70, train loss:0.9837784767150879, Elapsed time for epoch : 0.823022170861562
Epoch 4, Batch 80, train loss:0.9316459894180298, Elapsed time for epoch : 0.9388840874036153
Epoch 4, Batch 90, train loss:0.9426449537277222, Elapsed time for epoch : 1.054737909634908
Epoch 4, Batch 100, train loss:0.9223838448524475, Elapsed time for epoch : 1.1705457369486492
Epoch 4, Batch 110, train loss:0.9524491429328918, Elapsed time for epoch : 1.286196764310201
Batch 0, val loss:3.6308979988098145
Batch 10, val loss:3.1232123374938965
Batch 20, val loss:1.970996618270874
Batch 30, val loss:2.5972392559051514
Epoch 4, Train Loss:0.9258323306622712, Val loss:6.747061683071984
wandb: - 0.102 MB of 0.116 MB uploadedwandb: \ 0.102 MB of 0.116 MB uploadedwandb: | 0.116 MB of 0.116 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.92583
wandb:         Val Loss 6.74706
wandb:      train_batch 110
wandb: train_batch_loss 0.95245
wandb:        val_batch 30
wandb:   val_batch_loss 2.59724
wandb: 
wandb: üöÄ View run easy-cherry-312 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/stxergrl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_001818-stxergrl/logs
Seed completed execution! 42 0.8_1
------------------------------------------------------------------
Running for seed 89 of experiment 0.8_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_002602-sadk5iys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-cloud-314
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/sadk5iys
Epoch 0, Batch 0, train loss:8.552826881408691, Elapsed time for epoch : 0.013387755552927653
Epoch 0, Batch 10, train loss:3.12107515335083, Elapsed time for epoch : 0.12901314496994018
Epoch 0, Batch 20, train loss:2.6773674488067627, Elapsed time for epoch : 0.24438393115997314
Epoch 0, Batch 30, train loss:2.4125542640686035, Elapsed time for epoch : 0.36061238845189414
Epoch 0, Batch 40, train loss:2.149223804473877, Elapsed time for epoch : 0.4764820734659831
Epoch 0, Batch 50, train loss:2.0407238006591797, Elapsed time for epoch : 0.5919700225194295
Epoch 0, Batch 60, train loss:1.9001915454864502, Elapsed time for epoch : 0.7075445373853048
Epoch 0, Batch 70, train loss:1.9667513370513916, Elapsed time for epoch : 0.8231299757957459
Epoch 0, Batch 80, train loss:1.765973687171936, Elapsed time for epoch : 0.9390976826349894
Epoch 0, Batch 90, train loss:1.7038229703903198, Elapsed time for epoch : 1.0550305326779683
Epoch 0, Batch 100, train loss:1.709254503250122, Elapsed time for epoch : 1.171447459856669
Epoch 0, Batch 110, train loss:1.5245072841644287, Elapsed time for epoch : 1.2879605730374655
Batch 0, val loss:3.21181321144104
Batch 10, val loss:5.64308500289917
Batch 20, val loss:4.529299259185791
Batch 30, val loss:5.170571327209473
Epoch 0, Train Loss:3.1661625271258145, Val loss:3.3339531355433993
Epoch 1, Batch 0, train loss:1.6201677322387695, Elapsed time for epoch : 0.011627344290415446
Epoch 1, Batch 10, train loss:1.554982304573059, Elapsed time for epoch : 0.12760692834854126
Epoch 1, Batch 20, train loss:1.4135915040969849, Elapsed time for epoch : 0.24399877389272054
Epoch 1, Batch 30, train loss:1.3944355249404907, Elapsed time for epoch : 0.3599063277244568
Epoch 1, Batch 40, train loss:1.4157421588897705, Elapsed time for epoch : 0.4756883899370829
Epoch 1, Batch 50, train loss:1.4690449237823486, Elapsed time for epoch : 0.5913987080256145
Epoch 1, Batch 60, train loss:1.3224468231201172, Elapsed time for epoch : 0.7073891957600912
Epoch 1, Batch 70, train loss:1.3589171171188354, Elapsed time for epoch : 0.8234265049298605
Epoch 1, Batch 80, train loss:1.2880085706710815, Elapsed time for epoch : 0.9403419335683186
Epoch 1, Batch 90, train loss:1.406941533088684, Elapsed time for epoch : 1.0565428376197814
Epoch 1, Batch 100, train loss:1.0429761409759521, Elapsed time for epoch : 1.1721999883651733
Epoch 1, Batch 110, train loss:1.259617567062378, Elapsed time for epoch : 1.2882932345072429
Batch 0, val loss:2.2565455436706543
Batch 10, val loss:6.921954154968262
Batch 20, val loss:2.7392616271972656
Batch 30, val loss:2.4656214714050293
Epoch 1, Train Loss:1.3881733210190483, Val loss:6.180936336517334
Epoch 2, Batch 0, train loss:1.1916574239730835, Elapsed time for epoch : 0.011613055070241293
Epoch 2, Batch 10, train loss:1.2351815700531006, Elapsed time for epoch : 0.1275968631108602
Epoch 2, Batch 20, train loss:1.183985710144043, Elapsed time for epoch : 0.2434835950533549
Epoch 2, Batch 30, train loss:1.1094070672988892, Elapsed time for epoch : 0.3593616565068563
Epoch 2, Batch 40, train loss:1.1456429958343506, Elapsed time for epoch : 0.4756744662920634
Epoch 2, Batch 50, train loss:1.198148488998413, Elapsed time for epoch : 0.5916968425114949
Epoch 2, Batch 60, train loss:1.0319526195526123, Elapsed time for epoch : 0.7077594637870789
Epoch 2, Batch 70, train loss:1.0187557935714722, Elapsed time for epoch : 0.8242382049560547
Epoch 2, Batch 80, train loss:1.2247917652130127, Elapsed time for epoch : 0.9402690172195435
Epoch 2, Batch 90, train loss:1.0989723205566406, Elapsed time for epoch : 1.0561498483022054
Epoch 2, Batch 100, train loss:1.0689162015914917, Elapsed time for epoch : 1.171969715754191
Epoch 2, Batch 110, train loss:1.0679970979690552, Elapsed time for epoch : 1.2878721674283347
Batch 0, val loss:22.517465591430664
Batch 10, val loss:3.4835188388824463
Batch 20, val loss:2.730771780014038
Batch 30, val loss:2.4606404304504395
Epoch 2, Train Loss:1.1211424407751664, Val loss:10.94334296716584
Epoch 3, Batch 0, train loss:1.0983387231826782, Elapsed time for epoch : 0.011658970514933269
Epoch 3, Batch 10, train loss:1.042340636253357, Elapsed time for epoch : 0.1276148796081543
Epoch 3, Batch 20, train loss:1.0182297229766846, Elapsed time for epoch : 0.24335114161173502
Epoch 3, Batch 30, train loss:1.0260558128356934, Elapsed time for epoch : 0.3590704878171285
Epoch 3, Batch 40, train loss:1.0344841480255127, Elapsed time for epoch : 0.47554153203964233
Epoch 3, Batch 50, train loss:0.8001105189323425, Elapsed time for epoch : 0.5914475917816162
Epoch 3, Batch 60, train loss:0.9646849036216736, Elapsed time for epoch : 0.7072917938232421
Epoch 3, Batch 70, train loss:0.9849231839179993, Elapsed time for epoch : 0.823465633392334
Epoch 3, Batch 80, train loss:1.0299787521362305, Elapsed time for epoch : 0.9391331911087036
Epoch 3, Batch 90, train loss:1.0230118036270142, Elapsed time for epoch : 1.0549407958984376
Epoch 3, Batch 100, train loss:0.7860875129699707, Elapsed time for epoch : 1.1709000666936238
Epoch 3, Batch 110, train loss:0.8033939003944397, Elapsed time for epoch : 1.2870550394058227
Batch 0, val loss:6.346395969390869
Batch 10, val loss:6.538214683532715
Batch 20, val loss:2.187405824661255
Batch 30, val loss:4.469576358795166
Epoch 3, Train Loss:0.9998316469399825, Val loss:8.039728664689594
Epoch 4, Batch 0, train loss:0.9914650321006775, Elapsed time for epoch : 0.011581337451934815
Epoch 4, Batch 10, train loss:0.800117015838623, Elapsed time for epoch : 0.1276844342549642
Epoch 4, Batch 20, train loss:0.8435159921646118, Elapsed time for epoch : 0.24345403909683228
Epoch 4, Batch 30, train loss:0.8810386061668396, Elapsed time for epoch : 0.35965388218561806
Epoch 4, Batch 40, train loss:0.9400702714920044, Elapsed time for epoch : 0.4757974704106649
Epoch 4, Batch 50, train loss:0.9885932803153992, Elapsed time for epoch : 0.5921578963597616
Epoch 4, Batch 60, train loss:0.9600507020950317, Elapsed time for epoch : 0.7083731055259704
Epoch 4, Batch 70, train loss:0.9837784767150879, Elapsed time for epoch : 0.8241279562314351
Epoch 4, Batch 80, train loss:0.9316459894180298, Elapsed time for epoch : 0.9401368737220764
Epoch 4, Batch 90, train loss:0.9426449537277222, Elapsed time for epoch : 1.056139385700226
Epoch 4, Batch 100, train loss:0.9223838448524475, Elapsed time for epoch : 1.1719440261522929
Epoch 4, Batch 110, train loss:0.9524491429328918, Elapsed time for epoch : 1.2880128343900046
Batch 0, val loss:3.6308979988098145
Batch 10, val loss:3.1232123374938965
Batch 20, val loss:1.970996618270874
Batch 30, val loss:2.5972392559051514
Epoch 4, Train Loss:0.9258323306622712, Val loss:6.747061683071984
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.157 MB of 0.157 MB uploadedwandb: / 0.248 MB of 0.248 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 1.5%             
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.92583
wandb:         Val Loss 6.74706
wandb:      train_batch 110
wandb: train_batch_loss 0.95245
wandb:        val_batch 30
wandb:   val_batch_loss 2.59724
wandb: 
wandb: üöÄ View run apricot-cloud-314 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/sadk5iys
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_002602-sadk5iys/logs
Seed completed execution! 89 0.8_1
------------------------------------------------------------------
Running for seed 23 of experiment 0.8_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_003349-oqiy3kco
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-gorge-316
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/oqiy3kco
Epoch 0, Batch 0, train loss:8.552826881408691, Elapsed time for epoch : 0.013382391134897868
Epoch 0, Batch 10, train loss:3.12107515335083, Elapsed time for epoch : 0.1287808338801066
Epoch 0, Batch 20, train loss:2.6773674488067627, Elapsed time for epoch : 0.2440940817197164
Epoch 0, Batch 30, train loss:2.4125542640686035, Elapsed time for epoch : 0.35974130233128865
Epoch 0, Batch 40, train loss:2.149223804473877, Elapsed time for epoch : 0.47603015502293905
Epoch 0, Batch 50, train loss:2.0407238006591797, Elapsed time for epoch : 0.5920752008756002
Epoch 0, Batch 60, train loss:1.9001915454864502, Elapsed time for epoch : 0.707571812470754
Epoch 0, Batch 70, train loss:1.9667513370513916, Elapsed time for epoch : 0.822981067498525
Epoch 0, Batch 80, train loss:1.765973687171936, Elapsed time for epoch : 0.9390410860379537
Epoch 0, Batch 90, train loss:1.7038229703903198, Elapsed time for epoch : 1.055465614795685
Epoch 0, Batch 100, train loss:1.709254503250122, Elapsed time for epoch : 1.171332875887553
Epoch 0, Batch 110, train loss:1.5245072841644287, Elapsed time for epoch : 1.2873008171717326
Batch 0, val loss:3.21181321144104
Batch 10, val loss:5.64308500289917
Batch 20, val loss:4.529299259185791
Batch 30, val loss:5.170571327209473
Epoch 0, Train Loss:3.1661625271258145, Val loss:3.3339531355433993
Epoch 1, Batch 0, train loss:1.6201677322387695, Elapsed time for epoch : 0.01164556344350179
Epoch 1, Batch 10, train loss:1.554982304573059, Elapsed time for epoch : 0.12783266305923463
Epoch 1, Batch 20, train loss:1.4135915040969849, Elapsed time for epoch : 0.24401813745498657
Epoch 1, Batch 30, train loss:1.3944355249404907, Elapsed time for epoch : 0.35997623999913536
Epoch 1, Batch 40, train loss:1.4157421588897705, Elapsed time for epoch : 0.4760794401168823
Epoch 1, Batch 50, train loss:1.4690449237823486, Elapsed time for epoch : 0.5925942262013754
Epoch 1, Batch 60, train loss:1.3224468231201172, Elapsed time for epoch : 0.7086721460024515
Epoch 1, Batch 70, train loss:1.3589171171188354, Elapsed time for epoch : 0.8244251211484274
Epoch 1, Batch 80, train loss:1.2880085706710815, Elapsed time for epoch : 0.9408763448397318
Epoch 1, Batch 90, train loss:1.406941533088684, Elapsed time for epoch : 1.05694739818573
Epoch 1, Batch 100, train loss:1.0429761409759521, Elapsed time for epoch : 1.1728530168533324
Epoch 1, Batch 110, train loss:1.259617567062378, Elapsed time for epoch : 1.2888280510902406
Batch 0, val loss:2.2565455436706543
Batch 10, val loss:6.921954154968262
Batch 20, val loss:2.7392616271972656
Batch 30, val loss:2.4656214714050293
Epoch 1, Train Loss:1.3881733210190483, Val loss:6.180936336517334
Epoch 2, Batch 0, train loss:1.1916574239730835, Elapsed time for epoch : 0.01162639856338501
Epoch 2, Batch 10, train loss:1.2351815700531006, Elapsed time for epoch : 0.1281975229581197
Epoch 2, Batch 20, train loss:1.183985710144043, Elapsed time for epoch : 0.24418004353841147
Epoch 2, Batch 30, train loss:1.1094070672988892, Elapsed time for epoch : 0.36048160394032797
Epoch 2, Batch 40, train loss:1.1456429958343506, Elapsed time for epoch : 0.4767475684483846
Epoch 2, Batch 50, train loss:1.198148488998413, Elapsed time for epoch : 0.5929620345433553
Epoch 2, Batch 60, train loss:1.0319526195526123, Elapsed time for epoch : 0.7090634306271871
Epoch 2, Batch 70, train loss:1.0187557935714722, Elapsed time for epoch : 0.8250548521677653
Epoch 2, Batch 80, train loss:1.2247917652130127, Elapsed time for epoch : 0.9412109891573588
Epoch 2, Batch 90, train loss:1.0989723205566406, Elapsed time for epoch : 1.0570124626159667
Epoch 2, Batch 100, train loss:1.0689162015914917, Elapsed time for epoch : 1.17320028146108
Epoch 2, Batch 110, train loss:1.0679970979690552, Elapsed time for epoch : 1.2890363891919454
Batch 0, val loss:22.517465591430664
Batch 10, val loss:3.4835188388824463
Batch 20, val loss:2.730771780014038
Batch 30, val loss:2.4606404304504395
Epoch 2, Train Loss:1.1211424407751664, Val loss:10.94334296716584
Epoch 3, Batch 0, train loss:1.0983387231826782, Elapsed time for epoch : 0.011599055926005046
Epoch 3, Batch 10, train loss:1.042340636253357, Elapsed time for epoch : 0.12756605545679728
Epoch 3, Batch 20, train loss:1.0182297229766846, Elapsed time for epoch : 0.24396763642628988
Epoch 3, Batch 30, train loss:1.0260558128356934, Elapsed time for epoch : 0.36052329937616984
Epoch 3, Batch 40, train loss:1.0344841480255127, Elapsed time for epoch : 0.4768436670303345
Epoch 3, Batch 50, train loss:0.8001105189323425, Elapsed time for epoch : 0.5931978146235148
Epoch 3, Batch 60, train loss:0.9646849036216736, Elapsed time for epoch : 0.7095601081848144
Epoch 3, Batch 70, train loss:0.9849231839179993, Elapsed time for epoch : 0.8256672819455465
Epoch 3, Batch 80, train loss:1.0299787521362305, Elapsed time for epoch : 0.942138119538625
Epoch 3, Batch 90, train loss:1.0230118036270142, Elapsed time for epoch : 1.0587780078252156
Epoch 3, Batch 100, train loss:0.7860875129699707, Elapsed time for epoch : 1.1750203728675843
Epoch 3, Batch 110, train loss:0.8033939003944397, Elapsed time for epoch : 1.2915036797523498
Batch 0, val loss:6.346395969390869
Batch 10, val loss:6.538214683532715
Batch 20, val loss:2.187405824661255
Batch 30, val loss:4.469576358795166
Epoch 3, Train Loss:0.9998316469399825, Val loss:8.039728664689594
Epoch 4, Batch 0, train loss:0.9914650321006775, Elapsed time for epoch : 0.011721011002858479
Epoch 4, Batch 10, train loss:0.800117015838623, Elapsed time for epoch : 0.1297934095064799
Epoch 4, Batch 20, train loss:0.8435159921646118, Elapsed time for epoch : 0.24644784530003866
Epoch 4, Batch 30, train loss:0.8810386061668396, Elapsed time for epoch : 0.3637608249982198
Epoch 4, Batch 40, train loss:0.9400702714920044, Elapsed time for epoch : 0.4812055627504985
Epoch 4, Batch 50, train loss:0.9885932803153992, Elapsed time for epoch : 0.5982128024101258
Epoch 4, Batch 60, train loss:0.9600507020950317, Elapsed time for epoch : 0.7169299046198527
Epoch 4, Batch 70, train loss:0.9837784767150879, Elapsed time for epoch : 0.834311314423879
Epoch 4, Batch 80, train loss:0.9316459894180298, Elapsed time for epoch : 0.9509182413419087
Epoch 4, Batch 90, train loss:0.9426449537277222, Elapsed time for epoch : 1.0678956230481467
Epoch 4, Batch 100, train loss:0.9223838448524475, Elapsed time for epoch : 1.1849466522534688
Epoch 4, Batch 110, train loss:0.9524491429328918, Elapsed time for epoch : 1.3021238764127097
Batch 0, val loss:3.6308979988098145
Batch 10, val loss:3.1232123374938965
Batch 20, val loss:1.970996618270874
Batch 30, val loss:2.5972392559051514
Epoch 4, Train Loss:0.9258323306622712, Val loss:6.747061683071984
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.92583
wandb:         Val Loss 6.74706
wandb:      train_batch 110
wandb: train_batch_loss 0.95245
wandb:        val_batch 30
wandb:   val_batch_loss 2.59724
wandb: 
wandb: üöÄ View run wobbly-gorge-316 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/oqiy3kco
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_003349-oqiy3kco/logs
Seed completed execution! 23 0.8_1
------------------------------------------------------------------
Running for seed 113 of experiment 0.8_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_004136-lhm0hnba
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sunset-318
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/lhm0hnba
Epoch 0, Batch 0, train loss:8.552826881408691, Elapsed time for epoch : 0.013315161069234213
Epoch 0, Batch 10, train loss:3.12107515335083, Elapsed time for epoch : 0.12933359940846761
Epoch 0, Batch 20, train loss:2.6773674488067627, Elapsed time for epoch : 0.2447969436645508
Epoch 0, Batch 30, train loss:2.4125542640686035, Elapsed time for epoch : 0.36084203322728475
Epoch 0, Batch 40, train loss:2.149223804473877, Elapsed time for epoch : 0.47710617780685427
Epoch 0, Batch 50, train loss:2.0407238006591797, Elapsed time for epoch : 0.592831552028656
Epoch 0, Batch 60, train loss:1.9001915454864502, Elapsed time for epoch : 0.7089680274327596
Epoch 0, Batch 70, train loss:1.9667513370513916, Elapsed time for epoch : 0.825238561630249
Epoch 0, Batch 80, train loss:1.765973687171936, Elapsed time for epoch : 0.9410300731658936
Epoch 0, Batch 90, train loss:1.7038229703903198, Elapsed time for epoch : 1.0573049187660217
Epoch 0, Batch 100, train loss:1.709254503250122, Elapsed time for epoch : 1.1734119017918905
Epoch 0, Batch 110, train loss:1.5245072841644287, Elapsed time for epoch : 1.2891106843948363
Batch 0, val loss:3.21181321144104
Batch 10, val loss:5.64308500289917
Batch 20, val loss:4.529299259185791
Batch 30, val loss:5.170571327209473
Epoch 0, Train Loss:3.1661625271258145, Val loss:3.3339531355433993
Epoch 1, Batch 0, train loss:1.6201677322387695, Elapsed time for epoch : 0.011606597900390625
Epoch 1, Batch 10, train loss:1.554982304573059, Elapsed time for epoch : 0.1276771068572998
Epoch 1, Batch 20, train loss:1.4135915040969849, Elapsed time for epoch : 0.24380047718683878
Epoch 1, Batch 30, train loss:1.3944355249404907, Elapsed time for epoch : 0.359929092725118
Epoch 1, Batch 40, train loss:1.4157421588897705, Elapsed time for epoch : 0.4756539463996887
Epoch 1, Batch 50, train loss:1.4690449237823486, Elapsed time for epoch : 0.5918760657310486
Epoch 1, Batch 60, train loss:1.3224468231201172, Elapsed time for epoch : 0.7077603181203206
Epoch 1, Batch 70, train loss:1.3589171171188354, Elapsed time for epoch : 0.8239681919415792
Epoch 1, Batch 80, train loss:1.2880085706710815, Elapsed time for epoch : 0.9405849695205688
Epoch 1, Batch 90, train loss:1.406941533088684, Elapsed time for epoch : 1.0562572081883748
Epoch 1, Batch 100, train loss:1.0429761409759521, Elapsed time for epoch : 1.172318414847056
Epoch 1, Batch 110, train loss:1.259617567062378, Elapsed time for epoch : 1.2884839057922364
Batch 0, val loss:2.2565455436706543
Batch 10, val loss:6.921954154968262
Batch 20, val loss:2.7392616271972656
Batch 30, val loss:2.4656214714050293
Epoch 1, Train Loss:1.3881733210190483, Val loss:6.180936336517334
Epoch 2, Batch 0, train loss:1.1916574239730835, Elapsed time for epoch : 0.0116912841796875
Epoch 2, Batch 10, train loss:1.2351815700531006, Elapsed time for epoch : 0.12778515418370565
Epoch 2, Batch 20, train loss:1.183985710144043, Elapsed time for epoch : 0.24400233427683513
Epoch 2, Batch 30, train loss:1.1094070672988892, Elapsed time for epoch : 0.36013342142105104
Epoch 2, Batch 40, train loss:1.1456429958343506, Elapsed time for epoch : 0.47636249860127766
Epoch 2, Batch 50, train loss:1.198148488998413, Elapsed time for epoch : 0.592594317595164
Epoch 2, Batch 60, train loss:1.0319526195526123, Elapsed time for epoch : 0.7087592164675395
Epoch 2, Batch 70, train loss:1.0187557935714722, Elapsed time for epoch : 0.8246546705563863
Epoch 2, Batch 80, train loss:1.2247917652130127, Elapsed time for epoch : 0.9406353592872619
Epoch 2, Batch 90, train loss:1.0989723205566406, Elapsed time for epoch : 1.0566590428352356
Epoch 2, Batch 100, train loss:1.0689162015914917, Elapsed time for epoch : 1.1724286516507467
Epoch 2, Batch 110, train loss:1.0679970979690552, Elapsed time for epoch : 1.288786248366038
Batch 0, val loss:22.517465591430664
Batch 10, val loss:3.4835188388824463
Batch 20, val loss:2.730771780014038
Batch 30, val loss:2.4606404304504395
Epoch 2, Train Loss:1.1211424407751664, Val loss:10.94334296716584
Epoch 3, Batch 0, train loss:1.0983387231826782, Elapsed time for epoch : 0.011605485280354818
Epoch 3, Batch 10, train loss:1.042340636253357, Elapsed time for epoch : 0.12757195631663004
Epoch 3, Batch 20, train loss:1.0182297229766846, Elapsed time for epoch : 0.24335665702819825
Epoch 3, Batch 30, train loss:1.0260558128356934, Elapsed time for epoch : 0.3593374967575073
Epoch 3, Batch 40, train loss:1.0344841480255127, Elapsed time for epoch : 0.47528576453526816
Epoch 3, Batch 50, train loss:0.8001105189323425, Elapsed time for epoch : 0.5911469340324402
Epoch 3, Batch 60, train loss:0.9646849036216736, Elapsed time for epoch : 0.7068601449330648
Epoch 3, Batch 70, train loss:0.9849231839179993, Elapsed time for epoch : 0.8226784865061442
Epoch 3, Batch 80, train loss:1.0299787521362305, Elapsed time for epoch : 0.9385757962862651
Epoch 3, Batch 90, train loss:1.0230118036270142, Elapsed time for epoch : 1.0543463667233786
Epoch 3, Batch 100, train loss:0.7860875129699707, Elapsed time for epoch : 1.1706392129262289
Epoch 3, Batch 110, train loss:0.8033939003944397, Elapsed time for epoch : 1.286762257417043
Batch 0, val loss:6.346395969390869
Batch 10, val loss:6.538214683532715
Batch 20, val loss:2.187405824661255
Batch 30, val loss:4.469576358795166
Epoch 3, Train Loss:0.9998316469399825, Val loss:8.039728664689594
Epoch 4, Batch 0, train loss:0.9914650321006775, Elapsed time for epoch : 0.011637218793233236
Epoch 4, Batch 10, train loss:0.800117015838623, Elapsed time for epoch : 0.12792526086171468
Epoch 4, Batch 20, train loss:0.8435159921646118, Elapsed time for epoch : 0.2436266779899597
Epoch 4, Batch 30, train loss:0.8810386061668396, Elapsed time for epoch : 0.3595416784286499
Epoch 4, Batch 40, train loss:0.9400702714920044, Elapsed time for epoch : 0.4753678599993388
Epoch 4, Batch 50, train loss:0.9885932803153992, Elapsed time for epoch : 0.5911472320556641
Epoch 4, Batch 60, train loss:0.9600507020950317, Elapsed time for epoch : 0.7070647517840067
Epoch 4, Batch 70, train loss:0.9837784767150879, Elapsed time for epoch : 0.8232259392738343
Epoch 4, Batch 80, train loss:0.9316459894180298, Elapsed time for epoch : 0.9392468174298604
Epoch 4, Batch 90, train loss:0.9426449537277222, Elapsed time for epoch : 1.0550703366597494
Epoch 4, Batch 100, train loss:0.9223838448524475, Elapsed time for epoch : 1.1713526527086893
Epoch 4, Batch 110, train loss:0.9524491429328918, Elapsed time for epoch : 1.2872196912765503
Batch 0, val loss:3.6308979988098145
Batch 10, val loss:3.1232123374938965
Batch 20, val loss:1.970996618270874
Batch 30, val loss:2.5972392559051514
Epoch 4, Train Loss:0.9258323306622712, Val loss:6.747061683071984
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñÖ‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.92583
wandb:         Val Loss 6.74706
wandb:      train_batch 110
wandb: train_batch_loss 0.95245
wandb:        val_batch 30
wandb:   val_batch_loss 2.59724
wandb: 
wandb: üöÄ View run rosy-sunset-318 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/lhm0hnba
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_004136-lhm0hnba/logs
Seed completed execution! 113 0.8_1
------------------------------------------------------------------
Experiment complete 0.8_1
==========================================================================
Running experiment for setting 0.8_2
==========================================================================
Running for seed 1 of experiment 0.8_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_004922-rs8usw38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-shadow-320
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rs8usw38
Epoch 0, Batch 0, train loss:8.56776237487793, Elapsed time for epoch : 0.013455518086751302
Epoch 0, Batch 10, train loss:3.559582233428955, Elapsed time for epoch : 0.13629727760950724
Epoch 0, Batch 20, train loss:2.675314426422119, Elapsed time for epoch : 0.25864511728286743
Epoch 0, Batch 30, train loss:2.3998827934265137, Elapsed time for epoch : 0.38224384784698484
Epoch 0, Batch 40, train loss:2.4049360752105713, Elapsed time for epoch : 0.5067177375157674
Epoch 0, Batch 50, train loss:2.180896282196045, Elapsed time for epoch : 0.6311874548594157
Epoch 0, Batch 60, train loss:1.9719817638397217, Elapsed time for epoch : 0.7534491101900737
Epoch 0, Batch 70, train loss:1.9255770444869995, Elapsed time for epoch : 0.8754124204317729
Epoch 0, Batch 80, train loss:1.7648454904556274, Elapsed time for epoch : 1.0000931779543558
Epoch 0, Batch 90, train loss:1.6992788314819336, Elapsed time for epoch : 1.123755447069804
Epoch 0, Batch 100, train loss:1.528357744216919, Elapsed time for epoch : 1.2467616160710653
Epoch 0, Batch 110, train loss:1.3825933933258057, Elapsed time for epoch : 1.3710837682088217
Batch 0, val loss:4.860324382781982
Batch 10, val loss:6.421743869781494
Batch 20, val loss:5.992686748504639
Batch 30, val loss:8.127802848815918
Epoch 0, Train Loss:2.873920926840409, Val loss:5.108003278573354
Epoch 1, Batch 0, train loss:1.5504549741744995, Elapsed time for epoch : 0.01163716713587443
Epoch 1, Batch 10, train loss:1.5201976299285889, Elapsed time for epoch : 0.12735893726348876
Epoch 1, Batch 20, train loss:1.2673088312149048, Elapsed time for epoch : 0.24347693125406902
Epoch 1, Batch 30, train loss:1.389096975326538, Elapsed time for epoch : 0.35952085653940835
Epoch 1, Batch 40, train loss:1.3575891256332397, Elapsed time for epoch : 0.4754427989323934
Epoch 1, Batch 50, train loss:1.207114338874817, Elapsed time for epoch : 0.5913590987523397
Epoch 1, Batch 60, train loss:1.1504820585250854, Elapsed time for epoch : 0.7072084665298461
Epoch 1, Batch 70, train loss:1.198116421699524, Elapsed time for epoch : 0.8232270002365112
Epoch 1, Batch 80, train loss:1.087999701499939, Elapsed time for epoch : 0.9392721692721049
Epoch 1, Batch 90, train loss:1.071778416633606, Elapsed time for epoch : 1.055340842405955
Epoch 1, Batch 100, train loss:0.9299827814102173, Elapsed time for epoch : 1.1713452577590941
Epoch 1, Batch 110, train loss:1.0389044284820557, Elapsed time for epoch : 1.2877097884813944
Batch 0, val loss:2.000309705734253
Batch 10, val loss:6.604417324066162
Batch 20, val loss:4.509688854217529
Batch 30, val loss:2.8324222564697266
Epoch 1, Train Loss:1.2184564409048662, Val loss:8.816828489303589
Epoch 2, Batch 0, train loss:1.0636513233184814, Elapsed time for epoch : 0.011612387498219807
Epoch 2, Batch 10, train loss:1.072441577911377, Elapsed time for epoch : 0.1276806076367696
Epoch 2, Batch 20, train loss:1.0407154560089111, Elapsed time for epoch : 0.2437923272450765
Epoch 2, Batch 30, train loss:0.9157772064208984, Elapsed time for epoch : 0.3597991347312927
Epoch 2, Batch 40, train loss:0.9665301442146301, Elapsed time for epoch : 0.4763927896817525
Epoch 2, Batch 50, train loss:1.0038787126541138, Elapsed time for epoch : 0.5931421597798665
Epoch 2, Batch 60, train loss:0.9815201163291931, Elapsed time for epoch : 0.7094403783480326
Epoch 2, Batch 70, train loss:0.6954653859138489, Elapsed time for epoch : 0.8258157968521118
Epoch 2, Batch 80, train loss:0.9449545741081238, Elapsed time for epoch : 0.9422678152720133
Epoch 2, Batch 90, train loss:0.9272769093513489, Elapsed time for epoch : 1.0582459410031637
Epoch 2, Batch 100, train loss:0.8940500617027283, Elapsed time for epoch : 1.174152406056722
Epoch 2, Batch 110, train loss:0.841804563999176, Elapsed time for epoch : 1.2903116424878438
Batch 0, val loss:4.791118621826172
Batch 10, val loss:3.014070749282837
Batch 20, val loss:2.168447732925415
Batch 30, val loss:14.687562942504883
Epoch 2, Train Loss:0.9121604769126229, Val loss:5.8267294185029135
Epoch 3, Batch 0, train loss:0.8407198190689087, Elapsed time for epoch : 0.01179971694946289
Epoch 3, Batch 10, train loss:0.861141562461853, Elapsed time for epoch : 0.12814075152079266
Epoch 3, Batch 20, train loss:0.8653088212013245, Elapsed time for epoch : 0.24449377059936522
Epoch 3, Batch 30, train loss:0.8160027265548706, Elapsed time for epoch : 0.3604669332504272
Epoch 3, Batch 40, train loss:0.9147266149520874, Elapsed time for epoch : 0.4765693187713623
Epoch 3, Batch 50, train loss:0.5648489594459534, Elapsed time for epoch : 0.5926597317059835
Epoch 3, Batch 60, train loss:0.7098299860954285, Elapsed time for epoch : 0.7084529995918274
Epoch 3, Batch 70, train loss:0.8001719117164612, Elapsed time for epoch : 0.8245864431063334
Epoch 3, Batch 80, train loss:0.7254258990287781, Elapsed time for epoch : 0.9405022541681926
Epoch 3, Batch 90, train loss:0.7544225454330444, Elapsed time for epoch : 1.056536853313446
Epoch 3, Batch 100, train loss:0.46796688437461853, Elapsed time for epoch : 1.1727550268173217
Epoch 3, Batch 110, train loss:0.5883030295372009, Elapsed time for epoch : 1.288963222503662
Batch 0, val loss:7.046792507171631
Batch 10, val loss:14.075848579406738
Batch 20, val loss:21.367136001586914
Batch 30, val loss:2.181226968765259
Epoch 3, Train Loss:0.7315036885116412, Val loss:9.159714149104225
Epoch 4, Batch 0, train loss:0.6003488302230835, Elapsed time for epoch : 0.011830361684163411
Epoch 4, Batch 10, train loss:0.4067566692829132, Elapsed time for epoch : 0.12807572682698568
Epoch 4, Batch 20, train loss:0.4122988283634186, Elapsed time for epoch : 0.2443254828453064
Epoch 4, Batch 30, train loss:0.603821337223053, Elapsed time for epoch : 0.3606865207354228
Epoch 4, Batch 40, train loss:0.6273613572120667, Elapsed time for epoch : 0.47682795524597166
Epoch 4, Batch 50, train loss:0.5856834650039673, Elapsed time for epoch : 0.5932235201199849
Epoch 4, Batch 60, train loss:0.6285529732704163, Elapsed time for epoch : 0.7095625122388204
Epoch 4, Batch 70, train loss:0.5844739675521851, Elapsed time for epoch : 0.825845185915629
Epoch 4, Batch 80, train loss:0.39906612038612366, Elapsed time for epoch : 0.9419516921043396
Epoch 4, Batch 90, train loss:0.5938057899475098, Elapsed time for epoch : 1.0586055080095926
Epoch 4, Batch 100, train loss:0.5620409250259399, Elapsed time for epoch : 1.175131865342458
Epoch 4, Batch 110, train loss:0.5165095925331116, Elapsed time for epoch : 1.2916720867156983
Batch 0, val loss:1.4558929204940796
Batch 10, val loss:2.026252031326294
Batch 20, val loss:3.0076515674591064
Batch 30, val loss:3.3049919605255127
Epoch 4, Train Loss:0.5491462748983632, Val loss:6.54604314847125
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñá‚ñÇ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.54915
wandb:         Val Loss 6.54604
wandb:      train_batch 110
wandb: train_batch_loss 0.51651
wandb:        val_batch 30
wandb:   val_batch_loss 3.30499
wandb: 
wandb: üöÄ View run silvery-shadow-320 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rs8usw38
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_004922-rs8usw38/logs
Seed completed execution! 1 0.8_2
------------------------------------------------------------------
Running for seed 42 of experiment 0.8_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_005714-gw58204n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run graceful-shape-322
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gw58204n
Epoch 0, Batch 0, train loss:8.56776237487793, Elapsed time for epoch : 0.013191127777099609
Epoch 0, Batch 10, train loss:3.559582233428955, Elapsed time for epoch : 0.1290993889172872
Epoch 0, Batch 20, train loss:2.675314426422119, Elapsed time for epoch : 0.24501355091730753
Epoch 0, Batch 30, train loss:2.3998827934265137, Elapsed time for epoch : 0.3613208055496216
Epoch 0, Batch 40, train loss:2.4049360752105713, Elapsed time for epoch : 0.47765572865804035
Epoch 0, Batch 50, train loss:2.180896282196045, Elapsed time for epoch : 0.5940514326095581
Epoch 0, Batch 60, train loss:1.9719817638397217, Elapsed time for epoch : 0.7104260643323262
Epoch 0, Batch 70, train loss:1.9255770444869995, Elapsed time for epoch : 0.826503853003184
Epoch 0, Batch 80, train loss:1.7648454904556274, Elapsed time for epoch : 0.9427972634633383
Epoch 0, Batch 90, train loss:1.6992788314819336, Elapsed time for epoch : 1.0590489864349366
Epoch 0, Batch 100, train loss:1.528357744216919, Elapsed time for epoch : 1.1752724607785543
Epoch 0, Batch 110, train loss:1.3825933933258057, Elapsed time for epoch : 1.2911283095677695
Batch 0, val loss:4.860324382781982
Batch 10, val loss:6.421743869781494
Batch 20, val loss:5.992686748504639
Batch 30, val loss:8.127802848815918
Epoch 0, Train Loss:2.873920926840409, Val loss:5.108003278573354
Epoch 1, Batch 0, train loss:1.5504549741744995, Elapsed time for epoch : 0.011637194951375326
Epoch 1, Batch 10, train loss:1.5201976299285889, Elapsed time for epoch : 0.12757627964019774
Epoch 1, Batch 20, train loss:1.2673088312149048, Elapsed time for epoch : 0.2436357061068217
Epoch 1, Batch 30, train loss:1.389096975326538, Elapsed time for epoch : 0.35991783142089845
Epoch 1, Batch 40, train loss:1.3575891256332397, Elapsed time for epoch : 0.4759135365486145
Epoch 1, Batch 50, train loss:1.207114338874817, Elapsed time for epoch : 0.5923291722933451
Epoch 1, Batch 60, train loss:1.1504820585250854, Elapsed time for epoch : 0.7084923624992371
Epoch 1, Batch 70, train loss:1.198116421699524, Elapsed time for epoch : 0.8250150879224142
Epoch 1, Batch 80, train loss:1.087999701499939, Elapsed time for epoch : 0.9413287003835042
Epoch 1, Batch 90, train loss:1.071778416633606, Elapsed time for epoch : 1.0576946099599203
Epoch 1, Batch 100, train loss:0.9299827814102173, Elapsed time for epoch : 1.1742181619008383
Epoch 1, Batch 110, train loss:1.0389044284820557, Elapsed time for epoch : 1.2909534692764282
Batch 0, val loss:2.000309705734253
Batch 10, val loss:6.604417324066162
Batch 20, val loss:4.509688854217529
Batch 30, val loss:2.8324222564697266
Epoch 1, Train Loss:1.2184564409048662, Val loss:8.816828489303589
Epoch 2, Batch 0, train loss:1.0636513233184814, Elapsed time for epoch : 0.011633857091267904
Epoch 2, Batch 10, train loss:1.072441577911377, Elapsed time for epoch : 0.1279758373896281
Epoch 2, Batch 20, train loss:1.0407154560089111, Elapsed time for epoch : 0.24399284521738687
Epoch 2, Batch 30, train loss:0.9157772064208984, Elapsed time for epoch : 0.36013362805048627
Epoch 2, Batch 40, train loss:0.9665301442146301, Elapsed time for epoch : 0.47633895874023435
Epoch 2, Batch 50, train loss:1.0038787126541138, Elapsed time for epoch : 0.5925573070844015
Epoch 2, Batch 60, train loss:0.9815201163291931, Elapsed time for epoch : 0.7087122480074565
Epoch 2, Batch 70, train loss:0.6954653859138489, Elapsed time for epoch : 0.8252809007962545
Epoch 2, Batch 80, train loss:0.9449545741081238, Elapsed time for epoch : 0.941839607556661
Epoch 2, Batch 90, train loss:0.9272769093513489, Elapsed time for epoch : 1.057780416806539
Epoch 2, Batch 100, train loss:0.8940500617027283, Elapsed time for epoch : 1.1737653613090515
Epoch 2, Batch 110, train loss:0.841804563999176, Elapsed time for epoch : 1.289945920308431
Batch 0, val loss:4.791118621826172
Batch 10, val loss:3.014070749282837
Batch 20, val loss:2.168447732925415
Batch 30, val loss:14.687562942504883
Epoch 2, Train Loss:0.9121604769126229, Val loss:5.8267294185029135
Epoch 3, Batch 0, train loss:0.8407198190689087, Elapsed time for epoch : 0.011644852161407471
Epoch 3, Batch 10, train loss:0.861141562461853, Elapsed time for epoch : 0.12780932188034058
Epoch 3, Batch 20, train loss:0.8653088212013245, Elapsed time for epoch : 0.24428430398305256
Epoch 3, Batch 30, train loss:0.8160027265548706, Elapsed time for epoch : 0.3602597792943319
Epoch 3, Batch 40, train loss:0.9147266149520874, Elapsed time for epoch : 0.4763172745704651
Epoch 3, Batch 50, train loss:0.5648489594459534, Elapsed time for epoch : 0.5922512173652649
Epoch 3, Batch 60, train loss:0.7098299860954285, Elapsed time for epoch : 0.708810559908549
Epoch 3, Batch 70, train loss:0.8001719117164612, Elapsed time for epoch : 0.8252673467000325
Epoch 3, Batch 80, train loss:0.7254258990287781, Elapsed time for epoch : 0.9411482055981953
Epoch 3, Batch 90, train loss:0.7544225454330444, Elapsed time for epoch : 1.0576625386873881
Epoch 3, Batch 100, train loss:0.46796688437461853, Elapsed time for epoch : 1.1738909324010214
Epoch 3, Batch 110, train loss:0.5883030295372009, Elapsed time for epoch : 1.2900012334187825
Batch 0, val loss:7.046792507171631
Batch 10, val loss:14.075848579406738
Batch 20, val loss:21.367136001586914
Batch 30, val loss:2.181226968765259
Epoch 3, Train Loss:0.7315036885116412, Val loss:9.159714149104225
Epoch 4, Batch 0, train loss:0.6003488302230835, Elapsed time for epoch : 0.011660416920979818
Epoch 4, Batch 10, train loss:0.4067566692829132, Elapsed time for epoch : 0.1277599811553955
Epoch 4, Batch 20, train loss:0.4122988283634186, Elapsed time for epoch : 0.24422199328740438
Epoch 4, Batch 30, train loss:0.603821337223053, Elapsed time for epoch : 0.36030819416046145
Epoch 4, Batch 40, train loss:0.6273613572120667, Elapsed time for epoch : 0.476235834757487
Epoch 4, Batch 50, train loss:0.5856834650039673, Elapsed time for epoch : 0.5925036787986755
Epoch 4, Batch 60, train loss:0.6285529732704163, Elapsed time for epoch : 0.7087001840273539
Epoch 4, Batch 70, train loss:0.5844739675521851, Elapsed time for epoch : 0.824997075398763
Epoch 4, Batch 80, train loss:0.39906612038612366, Elapsed time for epoch : 0.9414025545120239
Epoch 4, Batch 90, train loss:0.5938057899475098, Elapsed time for epoch : 1.0576446731885274
Epoch 4, Batch 100, train loss:0.5620409250259399, Elapsed time for epoch : 1.1738625129063924
Epoch 4, Batch 110, train loss:0.5165095925331116, Elapsed time for epoch : 1.2901677131652831
Batch 0, val loss:1.4558929204940796
Batch 10, val loss:2.026252031326294
Batch 20, val loss:3.0076515674591064
Batch 30, val loss:3.3049919605255127
Epoch 4, Train Loss:0.5491462748983632, Val loss:6.54604314847125
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñá‚ñÇ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.54915
wandb:         Val Loss 6.54604
wandb:      train_batch 110
wandb: train_batch_loss 0.51651
wandb:        val_batch 30
wandb:   val_batch_loss 3.30499
wandb: 
wandb: üöÄ View run graceful-shape-322 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gw58204n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_005714-gw58204n/logs
Seed completed execution! 42 0.8_2
------------------------------------------------------------------
Running for seed 89 of experiment 0.8_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_010500-68zufgni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-puddle-324
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/68zufgni
Epoch 0, Batch 0, train loss:8.56776237487793, Elapsed time for epoch : 0.012143973509470623
Epoch 0, Batch 10, train loss:3.559582233428955, Elapsed time for epoch : 0.12739696502685546
Epoch 0, Batch 20, train loss:2.675314426422119, Elapsed time for epoch : 0.24316576321919758
Epoch 0, Batch 30, train loss:2.3998827934265137, Elapsed time for epoch : 0.3588855266571045
Epoch 0, Batch 40, train loss:2.4049360752105713, Elapsed time for epoch : 0.4746204137802124
Epoch 0, Batch 50, train loss:2.180896282196045, Elapsed time for epoch : 0.5904479622840881
Epoch 0, Batch 60, train loss:1.9719817638397217, Elapsed time for epoch : 0.7059075474739075
Epoch 0, Batch 70, train loss:1.9255770444869995, Elapsed time for epoch : 0.8221703410148621
Epoch 0, Batch 80, train loss:1.7648454904556274, Elapsed time for epoch : 0.9380083123842875
Epoch 0, Batch 90, train loss:1.6992788314819336, Elapsed time for epoch : 1.053995410601298
Epoch 0, Batch 100, train loss:1.528357744216919, Elapsed time for epoch : 1.1697517077128092
Epoch 0, Batch 110, train loss:1.3825933933258057, Elapsed time for epoch : 1.2859230875968932
Batch 0, val loss:4.860324382781982
Batch 10, val loss:6.421743869781494
Batch 20, val loss:5.992686748504639
Batch 30, val loss:8.127802848815918
Epoch 0, Train Loss:2.873920926840409, Val loss:5.108003278573354
Epoch 1, Batch 0, train loss:1.5504549741744995, Elapsed time for epoch : 0.01167906125386556
Epoch 1, Batch 10, train loss:1.5201976299285889, Elapsed time for epoch : 0.12781281471252443
Epoch 1, Batch 20, train loss:1.2673088312149048, Elapsed time for epoch : 0.24410942395528157
Epoch 1, Batch 30, train loss:1.389096975326538, Elapsed time for epoch : 0.3605044364929199
Epoch 1, Batch 40, train loss:1.3575891256332397, Elapsed time for epoch : 0.4767747481664022
Epoch 1, Batch 50, train loss:1.207114338874817, Elapsed time for epoch : 0.5930174946784973
Epoch 1, Batch 60, train loss:1.1504820585250854, Elapsed time for epoch : 0.7092628240585327
Epoch 1, Batch 70, train loss:1.198116421699524, Elapsed time for epoch : 0.8253913283348083
Epoch 1, Batch 80, train loss:1.087999701499939, Elapsed time for epoch : 0.9414472063382466
Epoch 1, Batch 90, train loss:1.071778416633606, Elapsed time for epoch : 1.0577630201975505
Epoch 1, Batch 100, train loss:0.9299827814102173, Elapsed time for epoch : 1.1738307078679402
Epoch 1, Batch 110, train loss:1.0389044284820557, Elapsed time for epoch : 1.2899453560511271
Batch 0, val loss:2.000309705734253
Batch 10, val loss:6.604417324066162
Batch 20, val loss:4.509688854217529
Batch 30, val loss:2.8324222564697266
Epoch 1, Train Loss:1.2184564409048662, Val loss:8.816828489303589
Epoch 2, Batch 0, train loss:1.0636513233184814, Elapsed time for epoch : 0.011828386783599853
Epoch 2, Batch 10, train loss:1.072441577911377, Elapsed time for epoch : 0.1281620184580485
Epoch 2, Batch 20, train loss:1.0407154560089111, Elapsed time for epoch : 0.24440929492314656
Epoch 2, Batch 30, train loss:0.9157772064208984, Elapsed time for epoch : 0.36051651239395144
Epoch 2, Batch 40, train loss:0.9665301442146301, Elapsed time for epoch : 0.47676788965861
Epoch 2, Batch 50, train loss:1.0038787126541138, Elapsed time for epoch : 0.5930379947026571
Epoch 2, Batch 60, train loss:0.9815201163291931, Elapsed time for epoch : 0.7092633366584777
Epoch 2, Batch 70, train loss:0.6954653859138489, Elapsed time for epoch : 0.8251819610595703
Epoch 2, Batch 80, train loss:0.9449545741081238, Elapsed time for epoch : 0.9413761377334595
Epoch 2, Batch 90, train loss:0.9272769093513489, Elapsed time for epoch : 1.0575170040130615
Epoch 2, Batch 100, train loss:0.8940500617027283, Elapsed time for epoch : 1.1734193762143452
Epoch 2, Batch 110, train loss:0.841804563999176, Elapsed time for epoch : 1.28944540421168
Batch 0, val loss:4.791118621826172
Batch 10, val loss:3.014070749282837
Batch 20, val loss:2.168447732925415
Batch 30, val loss:14.687562942504883
Epoch 2, Train Loss:0.9121604769126229, Val loss:5.8267294185029135
Epoch 3, Batch 0, train loss:0.8407198190689087, Elapsed time for epoch : 0.011626354853312175
Epoch 3, Batch 10, train loss:0.861141562461853, Elapsed time for epoch : 0.12816540400187174
Epoch 3, Batch 20, train loss:0.8653088212013245, Elapsed time for epoch : 0.24429749250411986
Epoch 3, Batch 30, train loss:0.8160027265548706, Elapsed time for epoch : 0.36030354102452594
Epoch 3, Batch 40, train loss:0.9147266149520874, Elapsed time for epoch : 0.47669047117233276
Epoch 3, Batch 50, train loss:0.5648489594459534, Elapsed time for epoch : 0.5929002285003662
Epoch 3, Batch 60, train loss:0.7098299860954285, Elapsed time for epoch : 0.7090301791826884
Epoch 3, Batch 70, train loss:0.8001719117164612, Elapsed time for epoch : 0.8252627571423848
Epoch 3, Batch 80, train loss:0.7254258990287781, Elapsed time for epoch : 0.9417663971583049
Epoch 3, Batch 90, train loss:0.7544225454330444, Elapsed time for epoch : 1.0578348557154338
Epoch 3, Batch 100, train loss:0.46796688437461853, Elapsed time for epoch : 1.1736851294835409
Epoch 3, Batch 110, train loss:0.5883030295372009, Elapsed time for epoch : 1.2897448817888895
Batch 0, val loss:7.046792507171631
Batch 10, val loss:14.075848579406738
Batch 20, val loss:21.367136001586914
Batch 30, val loss:2.181226968765259
Epoch 3, Train Loss:0.7315036885116412, Val loss:9.159714149104225
Epoch 4, Batch 0, train loss:0.6003488302230835, Elapsed time for epoch : 0.011619345347086588
Epoch 4, Batch 10, train loss:0.4067566692829132, Elapsed time for epoch : 0.127936323483785
Epoch 4, Batch 20, train loss:0.4122988283634186, Elapsed time for epoch : 0.24421955347061158
Epoch 4, Batch 30, train loss:0.603821337223053, Elapsed time for epoch : 0.36007874409357704
Epoch 4, Batch 40, train loss:0.6273613572120667, Elapsed time for epoch : 0.47596882581710814
Epoch 4, Batch 50, train loss:0.5856834650039673, Elapsed time for epoch : 0.5921916524569194
Epoch 4, Batch 60, train loss:0.6285529732704163, Elapsed time for epoch : 0.7083444794019064
Epoch 4, Batch 70, train loss:0.5844739675521851, Elapsed time for epoch : 0.8243850350379944
Epoch 4, Batch 80, train loss:0.39906612038612366, Elapsed time for epoch : 0.940337081750234
Epoch 4, Batch 90, train loss:0.5938057899475098, Elapsed time for epoch : 1.0560796499252318
Epoch 4, Batch 100, train loss:0.5620409250259399, Elapsed time for epoch : 1.1723882635434468
Epoch 4, Batch 110, train loss:0.5165095925331116, Elapsed time for epoch : 1.2892242789268493
Batch 0, val loss:1.4558929204940796
Batch 10, val loss:2.026252031326294
Batch 20, val loss:3.0076515674591064
Batch 30, val loss:3.3049919605255127
Epoch 4, Train Loss:0.5491462748983632, Val loss:6.54604314847125
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñá‚ñÇ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.54915
wandb:         Val Loss 6.54604
wandb:      train_batch 110
wandb: train_batch_loss 0.51651
wandb:        val_batch 30
wandb:   val_batch_loss 3.30499
wandb: 
wandb: üöÄ View run confused-puddle-324 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/68zufgni
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_010500-68zufgni/logs
Seed completed execution! 89 0.8_2
------------------------------------------------------------------
Running for seed 23 of experiment 0.8_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_011246-u68bukc8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-bush-326
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/u68bukc8
Epoch 0, Batch 0, train loss:8.56776237487793, Elapsed time for epoch : 0.011994520823160807
Epoch 0, Batch 10, train loss:3.559582233428955, Elapsed time for epoch : 0.1287141760190328
Epoch 0, Batch 20, train loss:2.675314426422119, Elapsed time for epoch : 0.24471803506215414
Epoch 0, Batch 30, train loss:2.3998827934265137, Elapsed time for epoch : 0.3608507474263509
Epoch 0, Batch 40, train loss:2.4049360752105713, Elapsed time for epoch : 0.47681719064712524
Epoch 0, Batch 50, train loss:2.180896282196045, Elapsed time for epoch : 0.593513822555542
Epoch 0, Batch 60, train loss:1.9719817638397217, Elapsed time for epoch : 0.7094788630803426
Epoch 0, Batch 70, train loss:1.9255770444869995, Elapsed time for epoch : 0.8257483959197998
Epoch 0, Batch 80, train loss:1.7648454904556274, Elapsed time for epoch : 0.9421932816505432
Epoch 0, Batch 90, train loss:1.6992788314819336, Elapsed time for epoch : 1.0581974943478902
Epoch 0, Batch 100, train loss:1.528357744216919, Elapsed time for epoch : 1.1745787858963013
Epoch 0, Batch 110, train loss:1.3825933933258057, Elapsed time for epoch : 1.2907313426335654
Batch 0, val loss:4.860324382781982
Batch 10, val loss:6.421743869781494
Batch 20, val loss:5.992686748504639
Batch 30, val loss:8.127802848815918
Epoch 0, Train Loss:2.873920926840409, Val loss:5.108003278573354
Epoch 1, Batch 0, train loss:1.5504549741744995, Elapsed time for epoch : 0.01184984048207601
Epoch 1, Batch 10, train loss:1.5201976299285889, Elapsed time for epoch : 0.128241757551829
Epoch 1, Batch 20, train loss:1.2673088312149048, Elapsed time for epoch : 0.2446359912554423
Epoch 1, Batch 30, train loss:1.389096975326538, Elapsed time for epoch : 0.3610184113184611
Epoch 1, Batch 40, train loss:1.3575891256332397, Elapsed time for epoch : 0.477633277575175
Epoch 1, Batch 50, train loss:1.207114338874817, Elapsed time for epoch : 0.593928066889445
Epoch 1, Batch 60, train loss:1.1504820585250854, Elapsed time for epoch : 0.7103569626808166
Epoch 1, Batch 70, train loss:1.198116421699524, Elapsed time for epoch : 0.8268977363904317
Epoch 1, Batch 80, train loss:1.087999701499939, Elapsed time for epoch : 0.9438812494277954
Epoch 1, Batch 90, train loss:1.071778416633606, Elapsed time for epoch : 1.060133421421051
Epoch 1, Batch 100, train loss:0.9299827814102173, Elapsed time for epoch : 1.1765764196713766
Epoch 1, Batch 110, train loss:1.0389044284820557, Elapsed time for epoch : 1.2927236080169677
Batch 0, val loss:2.000309705734253
Batch 10, val loss:6.604417324066162
Batch 20, val loss:4.509688854217529
Batch 30, val loss:2.8324222564697266
Epoch 1, Train Loss:1.2184564409048662, Val loss:8.816828489303589
Epoch 2, Batch 0, train loss:1.0636513233184814, Elapsed time for epoch : 0.011642662684122722
Epoch 2, Batch 10, train loss:1.072441577911377, Elapsed time for epoch : 0.12775593996047974
Epoch 2, Batch 20, train loss:1.0407154560089111, Elapsed time for epoch : 0.24385311206181845
Epoch 2, Batch 30, train loss:0.9157772064208984, Elapsed time for epoch : 0.36009828646977743
Epoch 2, Batch 40, train loss:0.9665301442146301, Elapsed time for epoch : 0.476936403910319
Epoch 2, Batch 50, train loss:1.0038787126541138, Elapsed time for epoch : 0.5933300296465556
Epoch 2, Batch 60, train loss:0.9815201163291931, Elapsed time for epoch : 0.709436293443044
Epoch 2, Batch 70, train loss:0.6954653859138489, Elapsed time for epoch : 0.8253126740455627
Epoch 2, Batch 80, train loss:0.9449545741081238, Elapsed time for epoch : 0.9412280599276225
Epoch 2, Batch 90, train loss:0.9272769093513489, Elapsed time for epoch : 1.0570570627848308
Epoch 2, Batch 100, train loss:0.8940500617027283, Elapsed time for epoch : 1.1730534195899964
Epoch 2, Batch 110, train loss:0.841804563999176, Elapsed time for epoch : 1.2891477028528848
Batch 0, val loss:4.791118621826172
Batch 10, val loss:3.014070749282837
Batch 20, val loss:2.168447732925415
Batch 30, val loss:14.687562942504883
Epoch 2, Train Loss:0.9121604769126229, Val loss:5.8267294185029135
Epoch 3, Batch 0, train loss:0.8407198190689087, Elapsed time for epoch : 0.011742552121480307
Epoch 3, Batch 10, train loss:0.861141562461853, Elapsed time for epoch : 0.128224782148997
Epoch 3, Batch 20, train loss:0.8653088212013245, Elapsed time for epoch : 0.24493470589319866
Epoch 3, Batch 30, train loss:0.8160027265548706, Elapsed time for epoch : 0.361098317305247
Epoch 3, Batch 40, train loss:0.9147266149520874, Elapsed time for epoch : 0.47727676630020144
Epoch 3, Batch 50, train loss:0.5648489594459534, Elapsed time for epoch : 0.5936890959739685
Epoch 3, Batch 60, train loss:0.7098299860954285, Elapsed time for epoch : 0.7107624252637227
Epoch 3, Batch 70, train loss:0.8001719117164612, Elapsed time for epoch : 0.8283383051554362
Epoch 3, Batch 80, train loss:0.7254258990287781, Elapsed time for epoch : 0.9463068246841431
Epoch 3, Batch 90, train loss:0.7544225454330444, Elapsed time for epoch : 1.062605325380961
Epoch 3, Batch 100, train loss:0.46796688437461853, Elapsed time for epoch : 1.178975490729014
Epoch 3, Batch 110, train loss:0.5883030295372009, Elapsed time for epoch : 1.2954255978266398
Batch 0, val loss:7.046792507171631
Batch 10, val loss:14.075848579406738
Batch 20, val loss:21.367136001586914
Batch 30, val loss:2.181226968765259
Epoch 3, Train Loss:0.7315036885116412, Val loss:9.159714149104225
Epoch 4, Batch 0, train loss:0.6003488302230835, Elapsed time for epoch : 0.011751969655354818
Epoch 4, Batch 10, train loss:0.4067566692829132, Elapsed time for epoch : 0.12927501201629638
Epoch 4, Batch 20, train loss:0.4122988283634186, Elapsed time for epoch : 0.24703439076741537
Epoch 4, Batch 30, train loss:0.603821337223053, Elapsed time for epoch : 0.36384704113006594
Epoch 4, Batch 40, train loss:0.6273613572120667, Elapsed time for epoch : 0.48169012864430744
Epoch 4, Batch 50, train loss:0.5856834650039673, Elapsed time for epoch : 0.5998437841733296
Epoch 4, Batch 60, train loss:0.6285529732704163, Elapsed time for epoch : 0.7167937994003296
Epoch 4, Batch 70, train loss:0.5844739675521851, Elapsed time for epoch : 0.8340460260709127
Epoch 4, Batch 80, train loss:0.39906612038612366, Elapsed time for epoch : 0.9520910461743672
Epoch 4, Batch 90, train loss:0.5938057899475098, Elapsed time for epoch : 1.0699231505393982
Epoch 4, Batch 100, train loss:0.5620409250259399, Elapsed time for epoch : 1.1871344725290933
Epoch 4, Batch 110, train loss:0.5165095925331116, Elapsed time for epoch : 1.3049190163612365
Batch 0, val loss:1.4558929204940796
Batch 10, val loss:2.026252031326294
Batch 20, val loss:3.0076515674591064
Batch 30, val loss:3.3049919605255127
Epoch 4, Train Loss:0.5491462748983632, Val loss:6.54604314847125
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñá‚ñÇ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.54915
wandb:         Val Loss 6.54604
wandb:      train_batch 110
wandb: train_batch_loss 0.51651
wandb:        val_batch 30
wandb:   val_batch_loss 3.30499
wandb: 
wandb: üöÄ View run expert-bush-326 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/u68bukc8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_011246-u68bukc8/logs
Seed completed execution! 23 0.8_2
------------------------------------------------------------------
Running for seed 113 of experiment 0.8_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_012034-x6kocwue
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-silence-328
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/x6kocwue
Epoch 0, Batch 0, train loss:8.56776237487793, Elapsed time for epoch : 0.012307767073313396
Epoch 0, Batch 10, train loss:3.559582233428955, Elapsed time for epoch : 0.1283270557721456
Epoch 0, Batch 20, train loss:2.675314426422119, Elapsed time for epoch : 0.2441551685333252
Epoch 0, Batch 30, train loss:2.3998827934265137, Elapsed time for epoch : 0.36054216225941976
Epoch 0, Batch 40, train loss:2.4049360752105713, Elapsed time for epoch : 0.4765055020650228
Epoch 0, Batch 50, train loss:2.180896282196045, Elapsed time for epoch : 0.5932277361551921
Epoch 0, Batch 60, train loss:1.9719817638397217, Elapsed time for epoch : 0.7103248755137126
Epoch 0, Batch 70, train loss:1.9255770444869995, Elapsed time for epoch : 0.8268706838289896
Epoch 0, Batch 80, train loss:1.7648454904556274, Elapsed time for epoch : 0.9434970219930013
Epoch 0, Batch 90, train loss:1.6992788314819336, Elapsed time for epoch : 1.0597965717315674
Epoch 0, Batch 100, train loss:1.528357744216919, Elapsed time for epoch : 1.1760860840479532
Epoch 0, Batch 110, train loss:1.3825933933258057, Elapsed time for epoch : 1.292047675450643
Batch 0, val loss:4.860324382781982
Batch 10, val loss:6.421743869781494
Batch 20, val loss:5.992686748504639
Batch 30, val loss:8.127802848815918
Epoch 0, Train Loss:2.873920926840409, Val loss:5.108003278573354
Epoch 1, Batch 0, train loss:1.5504549741744995, Elapsed time for epoch : 0.01171946922938029
Epoch 1, Batch 10, train loss:1.5201976299285889, Elapsed time for epoch : 0.12844343185424806
Epoch 1, Batch 20, train loss:1.2673088312149048, Elapsed time for epoch : 0.24473309516906738
Epoch 1, Batch 30, train loss:1.389096975326538, Elapsed time for epoch : 0.36080021460851036
Epoch 1, Batch 40, train loss:1.3575891256332397, Elapsed time for epoch : 0.4769339919090271
Epoch 1, Batch 50, train loss:1.207114338874817, Elapsed time for epoch : 0.5933351079622905
Epoch 1, Batch 60, train loss:1.1504820585250854, Elapsed time for epoch : 0.7098417123158772
Epoch 1, Batch 70, train loss:1.198116421699524, Elapsed time for epoch : 0.8260046283404032
Epoch 1, Batch 80, train loss:1.087999701499939, Elapsed time for epoch : 0.9422246098518372
Epoch 1, Batch 90, train loss:1.071778416633606, Elapsed time for epoch : 1.058370021979014
Epoch 1, Batch 100, train loss:0.9299827814102173, Elapsed time for epoch : 1.174493932723999
Epoch 1, Batch 110, train loss:1.0389044284820557, Elapsed time for epoch : 1.2907649795214335
Batch 0, val loss:2.000309705734253
Batch 10, val loss:6.604417324066162
Batch 20, val loss:4.509688854217529
Batch 30, val loss:2.8324222564697266
Epoch 1, Train Loss:1.2184564409048662, Val loss:8.816828489303589
Epoch 2, Batch 0, train loss:1.0636513233184814, Elapsed time for epoch : 0.011651055018107096
Epoch 2, Batch 10, train loss:1.072441577911377, Elapsed time for epoch : 0.12781176169713337
Epoch 2, Batch 20, train loss:1.0407154560089111, Elapsed time for epoch : 0.2440427303314209
Epoch 2, Batch 30, train loss:0.9157772064208984, Elapsed time for epoch : 0.36008219718933104
Epoch 2, Batch 40, train loss:0.9665301442146301, Elapsed time for epoch : 0.47641664346059165
Epoch 2, Batch 50, train loss:1.0038787126541138, Elapsed time for epoch : 0.5928155263264974
Epoch 2, Batch 60, train loss:0.9815201163291931, Elapsed time for epoch : 0.7089981317520142
Epoch 2, Batch 70, train loss:0.6954653859138489, Elapsed time for epoch : 0.8250716050465902
Epoch 2, Batch 80, train loss:0.9449545741081238, Elapsed time for epoch : 0.9416022578875224
Epoch 2, Batch 90, train loss:0.9272769093513489, Elapsed time for epoch : 1.057816243171692
Epoch 2, Batch 100, train loss:0.8940500617027283, Elapsed time for epoch : 1.1738786657651266
Epoch 2, Batch 110, train loss:0.841804563999176, Elapsed time for epoch : 1.2898534059524536
Batch 0, val loss:4.791118621826172
Batch 10, val loss:3.014070749282837
Batch 20, val loss:2.168447732925415
Batch 30, val loss:14.687562942504883
Epoch 2, Train Loss:0.9121604769126229, Val loss:5.8267294185029135
Epoch 3, Batch 0, train loss:0.8407198190689087, Elapsed time for epoch : 0.011648492018381754
Epoch 3, Batch 10, train loss:0.861141562461853, Elapsed time for epoch : 0.1276570240656535
Epoch 3, Batch 20, train loss:0.8653088212013245, Elapsed time for epoch : 0.24377739429473877
Epoch 3, Batch 30, train loss:0.8160027265548706, Elapsed time for epoch : 0.3601244529088338
Epoch 3, Batch 40, train loss:0.9147266149520874, Elapsed time for epoch : 0.47633181015650433
Epoch 3, Batch 50, train loss:0.5648489594459534, Elapsed time for epoch : 0.5926172057787578
Epoch 3, Batch 60, train loss:0.7098299860954285, Elapsed time for epoch : 0.7089593370755514
Epoch 3, Batch 70, train loss:0.8001719117164612, Elapsed time for epoch : 0.8251039902369182
Epoch 3, Batch 80, train loss:0.7254258990287781, Elapsed time for epoch : 0.9417508800824483
Epoch 3, Batch 90, train loss:0.7544225454330444, Elapsed time for epoch : 1.058066177368164
Epoch 3, Batch 100, train loss:0.46796688437461853, Elapsed time for epoch : 1.174119226137797
Epoch 3, Batch 110, train loss:0.5883030295372009, Elapsed time for epoch : 1.29030952056249
Batch 0, val loss:7.046792507171631
Batch 10, val loss:14.075848579406738
Batch 20, val loss:21.367136001586914
Batch 30, val loss:2.181226968765259
Epoch 3, Train Loss:0.7315036885116412, Val loss:9.159714149104225
Epoch 4, Batch 0, train loss:0.6003488302230835, Elapsed time for epoch : 0.011640862623850504
Epoch 4, Batch 10, train loss:0.4067566692829132, Elapsed time for epoch : 0.1280306577682495
Epoch 4, Batch 20, train loss:0.4122988283634186, Elapsed time for epoch : 0.24378089507420858
Epoch 4, Batch 30, train loss:0.603821337223053, Elapsed time for epoch : 0.3597267150878906
Epoch 4, Batch 40, train loss:0.6273613572120667, Elapsed time for epoch : 0.4756532708803813
Epoch 4, Batch 50, train loss:0.5856834650039673, Elapsed time for epoch : 0.5917237242062886
Epoch 4, Batch 60, train loss:0.6285529732704163, Elapsed time for epoch : 0.7075490276018779
Epoch 4, Batch 70, train loss:0.5844739675521851, Elapsed time for epoch : 0.8238196889559428
Epoch 4, Batch 80, train loss:0.39906612038612366, Elapsed time for epoch : 0.9400278886159261
Epoch 4, Batch 90, train loss:0.5938057899475098, Elapsed time for epoch : 1.0563021739323935
Epoch 4, Batch 100, train loss:0.5620409250259399, Elapsed time for epoch : 1.1724249164263407
Epoch 4, Batch 110, train loss:0.5165095925331116, Elapsed time for epoch : 1.2884228984514872
Batch 0, val loss:1.4558929204940796
Batch 10, val loss:2.026252031326294
Batch 20, val loss:3.0076515674591064
Batch 30, val loss:3.3049919605255127
Epoch 4, Train Loss:0.5491462748983632, Val loss:6.54604314847125
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñá‚ñÇ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.54915
wandb:         Val Loss 6.54604
wandb:      train_batch 110
wandb: train_batch_loss 0.51651
wandb:        val_batch 30
wandb:   val_batch_loss 3.30499
wandb: 
wandb: üöÄ View run rosy-silence-328 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/x6kocwue
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_012034-x6kocwue/logs
Seed completed execution! 113 0.8_2
------------------------------------------------------------------
Experiment complete 0.8_2
==========================================================================
Running experiment for setting 0.8_3
==========================================================================
Running for seed 1 of experiment 0.8_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_012820-6i4ylygg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-music-330
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6i4ylygg
Epoch 0, Batch 0, train loss:8.50267505645752, Elapsed time for epoch : 0.01345213254292806
Epoch 0, Batch 10, train loss:3.929492950439453, Elapsed time for epoch : 0.1359971880912781
Epoch 0, Batch 20, train loss:3.2503535747528076, Elapsed time for epoch : 0.25864856243133544
Epoch 0, Batch 30, train loss:3.3138840198516846, Elapsed time for epoch : 0.384003218015035
Epoch 0, Batch 40, train loss:2.997001886367798, Elapsed time for epoch : 0.5079022089640299
Epoch 0, Batch 50, train loss:2.69911527633667, Elapsed time for epoch : 0.6322733759880066
Epoch 0, Batch 60, train loss:2.6963226795196533, Elapsed time for epoch : 0.7556744774182638
Epoch 0, Batch 70, train loss:2.711196184158325, Elapsed time for epoch : 0.8788820544878642
Epoch 0, Batch 80, train loss:2.5469298362731934, Elapsed time for epoch : 1.00317010084788
Epoch 0, Batch 90, train loss:2.154214382171631, Elapsed time for epoch : 1.1278694907824198
Epoch 0, Batch 100, train loss:2.13700795173645, Elapsed time for epoch : 1.251021138827006
Epoch 0, Batch 110, train loss:1.767049789428711, Elapsed time for epoch : 1.3741906404495239
Batch 0, val loss:3.5811450481414795
Batch 10, val loss:3.3103811740875244
Batch 20, val loss:3.4923641681671143
Batch 30, val loss:4.960843563079834
Epoch 0, Train Loss:3.060737614009691, Val loss:3.763486603895823
Epoch 1, Batch 0, train loss:1.7856874465942383, Elapsed time for epoch : 0.011829888820648194
Epoch 1, Batch 10, train loss:1.7453216314315796, Elapsed time for epoch : 0.12809661626815796
Epoch 1, Batch 20, train loss:1.4719884395599365, Elapsed time for epoch : 0.2444973627726237
Epoch 1, Batch 30, train loss:1.7005703449249268, Elapsed time for epoch : 0.36084092458089195
Epoch 1, Batch 40, train loss:1.586786150932312, Elapsed time for epoch : 0.4774689912796021
Epoch 1, Batch 50, train loss:1.4009648561477661, Elapsed time for epoch : 0.5936474363009135
Epoch 1, Batch 60, train loss:1.2984153032302856, Elapsed time for epoch : 0.710085678100586
Epoch 1, Batch 70, train loss:1.3624842166900635, Elapsed time for epoch : 0.8272376815478008
Epoch 1, Batch 80, train loss:1.3584259748458862, Elapsed time for epoch : 0.9436087846755982
Epoch 1, Batch 90, train loss:1.063966989517212, Elapsed time for epoch : 1.0599132259686788
Epoch 1, Batch 100, train loss:0.8026071190834045, Elapsed time for epoch : 1.1765319029490153
Epoch 1, Batch 110, train loss:0.9930658340454102, Elapsed time for epoch : 1.2930625716845194
Batch 0, val loss:3.0482380390167236
Batch 10, val loss:3.5402774810791016
Batch 20, val loss:4.394863605499268
Batch 30, val loss:2.968579053878784
Epoch 1, Train Loss:1.3607041654379473, Val loss:6.35633079873191
Epoch 2, Batch 0, train loss:0.9439094662666321, Elapsed time for epoch : 0.011645038922627768
Epoch 2, Batch 10, train loss:0.969389021396637, Elapsed time for epoch : 0.12773295640945434
Epoch 2, Batch 20, train loss:0.8955464959144592, Elapsed time for epoch : 0.24403573671976725
Epoch 2, Batch 30, train loss:1.0724585056304932, Elapsed time for epoch : 0.36008511384328207
Epoch 2, Batch 40, train loss:0.8552680611610413, Elapsed time for epoch : 0.4763808846473694
Epoch 2, Batch 50, train loss:0.9822867512702942, Elapsed time for epoch : 0.5924808541933696
Epoch 2, Batch 60, train loss:0.9247282147407532, Elapsed time for epoch : 0.7092087268829346
Epoch 2, Batch 70, train loss:0.5330252051353455, Elapsed time for epoch : 0.8257052620251973
Epoch 2, Batch 80, train loss:0.8274419903755188, Elapsed time for epoch : 0.9420844674110412
Epoch 2, Batch 90, train loss:0.8685148358345032, Elapsed time for epoch : 1.0585871736208599
Epoch 2, Batch 100, train loss:0.7741762399673462, Elapsed time for epoch : 1.1753727237383524
Epoch 2, Batch 110, train loss:0.7705622315406799, Elapsed time for epoch : 1.2918877204259236
Batch 0, val loss:6.083832740783691
Batch 10, val loss:3.164226531982422
Batch 20, val loss:1.8997132778167725
Batch 30, val loss:5.480319499969482
Epoch 2, Train Loss:0.8403237327285434, Val loss:7.331458323531681
Epoch 3, Batch 0, train loss:0.727228045463562, Elapsed time for epoch : 0.011746466159820557
Epoch 3, Batch 10, train loss:0.9378278851509094, Elapsed time for epoch : 0.1285050630569458
Epoch 3, Batch 20, train loss:0.7907819151878357, Elapsed time for epoch : 0.24504637320836384
Epoch 3, Batch 30, train loss:0.709861159324646, Elapsed time for epoch : 0.3617169419924418
Epoch 3, Batch 40, train loss:0.8092982172966003, Elapsed time for epoch : 0.47793211936950686
Epoch 3, Batch 50, train loss:0.3278413414955139, Elapsed time for epoch : 0.5939160625139872
Epoch 3, Batch 60, train loss:0.6629754900932312, Elapsed time for epoch : 0.7102901975313822
Epoch 3, Batch 70, train loss:0.679587185382843, Elapsed time for epoch : 0.8267277359962464
Epoch 3, Batch 80, train loss:0.7752425074577332, Elapsed time for epoch : 0.9429181973139446
Epoch 3, Batch 90, train loss:0.571168065071106, Elapsed time for epoch : 1.0593044956525166
Epoch 3, Batch 100, train loss:0.1900956928730011, Elapsed time for epoch : 1.1757583936055502
Epoch 3, Batch 110, train loss:0.49610474705696106, Elapsed time for epoch : 1.2920866409937541
Batch 0, val loss:8.303558349609375
Batch 10, val loss:17.776697158813477
Batch 20, val loss:18.68522834777832
Batch 30, val loss:3.088165521621704
Epoch 3, Train Loss:0.5858511642269466, Val loss:10.00613094204002
Epoch 4, Batch 0, train loss:0.6903799772262573, Elapsed time for epoch : 0.011609172821044922
Epoch 4, Batch 10, train loss:0.22112058103084564, Elapsed time for epoch : 0.12845612366994222
Epoch 4, Batch 20, train loss:0.19638237357139587, Elapsed time for epoch : 0.24485822916030883
Epoch 4, Batch 30, train loss:0.48365482687950134, Elapsed time for epoch : 0.36120580037434896
Epoch 4, Batch 40, train loss:0.45583459734916687, Elapsed time for epoch : 0.47757920424143474
Epoch 4, Batch 50, train loss:0.43529757857322693, Elapsed time for epoch : 0.5937587936719259
Epoch 4, Batch 60, train loss:0.3752369284629822, Elapsed time for epoch : 0.7100915908813477
Epoch 4, Batch 70, train loss:0.4187489449977875, Elapsed time for epoch : 0.8265748023986816
Epoch 4, Batch 80, train loss:0.20830167829990387, Elapsed time for epoch : 0.9425040404001872
Epoch 4, Batch 90, train loss:0.40368205308914185, Elapsed time for epoch : 1.0588707447052002
Epoch 4, Batch 100, train loss:0.4995242953300476, Elapsed time for epoch : 1.1750666141510009
Epoch 4, Batch 110, train loss:0.4225638806819916, Elapsed time for epoch : 1.2918546319007873
Batch 0, val loss:12.22327995300293
Batch 10, val loss:2.929750442504883
Batch 20, val loss:3.1831581592559814
Batch 30, val loss:2.823903799057007
Epoch 4, Train Loss:0.39271070322264795, Val loss:8.315379876229498
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.39271
wandb:         Val Loss 8.31538
wandb:      train_batch 110
wandb: train_batch_loss 0.42256
wandb:        val_batch 30
wandb:   val_batch_loss 2.8239
wandb: 
wandb: üöÄ View run vital-music-330 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6i4ylygg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_012820-6i4ylygg/logs
Seed completed execution! 1 0.8_3
------------------------------------------------------------------
Running for seed 42 of experiment 0.8_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_013614-hihv0aju
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-terrain-332
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hihv0aju
Epoch 0, Batch 0, train loss:8.50267505645752, Elapsed time for epoch : 0.01333460807800293
Epoch 0, Batch 10, train loss:3.929492950439453, Elapsed time for epoch : 0.12921862999598185
Epoch 0, Batch 20, train loss:3.2503535747528076, Elapsed time for epoch : 0.24496914943059286
Epoch 0, Batch 30, train loss:3.3138840198516846, Elapsed time for epoch : 0.36073526938756306
Epoch 0, Batch 40, train loss:2.997001886367798, Elapsed time for epoch : 0.4768400192260742
Epoch 0, Batch 50, train loss:2.69911527633667, Elapsed time for epoch : 0.5928747415542602
Epoch 0, Batch 60, train loss:2.6963226795196533, Elapsed time for epoch : 0.7087480465571085
Epoch 0, Batch 70, train loss:2.711196184158325, Elapsed time for epoch : 0.8249907453854879
Epoch 0, Batch 80, train loss:2.5469298362731934, Elapsed time for epoch : 0.9410473903020223
Epoch 0, Batch 90, train loss:2.154214382171631, Elapsed time for epoch : 1.0574300646781922
Epoch 0, Batch 100, train loss:2.13700795173645, Elapsed time for epoch : 1.1735526164372763
Epoch 0, Batch 110, train loss:1.767049789428711, Elapsed time for epoch : 1.289791218439738
Batch 0, val loss:3.5811450481414795
Batch 10, val loss:3.3103811740875244
Batch 20, val loss:3.4923641681671143
Batch 30, val loss:4.960843563079834
Epoch 0, Train Loss:3.060737614009691, Val loss:3.763486603895823
Epoch 1, Batch 0, train loss:1.7856874465942383, Elapsed time for epoch : 0.011673847834269205
Epoch 1, Batch 10, train loss:1.7453216314315796, Elapsed time for epoch : 0.1279232939084371
Epoch 1, Batch 20, train loss:1.4719884395599365, Elapsed time for epoch : 0.24455240567525227
Epoch 1, Batch 30, train loss:1.7005703449249268, Elapsed time for epoch : 0.36142100890477497
Epoch 1, Batch 40, train loss:1.586786150932312, Elapsed time for epoch : 0.47801140944163006
Epoch 1, Batch 50, train loss:1.4009648561477661, Elapsed time for epoch : 0.5949259042739868
Epoch 1, Batch 60, train loss:1.2984153032302856, Elapsed time for epoch : 0.7118778347969055
Epoch 1, Batch 70, train loss:1.3624842166900635, Elapsed time for epoch : 0.8293164054552714
Epoch 1, Batch 80, train loss:1.3584259748458862, Elapsed time for epoch : 0.9467189868291219
Epoch 1, Batch 90, train loss:1.063966989517212, Elapsed time for epoch : 1.0642687598864238
Epoch 1, Batch 100, train loss:0.8026071190834045, Elapsed time for epoch : 1.1818125208218893
Epoch 1, Batch 110, train loss:0.9930658340454102, Elapsed time for epoch : 1.2996816635131836
Batch 0, val loss:3.0482380390167236
Batch 10, val loss:3.5402774810791016
Batch 20, val loss:4.394863605499268
Batch 30, val loss:2.968579053878784
Epoch 1, Train Loss:1.3607041654379473, Val loss:6.35633079873191
Epoch 2, Batch 0, train loss:0.9439094662666321, Elapsed time for epoch : 0.011724472045898438
Epoch 2, Batch 10, train loss:0.969389021396637, Elapsed time for epoch : 0.12846599022547403
Epoch 2, Batch 20, train loss:0.8955464959144592, Elapsed time for epoch : 0.24517369270324707
Epoch 2, Batch 30, train loss:1.0724585056304932, Elapsed time for epoch : 0.36185633738835654
Epoch 2, Batch 40, train loss:0.8552680611610413, Elapsed time for epoch : 0.4787111282348633
Epoch 2, Batch 50, train loss:0.9822867512702942, Elapsed time for epoch : 0.5954562544822692
Epoch 2, Batch 60, train loss:0.9247282147407532, Elapsed time for epoch : 0.7123793760935465
Epoch 2, Batch 70, train loss:0.5330252051353455, Elapsed time for epoch : 0.8293827613194783
Epoch 2, Batch 80, train loss:0.8274419903755188, Elapsed time for epoch : 0.9460808793703716
Epoch 2, Batch 90, train loss:0.8685148358345032, Elapsed time for epoch : 1.06287921667099
Epoch 2, Batch 100, train loss:0.7741762399673462, Elapsed time for epoch : 1.1799944798151651
Epoch 2, Batch 110, train loss:0.7705622315406799, Elapsed time for epoch : 1.2965599020322165
Batch 0, val loss:6.083832740783691
Batch 10, val loss:3.164226531982422
Batch 20, val loss:1.8997132778167725
Batch 30, val loss:5.480319499969482
Epoch 2, Train Loss:0.8403237327285434, Val loss:7.331458323531681
Epoch 3, Batch 0, train loss:0.727228045463562, Elapsed time for epoch : 0.011696736017862955
Epoch 3, Batch 10, train loss:0.9378278851509094, Elapsed time for epoch : 0.12834565242131551
Epoch 3, Batch 20, train loss:0.7907819151878357, Elapsed time for epoch : 0.24508679707845052
Epoch 3, Batch 30, train loss:0.709861159324646, Elapsed time for epoch : 0.36189186573028564
Epoch 3, Batch 40, train loss:0.8092982172966003, Elapsed time for epoch : 0.4786272128423055
Epoch 3, Batch 50, train loss:0.3278413414955139, Elapsed time for epoch : 0.5950541416803996
Epoch 3, Batch 60, train loss:0.6629754900932312, Elapsed time for epoch : 0.711902129650116
Epoch 3, Batch 70, train loss:0.679587185382843, Elapsed time for epoch : 0.8293335239092509
Epoch 3, Batch 80, train loss:0.7752425074577332, Elapsed time for epoch : 0.9466227372487386
Epoch 3, Batch 90, train loss:0.571168065071106, Elapsed time for epoch : 1.064254899819692
Epoch 3, Batch 100, train loss:0.1900956928730011, Elapsed time for epoch : 1.1818761507670084
Epoch 3, Batch 110, train loss:0.49610474705696106, Elapsed time for epoch : 1.2995556314786276
Batch 0, val loss:8.303558349609375
Batch 10, val loss:17.776697158813477
Batch 20, val loss:18.68522834777832
Batch 30, val loss:3.088165521621704
Epoch 3, Train Loss:0.5858511642269466, Val loss:10.00613094204002
Epoch 4, Batch 0, train loss:0.6903799772262573, Elapsed time for epoch : 0.011652930577596029
Epoch 4, Batch 10, train loss:0.22112058103084564, Elapsed time for epoch : 0.12866787910461425
Epoch 4, Batch 20, train loss:0.19638237357139587, Elapsed time for epoch : 0.2460787018140157
Epoch 4, Batch 30, train loss:0.48365482687950134, Elapsed time for epoch : 0.36366870005925495
Epoch 4, Batch 40, train loss:0.45583459734916687, Elapsed time for epoch : 0.4807859301567078
Epoch 4, Batch 50, train loss:0.43529757857322693, Elapsed time for epoch : 0.5981532335281372
Epoch 4, Batch 60, train loss:0.3752369284629822, Elapsed time for epoch : 0.7154122948646545
Epoch 4, Batch 70, train loss:0.4187489449977875, Elapsed time for epoch : 0.8329468250274659
Epoch 4, Batch 80, train loss:0.20830167829990387, Elapsed time for epoch : 0.9505362709363302
Epoch 4, Batch 90, train loss:0.40368205308914185, Elapsed time for epoch : 1.0677550315856934
Epoch 4, Batch 100, train loss:0.4995242953300476, Elapsed time for epoch : 1.1852946798006694
Epoch 4, Batch 110, train loss:0.4225638806819916, Elapsed time for epoch : 1.3030885497728983
Batch 0, val loss:12.22327995300293
Batch 10, val loss:2.929750442504883
Batch 20, val loss:3.1831581592559814
Batch 30, val loss:2.823903799057007
Epoch 4, Train Loss:0.39271070322264795, Val loss:8.315379876229498
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.39271
wandb:         Val Loss 8.31538
wandb:      train_batch 110
wandb: train_batch_loss 0.42256
wandb:        val_batch 30
wandb:   val_batch_loss 2.8239
wandb: 
wandb: üöÄ View run dutiful-terrain-332 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hihv0aju
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_013614-hihv0aju/logs
Seed completed execution! 42 0.8_3
------------------------------------------------------------------
Running for seed 89 of experiment 0.8_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_014404-55bn0gzu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-vortex-334
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/55bn0gzu
Epoch 0, Batch 0, train loss:8.50267505645752, Elapsed time for epoch : 0.013174676895141601
Epoch 0, Batch 10, train loss:3.929492950439453, Elapsed time for epoch : 0.12908653020858765
Epoch 0, Batch 20, train loss:3.2503535747528076, Elapsed time for epoch : 0.2451790730158488
Epoch 0, Batch 30, train loss:3.3138840198516846, Elapsed time for epoch : 0.3612674713134766
Epoch 0, Batch 40, train loss:2.997001886367798, Elapsed time for epoch : 0.4776697317759196
Epoch 0, Batch 50, train loss:2.69911527633667, Elapsed time for epoch : 0.5941021203994751
Epoch 0, Batch 60, train loss:2.6963226795196533, Elapsed time for epoch : 0.7103671073913574
Epoch 0, Batch 70, train loss:2.711196184158325, Elapsed time for epoch : 0.8266885320345561
Epoch 0, Batch 80, train loss:2.5469298362731934, Elapsed time for epoch : 0.9431241114934286
Epoch 0, Batch 90, train loss:2.154214382171631, Elapsed time for epoch : 1.0597166776657105
Epoch 0, Batch 100, train loss:2.13700795173645, Elapsed time for epoch : 1.1758357763290406
Epoch 0, Batch 110, train loss:1.767049789428711, Elapsed time for epoch : 1.2920058647791544
Batch 0, val loss:3.5811450481414795
Batch 10, val loss:3.3103811740875244
Batch 20, val loss:3.4923641681671143
Batch 30, val loss:4.960843563079834
Epoch 0, Train Loss:3.060737614009691, Val loss:3.763486603895823
Epoch 1, Batch 0, train loss:1.7856874465942383, Elapsed time for epoch : 0.011709181467692058
Epoch 1, Batch 10, train loss:1.7453216314315796, Elapsed time for epoch : 0.1276651382446289
Epoch 1, Batch 20, train loss:1.4719884395599365, Elapsed time for epoch : 0.2441078503926595
Epoch 1, Batch 30, train loss:1.7005703449249268, Elapsed time for epoch : 0.3603189428647359
Epoch 1, Batch 40, train loss:1.586786150932312, Elapsed time for epoch : 0.47686622937520345
Epoch 1, Batch 50, train loss:1.4009648561477661, Elapsed time for epoch : 0.5932120720545451
Epoch 1, Batch 60, train loss:1.2984153032302856, Elapsed time for epoch : 0.7097341458002726
Epoch 1, Batch 70, train loss:1.3624842166900635, Elapsed time for epoch : 0.826809823513031
Epoch 1, Batch 80, train loss:1.3584259748458862, Elapsed time for epoch : 0.9431031068166097
Epoch 1, Batch 90, train loss:1.063966989517212, Elapsed time for epoch : 1.059962030251821
Epoch 1, Batch 100, train loss:0.8026071190834045, Elapsed time for epoch : 1.1765467961629232
Epoch 1, Batch 110, train loss:0.9930658340454102, Elapsed time for epoch : 1.2929710149765015
Batch 0, val loss:3.0482380390167236
Batch 10, val loss:3.5402774810791016
Batch 20, val loss:4.394863605499268
Batch 30, val loss:2.968579053878784
Epoch 1, Train Loss:1.3607041654379473, Val loss:6.35633079873191
Epoch 2, Batch 0, train loss:0.9439094662666321, Elapsed time for epoch : 0.011744284629821777
Epoch 2, Batch 10, train loss:0.969389021396637, Elapsed time for epoch : 0.12802749872207642
Epoch 2, Batch 20, train loss:0.8955464959144592, Elapsed time for epoch : 0.24434187412261962
Epoch 2, Batch 30, train loss:1.0724585056304932, Elapsed time for epoch : 0.36069833834966025
Epoch 2, Batch 40, train loss:0.8552680611610413, Elapsed time for epoch : 0.4771241267522176
Epoch 2, Batch 50, train loss:0.9822867512702942, Elapsed time for epoch : 0.5937626083691915
Epoch 2, Batch 60, train loss:0.9247282147407532, Elapsed time for epoch : 0.7102508862813314
Epoch 2, Batch 70, train loss:0.5330252051353455, Elapsed time for epoch : 0.8266922275225321
Epoch 2, Batch 80, train loss:0.8274419903755188, Elapsed time for epoch : 0.9430724183718363
Epoch 2, Batch 90, train loss:0.8685148358345032, Elapsed time for epoch : 1.0591366211573283
Epoch 2, Batch 100, train loss:0.7741762399673462, Elapsed time for epoch : 1.1753729820251464
Epoch 2, Batch 110, train loss:0.7705622315406799, Elapsed time for epoch : 1.2915845115979512
Batch 0, val loss:6.083832740783691
Batch 10, val loss:3.164226531982422
Batch 20, val loss:1.8997132778167725
Batch 30, val loss:5.480319499969482
Epoch 2, Train Loss:0.8403237327285434, Val loss:7.331458323531681
Epoch 3, Batch 0, train loss:0.727228045463562, Elapsed time for epoch : 0.011696275075276692
Epoch 3, Batch 10, train loss:0.9378278851509094, Elapsed time for epoch : 0.12781011660893757
Epoch 3, Batch 20, train loss:0.7907819151878357, Elapsed time for epoch : 0.2439228614171346
Epoch 3, Batch 30, train loss:0.709861159324646, Elapsed time for epoch : 0.3600913961728414
Epoch 3, Batch 40, train loss:0.8092982172966003, Elapsed time for epoch : 0.47648828426996864
Epoch 3, Batch 50, train loss:0.3278413414955139, Elapsed time for epoch : 0.5929760654767354
Epoch 3, Batch 60, train loss:0.6629754900932312, Elapsed time for epoch : 0.7094443678855896
Epoch 3, Batch 70, train loss:0.679587185382843, Elapsed time for epoch : 0.826460059483846
Epoch 3, Batch 80, train loss:0.7752425074577332, Elapsed time for epoch : 0.9431059757868449
Epoch 3, Batch 90, train loss:0.571168065071106, Elapsed time for epoch : 1.0597699364026387
Epoch 3, Batch 100, train loss:0.1900956928730011, Elapsed time for epoch : 1.1763402144114177
Epoch 3, Batch 110, train loss:0.49610474705696106, Elapsed time for epoch : 1.2924556930859883
Batch 0, val loss:8.303558349609375
Batch 10, val loss:17.776697158813477
Batch 20, val loss:18.68522834777832
Batch 30, val loss:3.088165521621704
Epoch 3, Train Loss:0.5858511642269466, Val loss:10.00613094204002
Epoch 4, Batch 0, train loss:0.6903799772262573, Elapsed time for epoch : 0.011680889129638671
Epoch 4, Batch 10, train loss:0.22112058103084564, Elapsed time for epoch : 0.1280330220858256
Epoch 4, Batch 20, train loss:0.19638237357139587, Elapsed time for epoch : 0.24472784598668415
Epoch 4, Batch 30, train loss:0.48365482687950134, Elapsed time for epoch : 0.3611627459526062
Epoch 4, Batch 40, train loss:0.45583459734916687, Elapsed time for epoch : 0.47760886351267495
Epoch 4, Batch 50, train loss:0.43529757857322693, Elapsed time for epoch : 0.5939728538195292
Epoch 4, Batch 60, train loss:0.3752369284629822, Elapsed time for epoch : 0.7101903994878133
Epoch 4, Batch 70, train loss:0.4187489449977875, Elapsed time for epoch : 0.8266440192858379
Epoch 4, Batch 80, train loss:0.20830167829990387, Elapsed time for epoch : 0.9429020166397095
Epoch 4, Batch 90, train loss:0.40368205308914185, Elapsed time for epoch : 1.0589189569155375
Epoch 4, Batch 100, train loss:0.4995242953300476, Elapsed time for epoch : 1.1753012816111246
Epoch 4, Batch 110, train loss:0.4225638806819916, Elapsed time for epoch : 1.2914230744043986
Batch 0, val loss:12.22327995300293
Batch 10, val loss:2.929750442504883
Batch 20, val loss:3.1831581592559814
Batch 30, val loss:2.823903799057007
Epoch 4, Train Loss:0.39271070322264795, Val loss:8.315379876229498
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.39271
wandb:         Val Loss 8.31538
wandb:      train_batch 110
wandb: train_batch_loss 0.42256
wandb:        val_batch 30
wandb:   val_batch_loss 2.8239
wandb: 
wandb: üöÄ View run wild-vortex-334 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/55bn0gzu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_014404-55bn0gzu/logs
Seed completed execution! 89 0.8_3
------------------------------------------------------------------
Running for seed 23 of experiment 0.8_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_015150-66qtjfem
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-jazz-336
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/66qtjfem
Epoch 0, Batch 0, train loss:8.50267505645752, Elapsed time for epoch : 0.013524997234344482
Epoch 0, Batch 10, train loss:3.929492950439453, Elapsed time for epoch : 0.13012249867121378
Epoch 0, Batch 20, train loss:3.2503535747528076, Elapsed time for epoch : 0.246122145652771
Epoch 0, Batch 30, train loss:3.3138840198516846, Elapsed time for epoch : 0.36177727381388347
Epoch 0, Batch 40, train loss:2.997001886367798, Elapsed time for epoch : 0.4779659867286682
Epoch 0, Batch 50, train loss:2.69911527633667, Elapsed time for epoch : 0.5941375811894735
Epoch 0, Batch 60, train loss:2.6963226795196533, Elapsed time for epoch : 0.7102331121762594
Epoch 0, Batch 70, train loss:2.711196184158325, Elapsed time for epoch : 0.8264852682749431
Epoch 0, Batch 80, train loss:2.5469298362731934, Elapsed time for epoch : 0.9432037274042765
Epoch 0, Batch 90, train loss:2.154214382171631, Elapsed time for epoch : 1.0593116521835326
Epoch 0, Batch 100, train loss:2.13700795173645, Elapsed time for epoch : 1.175644862651825
Epoch 0, Batch 110, train loss:1.767049789428711, Elapsed time for epoch : 1.2916686534881592
Batch 0, val loss:3.5811450481414795
Batch 10, val loss:3.3103811740875244
Batch 20, val loss:3.4923641681671143
Batch 30, val loss:4.960843563079834
Epoch 0, Train Loss:3.060737614009691, Val loss:3.763486603895823
Epoch 1, Batch 0, train loss:1.7856874465942383, Elapsed time for epoch : 0.011769429842631022
Epoch 1, Batch 10, train loss:1.7453216314315796, Elapsed time for epoch : 0.12820507685343424
Epoch 1, Batch 20, train loss:1.4719884395599365, Elapsed time for epoch : 0.24450679222742716
Epoch 1, Batch 30, train loss:1.7005703449249268, Elapsed time for epoch : 0.3609406153361003
Epoch 1, Batch 40, train loss:1.586786150932312, Elapsed time for epoch : 0.47749618291854856
Epoch 1, Batch 50, train loss:1.4009648561477661, Elapsed time for epoch : 0.5937634229660034
Epoch 1, Batch 60, train loss:1.2984153032302856, Elapsed time for epoch : 0.7103101054827372
Epoch 1, Batch 70, train loss:1.3624842166900635, Elapsed time for epoch : 0.8264184872309367
Epoch 1, Batch 80, train loss:1.3584259748458862, Elapsed time for epoch : 0.9427075107892354
Epoch 1, Batch 90, train loss:1.063966989517212, Elapsed time for epoch : 1.0590840021769206
Epoch 1, Batch 100, train loss:0.8026071190834045, Elapsed time for epoch : 1.1752137462298076
Epoch 1, Batch 110, train loss:0.9930658340454102, Elapsed time for epoch : 1.291704742113749
Batch 0, val loss:3.0482380390167236
Batch 10, val loss:3.5402774810791016
Batch 20, val loss:4.394863605499268
Batch 30, val loss:2.968579053878784
Epoch 1, Train Loss:1.3607041654379473, Val loss:6.35633079873191
Epoch 2, Batch 0, train loss:0.9439094662666321, Elapsed time for epoch : 0.011709384123484294
Epoch 2, Batch 10, train loss:0.969389021396637, Elapsed time for epoch : 0.12794922987620036
Epoch 2, Batch 20, train loss:0.8955464959144592, Elapsed time for epoch : 0.2445671598116557
Epoch 2, Batch 30, train loss:1.0724585056304932, Elapsed time for epoch : 0.3605703353881836
Epoch 2, Batch 40, train loss:0.8552680611610413, Elapsed time for epoch : 0.47694462140401206
Epoch 2, Batch 50, train loss:0.9822867512702942, Elapsed time for epoch : 0.5934046347935994
Epoch 2, Batch 60, train loss:0.9247282147407532, Elapsed time for epoch : 0.7096880992253621
Epoch 2, Batch 70, train loss:0.5330252051353455, Elapsed time for epoch : 0.8262059450149536
Epoch 2, Batch 80, train loss:0.8274419903755188, Elapsed time for epoch : 0.9426440397898356
Epoch 2, Batch 90, train loss:0.8685148358345032, Elapsed time for epoch : 1.0589375972747803
Epoch 2, Batch 100, train loss:0.7741762399673462, Elapsed time for epoch : 1.1748996218045553
Epoch 2, Batch 110, train loss:0.7705622315406799, Elapsed time for epoch : 1.2910398880640666
Batch 0, val loss:6.083832740783691
Batch 10, val loss:3.164226531982422
Batch 20, val loss:1.8997132778167725
Batch 30, val loss:5.480319499969482
Epoch 2, Train Loss:0.8403237327285434, Val loss:7.331458323531681
Epoch 3, Batch 0, train loss:0.727228045463562, Elapsed time for epoch : 0.011725858847300211
Epoch 3, Batch 10, train loss:0.9378278851509094, Elapsed time for epoch : 0.12821165720621744
Epoch 3, Batch 20, train loss:0.7907819151878357, Elapsed time for epoch : 0.24497478405634562
Epoch 3, Batch 30, train loss:0.709861159324646, Elapsed time for epoch : 0.36180551449457804
Epoch 3, Batch 40, train loss:0.8092982172966003, Elapsed time for epoch : 0.4783351977666219
Epoch 3, Batch 50, train loss:0.3278413414955139, Elapsed time for epoch : 0.594594399134318
Epoch 3, Batch 60, train loss:0.6629754900932312, Elapsed time for epoch : 0.7114513874053955
Epoch 3, Batch 70, train loss:0.679587185382843, Elapsed time for epoch : 0.8280120333035786
Epoch 3, Batch 80, train loss:0.7752425074577332, Elapsed time for epoch : 0.9446111957232157
Epoch 3, Batch 90, train loss:0.571168065071106, Elapsed time for epoch : 1.0614773591359457
Epoch 3, Batch 100, train loss:0.1900956928730011, Elapsed time for epoch : 1.1784813046455382
Epoch 3, Batch 110, train loss:0.49610474705696106, Elapsed time for epoch : 1.2948984543482462
Batch 0, val loss:8.303558349609375
Batch 10, val loss:17.776697158813477
Batch 20, val loss:18.68522834777832
Batch 30, val loss:3.088165521621704
Epoch 3, Train Loss:0.5858511642269466, Val loss:10.00613094204002
Epoch 4, Batch 0, train loss:0.6903799772262573, Elapsed time for epoch : 0.011773741245269776
Epoch 4, Batch 10, train loss:0.22112058103084564, Elapsed time for epoch : 0.13018424113591512
Epoch 4, Batch 20, train loss:0.19638237357139587, Elapsed time for epoch : 0.24751584927241008
Epoch 4, Batch 30, train loss:0.48365482687950134, Elapsed time for epoch : 0.36505347887674966
Epoch 4, Batch 40, train loss:0.45583459734916687, Elapsed time for epoch : 0.4827317555745443
Epoch 4, Batch 50, train loss:0.43529757857322693, Elapsed time for epoch : 0.6009506742159526
Epoch 4, Batch 60, train loss:0.3752369284629822, Elapsed time for epoch : 0.7183473110198975
Epoch 4, Batch 70, train loss:0.4187489449977875, Elapsed time for epoch : 0.8361064791679382
Epoch 4, Batch 80, train loss:0.20830167829990387, Elapsed time for epoch : 0.9538671294848124
Epoch 4, Batch 90, train loss:0.40368205308914185, Elapsed time for epoch : 1.0713751673698426
Epoch 4, Batch 100, train loss:0.4995242953300476, Elapsed time for epoch : 1.188908608754476
Epoch 4, Batch 110, train loss:0.4225638806819916, Elapsed time for epoch : 1.3062159140904746
Batch 0, val loss:12.22327995300293
Batch 10, val loss:2.929750442504883
Batch 20, val loss:3.1831581592559814
Batch 30, val loss:2.823903799057007
Epoch 4, Train Loss:0.39271070322264795, Val loss:8.315379876229498
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.39271
wandb:         Val Loss 8.31538
wandb:      train_batch 110
wandb: train_batch_loss 0.42256
wandb:        val_batch 30
wandb:   val_batch_loss 2.8239
wandb: 
wandb: üöÄ View run daily-jazz-336 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/66qtjfem
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_015150-66qtjfem/logs
Seed completed execution! 23 0.8_3
------------------------------------------------------------------
Running for seed 113 of experiment 0.8_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_015938-t1yp6jfs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-haze-338
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/t1yp6jfs
Epoch 0, Batch 0, train loss:8.50267505645752, Elapsed time for epoch : 0.013277562459309895
Epoch 0, Batch 10, train loss:3.929492950439453, Elapsed time for epoch : 0.1291393200556437
Epoch 0, Batch 20, train loss:3.2503535747528076, Elapsed time for epoch : 0.24549583594004312
Epoch 0, Batch 30, train loss:3.3138840198516846, Elapsed time for epoch : 0.36155498822530113
Epoch 0, Batch 40, train loss:2.997001886367798, Elapsed time for epoch : 0.4773572285970052
Epoch 0, Batch 50, train loss:2.69911527633667, Elapsed time for epoch : 0.5933818419774374
Epoch 0, Batch 60, train loss:2.6963226795196533, Elapsed time for epoch : 0.7092210451761881
Epoch 0, Batch 70, train loss:2.711196184158325, Elapsed time for epoch : 0.8251720825831096
Epoch 0, Batch 80, train loss:2.5469298362731934, Elapsed time for epoch : 0.941330619653066
Epoch 0, Batch 90, train loss:2.154214382171631, Elapsed time for epoch : 1.0579556902249654
Epoch 0, Batch 100, train loss:2.13700795173645, Elapsed time for epoch : 1.1738606651624044
Epoch 0, Batch 110, train loss:1.767049789428711, Elapsed time for epoch : 1.2900891741116842
Batch 0, val loss:3.5811450481414795
Batch 10, val loss:3.3103811740875244
Batch 20, val loss:3.4923641681671143
Batch 30, val loss:4.960843563079834
Epoch 0, Train Loss:3.060737614009691, Val loss:3.763486603895823
Epoch 1, Batch 0, train loss:1.7856874465942383, Elapsed time for epoch : 0.011700340112050374
Epoch 1, Batch 10, train loss:1.7453216314315796, Elapsed time for epoch : 0.1275671362876892
Epoch 1, Batch 20, train loss:1.4719884395599365, Elapsed time for epoch : 0.2441030263900757
Epoch 1, Batch 30, train loss:1.7005703449249268, Elapsed time for epoch : 0.36030883391698204
Epoch 1, Batch 40, train loss:1.586786150932312, Elapsed time for epoch : 0.4764905055363973
Epoch 1, Batch 50, train loss:1.4009648561477661, Elapsed time for epoch : 0.592495878537496
Epoch 1, Batch 60, train loss:1.2984153032302856, Elapsed time for epoch : 0.7087026635805765
Epoch 1, Batch 70, train loss:1.3624842166900635, Elapsed time for epoch : 0.8250327388445536
Epoch 1, Batch 80, train loss:1.3584259748458862, Elapsed time for epoch : 0.9412562410036723
Epoch 1, Batch 90, train loss:1.063966989517212, Elapsed time for epoch : 1.0580750385920206
Epoch 1, Batch 100, train loss:0.8026071190834045, Elapsed time for epoch : 1.1741379459698995
Epoch 1, Batch 110, train loss:0.9930658340454102, Elapsed time for epoch : 1.2902934193611144
Batch 0, val loss:3.0482380390167236
Batch 10, val loss:3.5402774810791016
Batch 20, val loss:4.394863605499268
Batch 30, val loss:2.968579053878784
Epoch 1, Train Loss:1.3607041654379473, Val loss:6.35633079873191
Epoch 2, Batch 0, train loss:0.9439094662666321, Elapsed time for epoch : 0.011693155765533448
Epoch 2, Batch 10, train loss:0.969389021396637, Elapsed time for epoch : 0.12769357760747274
Epoch 2, Batch 20, train loss:0.8955464959144592, Elapsed time for epoch : 0.24408448139826458
Epoch 2, Batch 30, train loss:1.0724585056304932, Elapsed time for epoch : 0.3604120175043742
Epoch 2, Batch 40, train loss:0.8552680611610413, Elapsed time for epoch : 0.47673645814259846
Epoch 2, Batch 50, train loss:0.9822867512702942, Elapsed time for epoch : 0.5931374947230021
Epoch 2, Batch 60, train loss:0.9247282147407532, Elapsed time for epoch : 0.7092194477717082
Epoch 2, Batch 70, train loss:0.5330252051353455, Elapsed time for epoch : 0.8252872029940287
Epoch 2, Batch 80, train loss:0.8274419903755188, Elapsed time for epoch : 0.9416515270868937
Epoch 2, Batch 90, train loss:0.8685148358345032, Elapsed time for epoch : 1.0578882853190104
Epoch 2, Batch 100, train loss:0.7741762399673462, Elapsed time for epoch : 1.1740040500958762
Epoch 2, Batch 110, train loss:0.7705622315406799, Elapsed time for epoch : 1.2903011639912922
Batch 0, val loss:6.083832740783691
Batch 10, val loss:3.164226531982422
Batch 20, val loss:1.8997132778167725
Batch 30, val loss:5.480319499969482
Epoch 2, Train Loss:0.8403237327285434, Val loss:7.331458323531681
Epoch 3, Batch 0, train loss:0.727228045463562, Elapsed time for epoch : 0.011714414755503336
Epoch 3, Batch 10, train loss:0.9378278851509094, Elapsed time for epoch : 0.1279504418373108
Epoch 3, Batch 20, train loss:0.7907819151878357, Elapsed time for epoch : 0.24448819955190024
Epoch 3, Batch 30, train loss:0.709861159324646, Elapsed time for epoch : 0.36069124142328896
Epoch 3, Batch 40, train loss:0.8092982172966003, Elapsed time for epoch : 0.47802536487579345
Epoch 3, Batch 50, train loss:0.3278413414955139, Elapsed time for epoch : 0.5945273001988729
Epoch 3, Batch 60, train loss:0.6629754900932312, Elapsed time for epoch : 0.7107482234636943
Epoch 3, Batch 70, train loss:0.679587185382843, Elapsed time for epoch : 0.8269160469373067
Epoch 3, Batch 80, train loss:0.7752425074577332, Elapsed time for epoch : 0.9433057308197021
Epoch 3, Batch 90, train loss:0.571168065071106, Elapsed time for epoch : 1.0598743955294292
Epoch 3, Batch 100, train loss:0.1900956928730011, Elapsed time for epoch : 1.1758081634839377
Epoch 3, Batch 110, train loss:0.49610474705696106, Elapsed time for epoch : 1.2923205455144247
Batch 0, val loss:8.303558349609375
Batch 10, val loss:17.776697158813477
Batch 20, val loss:18.68522834777832
Batch 30, val loss:3.088165521621704
Epoch 3, Train Loss:0.5858511642269466, Val loss:10.00613094204002
Epoch 4, Batch 0, train loss:0.6903799772262573, Elapsed time for epoch : 0.011649696032206218
Epoch 4, Batch 10, train loss:0.22112058103084564, Elapsed time for epoch : 0.12805847724278768
Epoch 4, Batch 20, train loss:0.19638237357139587, Elapsed time for epoch : 0.24446595509847005
Epoch 4, Batch 30, train loss:0.48365482687950134, Elapsed time for epoch : 0.360484500726064
Epoch 4, Batch 40, train loss:0.45583459734916687, Elapsed time for epoch : 0.4769474903742472
Epoch 4, Batch 50, train loss:0.43529757857322693, Elapsed time for epoch : 0.5931225856145222
Epoch 4, Batch 60, train loss:0.3752369284629822, Elapsed time for epoch : 0.7094227433204651
Epoch 4, Batch 70, train loss:0.4187489449977875, Elapsed time for epoch : 0.8259753108024597
Epoch 4, Batch 80, train loss:0.20830167829990387, Elapsed time for epoch : 0.9420290907224019
Epoch 4, Batch 90, train loss:0.40368205308914185, Elapsed time for epoch : 1.0581735889116923
Epoch 4, Batch 100, train loss:0.4995242953300476, Elapsed time for epoch : 1.1744543155034384
Epoch 4, Batch 110, train loss:0.4225638806819916, Elapsed time for epoch : 1.2905363321304322
Batch 0, val loss:12.22327995300293
Batch 10, val loss:2.929750442504883
Batch 20, val loss:3.1831581592559814
Batch 30, val loss:2.823903799057007
Epoch 4, Train Loss:0.39271070322264795, Val loss:8.315379876229498
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.39271
wandb:         Val Loss 8.31538
wandb:      train_batch 110
wandb: train_batch_loss 0.42256
wandb:        val_batch 30
wandb:   val_batch_loss 2.8239
wandb: 
wandb: üöÄ View run glad-haze-338 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/t1yp6jfs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_015938-t1yp6jfs/logs
Seed completed execution! 113 0.8_3
------------------------------------------------------------------
Experiment complete 0.8_3
==========================================================================
Running experiment for setting 0.8_4
==========================================================================
Running for seed 1 of experiment 0.8_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_020725-7v159xwa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-star-340
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/7v159xwa
Epoch 0, Batch 0, train loss:8.46021556854248, Elapsed time for epoch : 0.012792050838470459
Epoch 0, Batch 10, train loss:5.66502046585083, Elapsed time for epoch : 0.13726837237675985
Epoch 0, Batch 20, train loss:3.6718709468841553, Elapsed time for epoch : 0.2604678750038147
Epoch 0, Batch 30, train loss:3.6327857971191406, Elapsed time for epoch : 0.3834100842475891
Epoch 0, Batch 40, train loss:3.2900466918945312, Elapsed time for epoch : 0.5070441881815593
Epoch 0, Batch 50, train loss:2.555213212966919, Elapsed time for epoch : 0.6322403987248738
Epoch 0, Batch 60, train loss:2.5447001457214355, Elapsed time for epoch : 0.7548621336619059
Epoch 0, Batch 70, train loss:2.5709855556488037, Elapsed time for epoch : 0.8788795073827108
Epoch 0, Batch 80, train loss:2.352226972579956, Elapsed time for epoch : 1.0036186138788858
Epoch 0, Batch 90, train loss:2.359292507171631, Elapsed time for epoch : 1.1266264955202738
Epoch 0, Batch 100, train loss:1.8969405889511108, Elapsed time for epoch : 1.2492691636085511
Epoch 0, Batch 110, train loss:1.7627841234207153, Elapsed time for epoch : 1.3730932792027792
Batch 0, val loss:3.4417355060577393
Batch 10, val loss:3.4166698455810547
Batch 20, val loss:2.814253091812134
Batch 30, val loss:4.425282955169678
Epoch 0, Train Loss:3.1216040631999142, Val loss:3.702179398801592
Epoch 1, Batch 0, train loss:1.9035245180130005, Elapsed time for epoch : 0.01168589194615682
Epoch 1, Batch 10, train loss:1.6789453029632568, Elapsed time for epoch : 0.12829562425613403
Epoch 1, Batch 20, train loss:1.4087671041488647, Elapsed time for epoch : 0.2445944348971049
Epoch 1, Batch 30, train loss:1.6584151983261108, Elapsed time for epoch : 0.3607040802637736
Epoch 1, Batch 40, train loss:1.500169038772583, Elapsed time for epoch : 0.4768716057141622
Epoch 1, Batch 50, train loss:1.3558063507080078, Elapsed time for epoch : 0.5932008266448975
Epoch 1, Batch 60, train loss:1.3241344690322876, Elapsed time for epoch : 0.7094499905904134
Epoch 1, Batch 70, train loss:1.4066888093948364, Elapsed time for epoch : 0.8260576804478963
Epoch 1, Batch 80, train loss:1.328456163406372, Elapsed time for epoch : 0.9423103729883829
Epoch 1, Batch 90, train loss:1.2531636953353882, Elapsed time for epoch : 1.0587308804194133
Epoch 1, Batch 100, train loss:1.1225268840789795, Elapsed time for epoch : 1.1749217669169107
Epoch 1, Batch 110, train loss:1.0947104692459106, Elapsed time for epoch : 1.2914994756380718
Batch 0, val loss:2.708939790725708
Batch 10, val loss:1.9382455348968506
Batch 20, val loss:4.977107048034668
Batch 30, val loss:4.637631416320801
Epoch 1, Train Loss:1.4095528244972229, Val loss:4.114034331507153
Epoch 2, Batch 0, train loss:1.0769912004470825, Elapsed time for epoch : 0.011766791343688965
Epoch 2, Batch 10, train loss:1.1204888820648193, Elapsed time for epoch : 0.1288039763768514
Epoch 2, Batch 20, train loss:1.1063909530639648, Elapsed time for epoch : 0.24586571057637532
Epoch 2, Batch 30, train loss:1.0660799741744995, Elapsed time for epoch : 0.3622960050900777
Epoch 2, Batch 40, train loss:1.071070909500122, Elapsed time for epoch : 0.4788263956705729
Epoch 2, Batch 50, train loss:1.0891125202178955, Elapsed time for epoch : 0.5950218439102173
Epoch 2, Batch 60, train loss:1.1351220607757568, Elapsed time for epoch : 0.7113415718078613
Epoch 2, Batch 70, train loss:0.6099258065223694, Elapsed time for epoch : 0.8277745604515075
Epoch 2, Batch 80, train loss:0.9623298048973083, Elapsed time for epoch : 0.9441593567530314
Epoch 2, Batch 90, train loss:0.9149845242500305, Elapsed time for epoch : 1.0602157990137735
Epoch 2, Batch 100, train loss:0.9806971549987793, Elapsed time for epoch : 1.176655606428782
Epoch 2, Batch 110, train loss:0.8749297261238098, Elapsed time for epoch : 1.2935586333274842
Batch 0, val loss:1.2363866567611694
Batch 10, val loss:1.952571988105774
Batch 20, val loss:3.3140056133270264
Batch 30, val loss:3.554605484008789
Epoch 2, Train Loss:0.951618002290311, Val loss:3.3409249401754804
Epoch 3, Batch 0, train loss:0.8837815523147583, Elapsed time for epoch : 0.011768535772959391
Epoch 3, Batch 10, train loss:0.8430439233779907, Elapsed time for epoch : 0.12810697158177695
Epoch 3, Batch 20, train loss:0.8799654841423035, Elapsed time for epoch : 0.2443198800086975
Epoch 3, Batch 30, train loss:0.8248592615127563, Elapsed time for epoch : 0.3608916958173116
Epoch 3, Batch 40, train loss:0.9715725183486938, Elapsed time for epoch : 0.4772097706794739
Epoch 3, Batch 50, train loss:0.3849340081214905, Elapsed time for epoch : 0.5935279568036397
Epoch 3, Batch 60, train loss:0.7764536142349243, Elapsed time for epoch : 0.7098957578341166
Epoch 3, Batch 70, train loss:0.7836403250694275, Elapsed time for epoch : 0.8269070068995158
Epoch 3, Batch 80, train loss:0.7755357027053833, Elapsed time for epoch : 0.9435072422027588
Epoch 3, Batch 90, train loss:0.651088297367096, Elapsed time for epoch : 1.0602149486541748
Epoch 3, Batch 100, train loss:0.2968215346336365, Elapsed time for epoch : 1.1764792124430339
Epoch 3, Batch 110, train loss:0.6813176274299622, Elapsed time for epoch : 1.2931283831596374
Batch 0, val loss:5.206582546234131
Batch 10, val loss:8.216694831848145
Batch 20, val loss:14.117790222167969
Batch 30, val loss:2.601100444793701
Epoch 3, Train Loss:0.715617591401805, Val loss:5.43496089677016
Epoch 4, Batch 0, train loss:1.0618852376937866, Elapsed time for epoch : 0.011706272761027018
Epoch 4, Batch 10, train loss:0.3469524383544922, Elapsed time for epoch : 0.12814971605936687
Epoch 4, Batch 20, train loss:0.2591385841369629, Elapsed time for epoch : 0.24422418276468913
Epoch 4, Batch 30, train loss:0.6174682378768921, Elapsed time for epoch : 0.3605686982472738
Epoch 4, Batch 40, train loss:0.5908051133155823, Elapsed time for epoch : 0.4768896341323853
Epoch 4, Batch 50, train loss:0.6392977833747864, Elapsed time for epoch : 0.5934410214424133
Epoch 4, Batch 60, train loss:0.5909881591796875, Elapsed time for epoch : 0.7098298827807109
Epoch 4, Batch 70, train loss:0.5513695478439331, Elapsed time for epoch : 0.8264005144437154
Epoch 4, Batch 80, train loss:0.269618958234787, Elapsed time for epoch : 0.9426559805870056
Epoch 4, Batch 90, train loss:0.6408963799476624, Elapsed time for epoch : 1.0591891050338744
Epoch 4, Batch 100, train loss:0.541032612323761, Elapsed time for epoch : 1.1758182605107625
Epoch 4, Batch 110, train loss:0.6422269940376282, Elapsed time for epoch : 1.2924368818600973
Batch 0, val loss:4.796104431152344
Batch 10, val loss:2.012908697128296
Batch 20, val loss:2.8557944297790527
Batch 30, val loss:3.2009596824645996
Epoch 4, Train Loss:0.521660025741743, Val loss:5.3030252158641815
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.52166
wandb:         Val Loss 5.30303
wandb:      train_batch 110
wandb: train_batch_loss 0.64223
wandb:        val_batch 30
wandb:   val_batch_loss 3.20096
wandb: 
wandb: üöÄ View run peachy-star-340 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/7v159xwa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_020725-7v159xwa/logs
Seed completed execution! 1 0.8_4
------------------------------------------------------------------
Running for seed 42 of experiment 0.8_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_021519-0rb5n4mc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-spaceship-342
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/0rb5n4mc
Epoch 0, Batch 0, train loss:8.46021556854248, Elapsed time for epoch : 0.013458991050720214
Epoch 0, Batch 10, train loss:5.66502046585083, Elapsed time for epoch : 0.12960835695266723
Epoch 0, Batch 20, train loss:3.6718709468841553, Elapsed time for epoch : 0.24525549014409384
Epoch 0, Batch 30, train loss:3.6327857971191406, Elapsed time for epoch : 0.3614911238352458
Epoch 0, Batch 40, train loss:3.2900466918945312, Elapsed time for epoch : 0.47749568621317545
Epoch 0, Batch 50, train loss:2.555213212966919, Elapsed time for epoch : 0.5934120575586955
Epoch 0, Batch 60, train loss:2.5447001457214355, Elapsed time for epoch : 0.7097376585006714
Epoch 0, Batch 70, train loss:2.5709855556488037, Elapsed time for epoch : 0.8258424957593282
Epoch 0, Batch 80, train loss:2.352226972579956, Elapsed time for epoch : 0.9420315186182658
Epoch 0, Batch 90, train loss:2.359292507171631, Elapsed time for epoch : 1.058682882785797
Epoch 0, Batch 100, train loss:1.8969405889511108, Elapsed time for epoch : 1.1749732931454977
Epoch 0, Batch 110, train loss:1.7627841234207153, Elapsed time for epoch : 1.2910800139109293
Batch 0, val loss:3.4417355060577393
Batch 10, val loss:3.4166698455810547
Batch 20, val loss:2.814253091812134
Batch 30, val loss:4.425282955169678
Epoch 0, Train Loss:3.1216040631999142, Val loss:3.702179398801592
Epoch 1, Batch 0, train loss:1.9035245180130005, Elapsed time for epoch : 0.011682403087615967
Epoch 1, Batch 10, train loss:1.6789453029632568, Elapsed time for epoch : 0.12790911595026652
Epoch 1, Batch 20, train loss:1.4087671041488647, Elapsed time for epoch : 0.24458345969518025
Epoch 1, Batch 30, train loss:1.6584151983261108, Elapsed time for epoch : 0.361020028591156
Epoch 1, Batch 40, train loss:1.500169038772583, Elapsed time for epoch : 0.47837520043055215
Epoch 1, Batch 50, train loss:1.3558063507080078, Elapsed time for epoch : 0.5948275248209636
Epoch 1, Batch 60, train loss:1.3241344690322876, Elapsed time for epoch : 0.71131458679835
Epoch 1, Batch 70, train loss:1.4066888093948364, Elapsed time for epoch : 0.827511441707611
Epoch 1, Batch 80, train loss:1.328456163406372, Elapsed time for epoch : 0.9442460060119628
Epoch 1, Batch 90, train loss:1.2531636953353882, Elapsed time for epoch : 1.0609438061714171
Epoch 1, Batch 100, train loss:1.1225268840789795, Elapsed time for epoch : 1.1771436254183452
Epoch 1, Batch 110, train loss:1.0947104692459106, Elapsed time for epoch : 1.2933732589085898
Batch 0, val loss:2.708939790725708
Batch 10, val loss:1.9382455348968506
Batch 20, val loss:4.977107048034668
Batch 30, val loss:4.637631416320801
Epoch 1, Train Loss:1.4095528244972229, Val loss:4.114034331507153
Epoch 2, Batch 0, train loss:1.0769912004470825, Elapsed time for epoch : 0.011729296048482258
Epoch 2, Batch 10, train loss:1.1204888820648193, Elapsed time for epoch : 0.12825307448705037
Epoch 2, Batch 20, train loss:1.1063909530639648, Elapsed time for epoch : 0.24478185574213665
Epoch 2, Batch 30, train loss:1.0660799741744995, Elapsed time for epoch : 0.3610828161239624
Epoch 2, Batch 40, train loss:1.071070909500122, Elapsed time for epoch : 0.4783111333847046
Epoch 2, Batch 50, train loss:1.0891125202178955, Elapsed time for epoch : 0.5946810603141784
Epoch 2, Batch 60, train loss:1.1351220607757568, Elapsed time for epoch : 0.7109527309735616
Epoch 2, Batch 70, train loss:0.6099258065223694, Elapsed time for epoch : 0.8274343291918437
Epoch 2, Batch 80, train loss:0.9623298048973083, Elapsed time for epoch : 0.9443199316660563
Epoch 2, Batch 90, train loss:0.9149845242500305, Elapsed time for epoch : 1.060727302233378
Epoch 2, Batch 100, train loss:0.9806971549987793, Elapsed time for epoch : 1.177249018351237
Epoch 2, Batch 110, train loss:0.8749297261238098, Elapsed time for epoch : 1.2937022169431052
Batch 0, val loss:1.2363866567611694
Batch 10, val loss:1.952571988105774
Batch 20, val loss:3.3140056133270264
Batch 30, val loss:3.554605484008789
Epoch 2, Train Loss:0.951618002290311, Val loss:3.3409249401754804
Epoch 3, Batch 0, train loss:0.8837815523147583, Elapsed time for epoch : 0.01173859437306722
Epoch 3, Batch 10, train loss:0.8430439233779907, Elapsed time for epoch : 0.1279592235883077
Epoch 3, Batch 20, train loss:0.8799654841423035, Elapsed time for epoch : 0.24389266967773438
Epoch 3, Batch 30, train loss:0.8248592615127563, Elapsed time for epoch : 0.3599244038263957
Epoch 3, Batch 40, train loss:0.9715725183486938, Elapsed time for epoch : 0.47614704370498656
Epoch 3, Batch 50, train loss:0.3849340081214905, Elapsed time for epoch : 0.5923829396565755
Epoch 3, Batch 60, train loss:0.7764536142349243, Elapsed time for epoch : 0.708593213558197
Epoch 3, Batch 70, train loss:0.7836403250694275, Elapsed time for epoch : 0.8249912222226461
Epoch 3, Batch 80, train loss:0.7755357027053833, Elapsed time for epoch : 0.9410850723584493
Epoch 3, Batch 90, train loss:0.651088297367096, Elapsed time for epoch : 1.057537563641866
Epoch 3, Batch 100, train loss:0.2968215346336365, Elapsed time for epoch : 1.1735539158185324
Epoch 3, Batch 110, train loss:0.6813176274299622, Elapsed time for epoch : 1.2896188100179036
Batch 0, val loss:5.206582546234131
Batch 10, val loss:8.216694831848145
Batch 20, val loss:14.117790222167969
Batch 30, val loss:2.601100444793701
Epoch 3, Train Loss:0.715617591401805, Val loss:5.43496089677016
Epoch 4, Batch 0, train loss:1.0618852376937866, Elapsed time for epoch : 0.01171191930770874
Epoch 4, Batch 10, train loss:0.3469524383544922, Elapsed time for epoch : 0.12813520431518555
Epoch 4, Batch 20, train loss:0.2591385841369629, Elapsed time for epoch : 0.24447943766911825
Epoch 4, Batch 30, train loss:0.6174682378768921, Elapsed time for epoch : 0.3606540004412333
Epoch 4, Batch 40, train loss:0.5908051133155823, Elapsed time for epoch : 0.47705822388331093
Epoch 4, Batch 50, train loss:0.6392977833747864, Elapsed time for epoch : 0.5936137636502584
Epoch 4, Batch 60, train loss:0.5909881591796875, Elapsed time for epoch : 0.7099444746971131
Epoch 4, Batch 70, train loss:0.5513695478439331, Elapsed time for epoch : 0.8261754115422567
Epoch 4, Batch 80, train loss:0.269618958234787, Elapsed time for epoch : 0.9431131561597188
Epoch 4, Batch 90, train loss:0.6408963799476624, Elapsed time for epoch : 1.0594217896461486
Epoch 4, Batch 100, train loss:0.541032612323761, Elapsed time for epoch : 1.1756245096524556
Epoch 4, Batch 110, train loss:0.6422269940376282, Elapsed time for epoch : 1.2917518099149068
Batch 0, val loss:4.796104431152344
Batch 10, val loss:2.012908697128296
Batch 20, val loss:2.8557944297790527
Batch 30, val loss:3.2009596824645996
Epoch 4, Train Loss:0.521660025741743, Val loss:5.3030252158641815
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.52166
wandb:         Val Loss 5.30303
wandb:      train_batch 110
wandb: train_batch_loss 0.64223
wandb:        val_batch 30
wandb:   val_batch_loss 3.20096
wandb: 
wandb: üöÄ View run stellar-spaceship-342 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/0rb5n4mc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_021519-0rb5n4mc/logs
Seed completed execution! 42 0.8_4
------------------------------------------------------------------
Running for seed 89 of experiment 0.8_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_022305-8zzky4el
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-meadow-344
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8zzky4el
Epoch 0, Batch 0, train loss:8.46021556854248, Elapsed time for epoch : 0.01324170430501302
Epoch 0, Batch 10, train loss:5.66502046585083, Elapsed time for epoch : 0.1292665203412374
Epoch 0, Batch 20, train loss:3.6718709468841553, Elapsed time for epoch : 0.24520301421483356
Epoch 0, Batch 30, train loss:3.6327857971191406, Elapsed time for epoch : 0.3612719456354777
Epoch 0, Batch 40, train loss:3.2900466918945312, Elapsed time for epoch : 0.47781296968460085
Epoch 0, Batch 50, train loss:2.555213212966919, Elapsed time for epoch : 0.5939487099647522
Epoch 0, Batch 60, train loss:2.5447001457214355, Elapsed time for epoch : 0.7100683689117432
Epoch 0, Batch 70, train loss:2.5709855556488037, Elapsed time for epoch : 0.8265896161397298
Epoch 0, Batch 80, train loss:2.352226972579956, Elapsed time for epoch : 0.9430883089701335
Epoch 0, Batch 90, train loss:2.359292507171631, Elapsed time for epoch : 1.0590715090433755
Epoch 0, Batch 100, train loss:1.8969405889511108, Elapsed time for epoch : 1.1750487804412841
Epoch 0, Batch 110, train loss:1.7627841234207153, Elapsed time for epoch : 1.2911184191703797
Batch 0, val loss:3.4417355060577393
Batch 10, val loss:3.4166698455810547
Batch 20, val loss:2.814253091812134
Batch 30, val loss:4.425282955169678
Epoch 0, Train Loss:3.1216040631999142, Val loss:3.702179398801592
Epoch 1, Batch 0, train loss:1.9035245180130005, Elapsed time for epoch : 0.011686845620473226
Epoch 1, Batch 10, train loss:1.6789453029632568, Elapsed time for epoch : 0.1278040568033854
Epoch 1, Batch 20, train loss:1.4087671041488647, Elapsed time for epoch : 0.24410401185353597
Epoch 1, Batch 30, train loss:1.6584151983261108, Elapsed time for epoch : 0.36028391122817993
Epoch 1, Batch 40, train loss:1.500169038772583, Elapsed time for epoch : 0.4763827164967855
Epoch 1, Batch 50, train loss:1.3558063507080078, Elapsed time for epoch : 0.5926486810048421
Epoch 1, Batch 60, train loss:1.3241344690322876, Elapsed time for epoch : 0.7091420729955037
Epoch 1, Batch 70, train loss:1.4066888093948364, Elapsed time for epoch : 0.8254943052927654
Epoch 1, Batch 80, train loss:1.328456163406372, Elapsed time for epoch : 0.9416841268539429
Epoch 1, Batch 90, train loss:1.2531636953353882, Elapsed time for epoch : 1.0578524192174277
Epoch 1, Batch 100, train loss:1.1225268840789795, Elapsed time for epoch : 1.1740930875142415
Epoch 1, Batch 110, train loss:1.0947104692459106, Elapsed time for epoch : 1.2902625799179077
Batch 0, val loss:2.708939790725708
Batch 10, val loss:1.9382455348968506
Batch 20, val loss:4.977107048034668
Batch 30, val loss:4.637631416320801
Epoch 1, Train Loss:1.4095528244972229, Val loss:4.114034331507153
Epoch 2, Batch 0, train loss:1.0769912004470825, Elapsed time for epoch : 0.0117087721824646
Epoch 2, Batch 10, train loss:1.1204888820648193, Elapsed time for epoch : 0.12792751789093018
Epoch 2, Batch 20, train loss:1.1063909530639648, Elapsed time for epoch : 0.2443109154701233
Epoch 2, Batch 30, train loss:1.0660799741744995, Elapsed time for epoch : 0.36043951908747357
Epoch 2, Batch 40, train loss:1.071070909500122, Elapsed time for epoch : 0.4765478769938151
Epoch 2, Batch 50, train loss:1.0891125202178955, Elapsed time for epoch : 0.5928629239400228
Epoch 2, Batch 60, train loss:1.1351220607757568, Elapsed time for epoch : 0.709088945388794
Epoch 2, Batch 70, train loss:0.6099258065223694, Elapsed time for epoch : 0.8254529595375061
Epoch 2, Batch 80, train loss:0.9623298048973083, Elapsed time for epoch : 0.9417502323786417
Epoch 2, Batch 90, train loss:0.9149845242500305, Elapsed time for epoch : 1.0579059799512227
Epoch 2, Batch 100, train loss:0.9806971549987793, Elapsed time for epoch : 1.1741959849993389
Epoch 2, Batch 110, train loss:0.8749297261238098, Elapsed time for epoch : 1.2904513398806254
Batch 0, val loss:1.2363866567611694
Batch 10, val loss:1.952571988105774
Batch 20, val loss:3.3140056133270264
Batch 30, val loss:3.554605484008789
Epoch 2, Train Loss:0.951618002290311, Val loss:3.3409249401754804
Epoch 3, Batch 0, train loss:0.8837815523147583, Elapsed time for epoch : 0.011866517861684163
Epoch 3, Batch 10, train loss:0.8430439233779907, Elapsed time for epoch : 0.1280820409456889
Epoch 3, Batch 20, train loss:0.8799654841423035, Elapsed time for epoch : 0.24438287019729615
Epoch 3, Batch 30, train loss:0.8248592615127563, Elapsed time for epoch : 0.3605500300725301
Epoch 3, Batch 40, train loss:0.9715725183486938, Elapsed time for epoch : 0.47691888411839806
Epoch 3, Batch 50, train loss:0.3849340081214905, Elapsed time for epoch : 0.5929817318916321
Epoch 3, Batch 60, train loss:0.7764536142349243, Elapsed time for epoch : 0.7094977140426636
Epoch 3, Batch 70, train loss:0.7836403250694275, Elapsed time for epoch : 0.825883169968923
Epoch 3, Batch 80, train loss:0.7755357027053833, Elapsed time for epoch : 0.942085075378418
Epoch 3, Batch 90, train loss:0.651088297367096, Elapsed time for epoch : 1.058354119459788
Epoch 3, Batch 100, train loss:0.2968215346336365, Elapsed time for epoch : 1.1743153174718222
Epoch 3, Batch 110, train loss:0.6813176274299622, Elapsed time for epoch : 1.2908839464187623
Batch 0, val loss:5.206582546234131
Batch 10, val loss:8.216694831848145
Batch 20, val loss:14.117790222167969
Batch 30, val loss:2.601100444793701
Epoch 3, Train Loss:0.715617591401805, Val loss:5.43496089677016
Epoch 4, Batch 0, train loss:1.0618852376937866, Elapsed time for epoch : 0.011874540646870931
Epoch 4, Batch 10, train loss:0.3469524383544922, Elapsed time for epoch : 0.1281307617823283
Epoch 4, Batch 20, train loss:0.2591385841369629, Elapsed time for epoch : 0.24457836945851644
Epoch 4, Batch 30, train loss:0.6174682378768921, Elapsed time for epoch : 0.36094298362731936
Epoch 4, Batch 40, train loss:0.5908051133155823, Elapsed time for epoch : 0.477456279595693
Epoch 4, Batch 50, train loss:0.6392977833747864, Elapsed time for epoch : 0.5939284880956014
Epoch 4, Batch 60, train loss:0.5909881591796875, Elapsed time for epoch : 0.7105735381444295
Epoch 4, Batch 70, train loss:0.5513695478439331, Elapsed time for epoch : 0.8267698526382447
Epoch 4, Batch 80, train loss:0.269618958234787, Elapsed time for epoch : 0.9432938615481059
Epoch 4, Batch 90, train loss:0.6408963799476624, Elapsed time for epoch : 1.059554386138916
Epoch 4, Batch 100, train loss:0.541032612323761, Elapsed time for epoch : 1.175843111673991
Epoch 4, Batch 110, train loss:0.6422269940376282, Elapsed time for epoch : 1.29197758436203
Batch 0, val loss:4.796104431152344
Batch 10, val loss:2.012908697128296
Batch 20, val loss:2.8557944297790527
Batch 30, val loss:3.2009596824645996
Epoch 4, Train Loss:0.521660025741743, Val loss:5.3030252158641815
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.52166
wandb:         Val Loss 5.30303
wandb:      train_batch 110
wandb: train_batch_loss 0.64223
wandb:        val_batch 30
wandb:   val_batch_loss 3.20096
wandb: 
wandb: üöÄ View run eager-meadow-344 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8zzky4el
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_022305-8zzky4el/logs
Seed completed execution! 89 0.8_4
------------------------------------------------------------------
Running for seed 23 of experiment 0.8_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_023051-lef007bv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-paper-346
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/lef007bv
Epoch 0, Batch 0, train loss:8.46021556854248, Elapsed time for epoch : 0.012025864919026692
Epoch 0, Batch 10, train loss:5.66502046585083, Elapsed time for epoch : 0.12821242014567058
Epoch 0, Batch 20, train loss:3.6718709468841553, Elapsed time for epoch : 0.24424293438593547
Epoch 0, Batch 30, train loss:3.6327857971191406, Elapsed time for epoch : 0.3604976614316305
Epoch 0, Batch 40, train loss:3.2900466918945312, Elapsed time for epoch : 0.47688093582789104
Epoch 0, Batch 50, train loss:2.555213212966919, Elapsed time for epoch : 0.5936781366666158
Epoch 0, Batch 60, train loss:2.5447001457214355, Elapsed time for epoch : 0.7106448888778687
Epoch 0, Batch 70, train loss:2.5709855556488037, Elapsed time for epoch : 0.8271520654360454
Epoch 0, Batch 80, train loss:2.352226972579956, Elapsed time for epoch : 0.94388906955719
Epoch 0, Batch 90, train loss:2.359292507171631, Elapsed time for epoch : 1.0600945432980855
Epoch 0, Batch 100, train loss:1.8969405889511108, Elapsed time for epoch : 1.176450459162394
Epoch 0, Batch 110, train loss:1.7627841234207153, Elapsed time for epoch : 1.2924718936284383
Batch 0, val loss:3.4417355060577393
Batch 10, val loss:3.4166698455810547
Batch 20, val loss:2.814253091812134
Batch 30, val loss:4.425282955169678
Epoch 0, Train Loss:3.1216040631999142, Val loss:3.702179398801592
Epoch 1, Batch 0, train loss:1.9035245180130005, Elapsed time for epoch : 0.011702950795491536
Epoch 1, Batch 10, train loss:1.6789453029632568, Elapsed time for epoch : 0.128042201201121
Epoch 1, Batch 20, train loss:1.4087671041488647, Elapsed time for epoch : 0.24462275505065917
Epoch 1, Batch 30, train loss:1.6584151983261108, Elapsed time for epoch : 0.36093191305796307
Epoch 1, Batch 40, train loss:1.500169038772583, Elapsed time for epoch : 0.4772052884101868
Epoch 1, Batch 50, train loss:1.3558063507080078, Elapsed time for epoch : 0.5937795996665954
Epoch 1, Batch 60, train loss:1.3241344690322876, Elapsed time for epoch : 0.7103122552235921
Epoch 1, Batch 70, train loss:1.4066888093948364, Elapsed time for epoch : 0.8264407714207967
Epoch 1, Batch 80, train loss:1.328456163406372, Elapsed time for epoch : 0.943410317103068
Epoch 1, Batch 90, train loss:1.2531636953353882, Elapsed time for epoch : 1.059622347354889
Epoch 1, Batch 100, train loss:1.1225268840789795, Elapsed time for epoch : 1.1758977770805359
Epoch 1, Batch 110, train loss:1.0947104692459106, Elapsed time for epoch : 1.2922383228937784
Batch 0, val loss:2.708939790725708
Batch 10, val loss:1.9382455348968506
Batch 20, val loss:4.977107048034668
Batch 30, val loss:4.637631416320801
Epoch 1, Train Loss:1.4095528244972229, Val loss:4.114034331507153
Epoch 2, Batch 0, train loss:1.0769912004470825, Elapsed time for epoch : 0.011749505996704102
Epoch 2, Batch 10, train loss:1.1204888820648193, Elapsed time for epoch : 0.1282041549682617
Epoch 2, Batch 20, train loss:1.1063909530639648, Elapsed time for epoch : 0.244577685991923
Epoch 2, Batch 30, train loss:1.0660799741744995, Elapsed time for epoch : 0.3607795516649882
Epoch 2, Batch 40, train loss:1.071070909500122, Elapsed time for epoch : 0.47769920031229657
Epoch 2, Batch 50, train loss:1.0891125202178955, Elapsed time for epoch : 0.5940243800481161
Epoch 2, Batch 60, train loss:1.1351220607757568, Elapsed time for epoch : 0.7104569832483928
Epoch 2, Batch 70, train loss:0.6099258065223694, Elapsed time for epoch : 0.8269851485888163
Epoch 2, Batch 80, train loss:0.9623298048973083, Elapsed time for epoch : 0.9433933814366658
Epoch 2, Batch 90, train loss:0.9149845242500305, Elapsed time for epoch : 1.059727430343628
Epoch 2, Batch 100, train loss:0.9806971549987793, Elapsed time for epoch : 1.1759280165036519
Epoch 2, Batch 110, train loss:0.8749297261238098, Elapsed time for epoch : 1.2922796289126077
Batch 0, val loss:1.2363866567611694
Batch 10, val loss:1.952571988105774
Batch 20, val loss:3.3140056133270264
Batch 30, val loss:3.554605484008789
Epoch 2, Train Loss:0.951618002290311, Val loss:3.3409249401754804
Epoch 3, Batch 0, train loss:0.8837815523147583, Elapsed time for epoch : 0.011899113655090332
Epoch 3, Batch 10, train loss:0.8430439233779907, Elapsed time for epoch : 0.12811679045359295
Epoch 3, Batch 20, train loss:0.8799654841423035, Elapsed time for epoch : 0.24477347135543823
Epoch 3, Batch 30, train loss:0.8248592615127563, Elapsed time for epoch : 0.36111188332239785
Epoch 3, Batch 40, train loss:0.9715725183486938, Elapsed time for epoch : 0.477443532148997
Epoch 3, Batch 50, train loss:0.3849340081214905, Elapsed time for epoch : 0.5942119042078654
Epoch 3, Batch 60, train loss:0.7764536142349243, Elapsed time for epoch : 0.7109280943870544
Epoch 3, Batch 70, train loss:0.7836403250694275, Elapsed time for epoch : 0.8275687257448833
Epoch 3, Batch 80, train loss:0.7755357027053833, Elapsed time for epoch : 0.9439185460408529
Epoch 3, Batch 90, train loss:0.651088297367096, Elapsed time for epoch : 1.0611008167266847
Epoch 3, Batch 100, train loss:0.2968215346336365, Elapsed time for epoch : 1.1777772943178813
Epoch 3, Batch 110, train loss:0.6813176274299622, Elapsed time for epoch : 1.2941062808036805
Batch 0, val loss:5.206582546234131
Batch 10, val loss:8.216694831848145
Batch 20, val loss:14.117790222167969
Batch 30, val loss:2.601100444793701
Epoch 3, Train Loss:0.715617591401805, Val loss:5.43496089677016
Epoch 4, Batch 0, train loss:1.0618852376937866, Elapsed time for epoch : 0.011759368578592937
Epoch 4, Batch 10, train loss:0.3469524383544922, Elapsed time for epoch : 0.12897398074467978
Epoch 4, Batch 20, train loss:0.2591385841369629, Elapsed time for epoch : 0.24623867670694988
Epoch 4, Batch 30, train loss:0.6174682378768921, Elapsed time for epoch : 0.36406587759653725
Epoch 4, Batch 40, train loss:0.5908051133155823, Elapsed time for epoch : 0.4819003105163574
Epoch 4, Batch 50, train loss:0.6392977833747864, Elapsed time for epoch : 0.5998864690462749
Epoch 4, Batch 60, train loss:0.5909881591796875, Elapsed time for epoch : 0.7171680172284444
Epoch 4, Batch 70, train loss:0.5513695478439331, Elapsed time for epoch : 0.8352397362391154
Epoch 4, Batch 80, train loss:0.269618958234787, Elapsed time for epoch : 0.9530528744061788
Epoch 4, Batch 90, train loss:0.6408963799476624, Elapsed time for epoch : 1.0701253612836201
Epoch 4, Batch 100, train loss:0.541032612323761, Elapsed time for epoch : 1.1879879713058472
Epoch 4, Batch 110, train loss:0.6422269940376282, Elapsed time for epoch : 1.3063804427782695
Batch 0, val loss:4.796104431152344
Batch 10, val loss:2.012908697128296
Batch 20, val loss:2.8557944297790527
Batch 30, val loss:3.2009596824645996
Epoch 4, Train Loss:0.521660025741743, Val loss:5.3030252158641815
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.52166
wandb:         Val Loss 5.30303
wandb:      train_batch 110
wandb: train_batch_loss 0.64223
wandb:        val_batch 30
wandb:   val_batch_loss 3.20096
wandb: 
wandb: üöÄ View run usual-paper-346 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/lef007bv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_023051-lef007bv/logs
Seed completed execution! 23 0.8_4
------------------------------------------------------------------
Running for seed 113 of experiment 0.8_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_023839-dk8qfbfs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sun-348
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/dk8qfbfs
Epoch 0, Batch 0, train loss:8.46021556854248, Elapsed time for epoch : 0.013498711585998534
Epoch 0, Batch 10, train loss:5.66502046585083, Elapsed time for epoch : 0.1297634482383728
Epoch 0, Batch 20, train loss:3.6718709468841553, Elapsed time for epoch : 0.2461138129234314
Epoch 0, Batch 30, train loss:3.6327857971191406, Elapsed time for epoch : 0.36204898754755654
Epoch 0, Batch 40, train loss:3.2900466918945312, Elapsed time for epoch : 0.4779789050420125
Epoch 0, Batch 50, train loss:2.555213212966919, Elapsed time for epoch : 0.5942893584569295
Epoch 0, Batch 60, train loss:2.5447001457214355, Elapsed time for epoch : 0.7103417595227559
Epoch 0, Batch 70, train loss:2.5709855556488037, Elapsed time for epoch : 0.8266213814417521
Epoch 0, Batch 80, train loss:2.352226972579956, Elapsed time for epoch : 0.9430219093958537
Epoch 0, Batch 90, train loss:2.359292507171631, Elapsed time for epoch : 1.0590878168741862
Epoch 0, Batch 100, train loss:1.8969405889511108, Elapsed time for epoch : 1.1756181716918945
Epoch 0, Batch 110, train loss:1.7627841234207153, Elapsed time for epoch : 1.2918391982714335
Batch 0, val loss:3.4417355060577393
Batch 10, val loss:3.4166698455810547
Batch 20, val loss:2.814253091812134
Batch 30, val loss:4.425282955169678
Epoch 0, Train Loss:3.1216040631999142, Val loss:3.702179398801592
Epoch 1, Batch 0, train loss:1.9035245180130005, Elapsed time for epoch : 0.011767927805582683
Epoch 1, Batch 10, train loss:1.6789453029632568, Elapsed time for epoch : 0.12787864208221436
Epoch 1, Batch 20, train loss:1.4087671041488647, Elapsed time for epoch : 0.24417961438496907
Epoch 1, Batch 30, train loss:1.6584151983261108, Elapsed time for epoch : 0.36052354574203493
Epoch 1, Batch 40, train loss:1.500169038772583, Elapsed time for epoch : 0.4773640394210815
Epoch 1, Batch 50, train loss:1.3558063507080078, Elapsed time for epoch : 0.5932979941368103
Epoch 1, Batch 60, train loss:1.3241344690322876, Elapsed time for epoch : 0.7095238288243612
Epoch 1, Batch 70, train loss:1.4066888093948364, Elapsed time for epoch : 0.8258990287780762
Epoch 1, Batch 80, train loss:1.328456163406372, Elapsed time for epoch : 0.9421640833218893
Epoch 1, Batch 90, train loss:1.2531636953353882, Elapsed time for epoch : 1.0585027535756428
Epoch 1, Batch 100, train loss:1.1225268840789795, Elapsed time for epoch : 1.174735418955485
Epoch 1, Batch 110, train loss:1.0947104692459106, Elapsed time for epoch : 1.2912323832511903
Batch 0, val loss:2.708939790725708
Batch 10, val loss:1.9382455348968506
Batch 20, val loss:4.977107048034668
Batch 30, val loss:4.637631416320801
Epoch 1, Train Loss:1.4095528244972229, Val loss:4.114034331507153
Epoch 2, Batch 0, train loss:1.0769912004470825, Elapsed time for epoch : 0.011697216828664144
Epoch 2, Batch 10, train loss:1.1204888820648193, Elapsed time for epoch : 0.1280186692873637
Epoch 2, Batch 20, train loss:1.1063909530639648, Elapsed time for epoch : 0.24441169102986654
Epoch 2, Batch 30, train loss:1.0660799741744995, Elapsed time for epoch : 0.36067413091659545
Epoch 2, Batch 40, train loss:1.071070909500122, Elapsed time for epoch : 0.47724788983662925
Epoch 2, Batch 50, train loss:1.0891125202178955, Elapsed time for epoch : 0.5934030254681905
Epoch 2, Batch 60, train loss:1.1351220607757568, Elapsed time for epoch : 0.7097936073939005
Epoch 2, Batch 70, train loss:0.6099258065223694, Elapsed time for epoch : 0.8264311949412028
Epoch 2, Batch 80, train loss:0.9623298048973083, Elapsed time for epoch : 0.9433576583862304
Epoch 2, Batch 90, train loss:0.9149845242500305, Elapsed time for epoch : 1.0599026521046957
Epoch 2, Batch 100, train loss:0.9806971549987793, Elapsed time for epoch : 1.1765956163406373
Epoch 2, Batch 110, train loss:0.8749297261238098, Elapsed time for epoch : 1.2934395829836527
Batch 0, val loss:1.2363866567611694
Batch 10, val loss:1.952571988105774
Batch 20, val loss:3.3140056133270264
Batch 30, val loss:3.554605484008789
Epoch 2, Train Loss:0.951618002290311, Val loss:3.3409249401754804
Epoch 3, Batch 0, train loss:0.8837815523147583, Elapsed time for epoch : 0.011713627974192302
Epoch 3, Batch 10, train loss:0.8430439233779907, Elapsed time for epoch : 0.12830384969711303
Epoch 3, Batch 20, train loss:0.8799654841423035, Elapsed time for epoch : 0.24442777236302693
Epoch 3, Batch 30, train loss:0.8248592615127563, Elapsed time for epoch : 0.36070294777552286
Epoch 3, Batch 40, train loss:0.9715725183486938, Elapsed time for epoch : 0.47690924803415935
Epoch 3, Batch 50, train loss:0.3849340081214905, Elapsed time for epoch : 0.5931998372077942
Epoch 3, Batch 60, train loss:0.7764536142349243, Elapsed time for epoch : 0.7095458149909973
Epoch 3, Batch 70, train loss:0.7836403250694275, Elapsed time for epoch : 0.8259855906168619
Epoch 3, Batch 80, train loss:0.7755357027053833, Elapsed time for epoch : 0.942591114838918
Epoch 3, Batch 90, train loss:0.651088297367096, Elapsed time for epoch : 1.0589337428410848
Epoch 3, Batch 100, train loss:0.2968215346336365, Elapsed time for epoch : 1.1757566928863525
Epoch 3, Batch 110, train loss:0.6813176274299622, Elapsed time for epoch : 1.2922229528427125
Batch 0, val loss:5.206582546234131
Batch 10, val loss:8.216694831848145
Batch 20, val loss:14.117790222167969
Batch 30, val loss:2.601100444793701
Epoch 3, Train Loss:0.715617591401805, Val loss:5.43496089677016
Epoch 4, Batch 0, train loss:1.0618852376937866, Elapsed time for epoch : 0.011716926097869873
Epoch 4, Batch 10, train loss:0.3469524383544922, Elapsed time for epoch : 0.12793265183766683
Epoch 4, Batch 20, train loss:0.2591385841369629, Elapsed time for epoch : 0.24404188394546508
Epoch 4, Batch 30, train loss:0.6174682378768921, Elapsed time for epoch : 0.36038051048914593
Epoch 4, Batch 40, train loss:0.5908051133155823, Elapsed time for epoch : 0.4766889015833537
Epoch 4, Batch 50, train loss:0.6392977833747864, Elapsed time for epoch : 0.5930804053942362
Epoch 4, Batch 60, train loss:0.5909881591796875, Elapsed time for epoch : 0.7095616579055786
Epoch 4, Batch 70, train loss:0.5513695478439331, Elapsed time for epoch : 0.8257509549458821
Epoch 4, Batch 80, train loss:0.269618958234787, Elapsed time for epoch : 0.942749297618866
Epoch 4, Batch 90, train loss:0.6408963799476624, Elapsed time for epoch : 1.0589908321698507
Epoch 4, Batch 100, train loss:0.541032612323761, Elapsed time for epoch : 1.175471544265747
Epoch 4, Batch 110, train loss:0.6422269940376282, Elapsed time for epoch : 1.2918951352437338
Batch 0, val loss:4.796104431152344
Batch 10, val loss:2.012908697128296
Batch 20, val loss:2.8557944297790527
Batch 30, val loss:3.2009596824645996
Epoch 4, Train Loss:0.521660025741743, Val loss:5.3030252158641815
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.52166
wandb:         Val Loss 5.30303
wandb:      train_batch 110
wandb: train_batch_loss 0.64223
wandb:        val_batch 30
wandb:   val_batch_loss 3.20096
wandb: 
wandb: üöÄ View run northern-sun-348 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/dk8qfbfs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_023839-dk8qfbfs/logs
Seed completed execution! 113 0.8_4
------------------------------------------------------------------
Experiment complete 0.8_4
==========================================================================
Running experiment for setting 0.8_5
==========================================================================
Running for seed 1 of experiment 0.8_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_024627-zvn0om3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gallant-bird-350
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/zvn0om3c
Epoch 0, Batch 0, train loss:8.513291358947754, Elapsed time for epoch : 0.013409094015757243
Epoch 0, Batch 10, train loss:4.180898189544678, Elapsed time for epoch : 0.13824588060379028
Epoch 0, Batch 20, train loss:3.6267616748809814, Elapsed time for epoch : 0.2618643999099731
Epoch 0, Batch 30, train loss:3.2001330852508545, Elapsed time for epoch : 0.384402326742808
Epoch 0, Batch 40, train loss:3.311025381088257, Elapsed time for epoch : 0.5073729713757833
Epoch 0, Batch 50, train loss:2.7424309253692627, Elapsed time for epoch : 0.6310762842496236
Epoch 0, Batch 60, train loss:2.866588830947876, Elapsed time for epoch : 0.7542831857999166
Epoch 0, Batch 70, train loss:2.779675245285034, Elapsed time for epoch : 0.8774107297261556
Epoch 0, Batch 80, train loss:2.8326549530029297, Elapsed time for epoch : 1.0008150815963746
Epoch 0, Batch 90, train loss:2.6184070110321045, Elapsed time for epoch : 1.1245211998621623
Epoch 0, Batch 100, train loss:2.2400755882263184, Elapsed time for epoch : 1.2485508044560751
Epoch 0, Batch 110, train loss:2.1169261932373047, Elapsed time for epoch : 1.3722712874412537
Batch 0, val loss:6.8978776931762695
Batch 10, val loss:6.715299129486084
Batch 20, val loss:3.5479390621185303
Batch 30, val loss:9.957322120666504
Epoch 0, Train Loss:3.2569123776062674, Val loss:4.829406804508633
Epoch 1, Batch 0, train loss:2.138335943222046, Elapsed time for epoch : 0.011698536078135173
Epoch 1, Batch 10, train loss:1.9288142919540405, Elapsed time for epoch : 0.12779123783111573
Epoch 1, Batch 20, train loss:1.6226396560668945, Elapsed time for epoch : 0.24406819740931193
Epoch 1, Batch 30, train loss:1.6346712112426758, Elapsed time for epoch : 0.36058237552642824
Epoch 1, Batch 40, train loss:1.698508381843567, Elapsed time for epoch : 0.47667434215545657
Epoch 1, Batch 50, train loss:1.5913894176483154, Elapsed time for epoch : 0.5927622238794963
Epoch 1, Batch 60, train loss:1.4703236818313599, Elapsed time for epoch : 0.7094731489817302
Epoch 1, Batch 70, train loss:1.4855797290802002, Elapsed time for epoch : 0.8258466005325318
Epoch 1, Batch 80, train loss:1.38066565990448, Elapsed time for epoch : 0.9421427170435588
Epoch 1, Batch 90, train loss:1.2863370180130005, Elapsed time for epoch : 1.0588838537534078
Epoch 1, Batch 100, train loss:0.9612608551979065, Elapsed time for epoch : 1.1750629107157389
Epoch 1, Batch 110, train loss:1.0018376111984253, Elapsed time for epoch : 1.291453516483307
Batch 0, val loss:2.8706533908843994
Batch 10, val loss:1.960553526878357
Batch 20, val loss:2.388474941253662
Batch 30, val loss:2.6824848651885986
Epoch 1, Train Loss:1.4753926572592362, Val loss:3.058097696966595
Epoch 2, Batch 0, train loss:0.9491936564445496, Elapsed time for epoch : 0.011624304453531902
Epoch 2, Batch 10, train loss:1.0148030519485474, Elapsed time for epoch : 0.12862242062886556
Epoch 2, Batch 20, train loss:1.0450317859649658, Elapsed time for epoch : 0.24518286784489948
Epoch 2, Batch 30, train loss:0.9534105062484741, Elapsed time for epoch : 0.36146384874979653
Epoch 2, Batch 40, train loss:0.9440765976905823, Elapsed time for epoch : 0.4778031309445699
Epoch 2, Batch 50, train loss:1.0704208612442017, Elapsed time for epoch : 0.5942347089449564
Epoch 2, Batch 60, train loss:0.9050626158714294, Elapsed time for epoch : 0.7104174812634786
Epoch 2, Batch 70, train loss:0.39578527212142944, Elapsed time for epoch : 0.826702618598938
Epoch 2, Batch 80, train loss:0.8265743255615234, Elapsed time for epoch : 0.9432813803354899
Epoch 2, Batch 90, train loss:0.8194169998168945, Elapsed time for epoch : 1.0594741503397624
Epoch 2, Batch 100, train loss:0.9761618375778198, Elapsed time for epoch : 1.1757810354232787
Epoch 2, Batch 110, train loss:0.688895046710968, Elapsed time for epoch : 1.2922269582748414
Batch 0, val loss:2.956672430038452
Batch 10, val loss:3.1261372566223145
Batch 20, val loss:1.7011984586715698
Batch 30, val loss:2.5490190982818604
Epoch 2, Train Loss:0.8054561521696008, Val loss:3.8704316342870393
Epoch 3, Batch 0, train loss:0.6220940351486206, Elapsed time for epoch : 0.01168283224105835
Epoch 3, Batch 10, train loss:0.6383910775184631, Elapsed time for epoch : 0.1277337710062663
Epoch 3, Batch 20, train loss:0.8586597442626953, Elapsed time for epoch : 0.24425281683603922
Epoch 3, Batch 30, train loss:0.5649629831314087, Elapsed time for epoch : 0.3604177395502726
Epoch 3, Batch 40, train loss:0.613006591796875, Elapsed time for epoch : 0.47638599475224813
Epoch 3, Batch 50, train loss:0.17586924135684967, Elapsed time for epoch : 0.5927637537320455
Epoch 3, Batch 60, train loss:0.5325947403907776, Elapsed time for epoch : 0.7088648438453674
Epoch 3, Batch 70, train loss:0.6017717123031616, Elapsed time for epoch : 0.8254256685574849
Epoch 3, Batch 80, train loss:0.5630633234977722, Elapsed time for epoch : 0.9415592948595682
Epoch 3, Batch 90, train loss:0.4060295522212982, Elapsed time for epoch : 1.057900619506836
Epoch 3, Batch 100, train loss:0.07474873960018158, Elapsed time for epoch : 1.1742507696151734
Epoch 3, Batch 110, train loss:0.4010932445526123, Elapsed time for epoch : 1.290684982140859
Batch 0, val loss:10.053498268127441
Batch 10, val loss:17.06313705444336
Batch 20, val loss:15.598190307617188
Batch 30, val loss:8.04853630065918
Epoch 3, Train Loss:0.4924223325822664, Val loss:12.490536345375908
Epoch 4, Batch 0, train loss:0.4777776896953583, Elapsed time for epoch : 0.011657810211181641
Epoch 4, Batch 10, train loss:0.1042519360780716, Elapsed time for epoch : 0.12821523348490396
Epoch 4, Batch 20, train loss:0.06812852621078491, Elapsed time for epoch : 0.24479320844014485
Epoch 4, Batch 30, train loss:0.4026232063770294, Elapsed time for epoch : 0.3613794287045797
Epoch 4, Batch 40, train loss:0.27457016706466675, Elapsed time for epoch : 0.478312079111735
Epoch 4, Batch 50, train loss:0.3712477385997772, Elapsed time for epoch : 0.594642432530721
Epoch 4, Batch 60, train loss:0.26613107323646545, Elapsed time for epoch : 0.7111169974009196
Epoch 4, Batch 70, train loss:0.32098764181137085, Elapsed time for epoch : 0.8280385851860046
Epoch 4, Batch 80, train loss:0.0737694650888443, Elapsed time for epoch : 0.9444329102834066
Epoch 4, Batch 90, train loss:0.24202857911586761, Elapsed time for epoch : 1.0606860280036927
Epoch 4, Batch 100, train loss:0.2784135639667511, Elapsed time for epoch : 1.177008581161499
Epoch 4, Batch 110, train loss:0.3221166729927063, Elapsed time for epoch : 1.2937413374582927
Batch 0, val loss:5.250369548797607
Batch 10, val loss:2.0428829193115234
Batch 20, val loss:3.6213650703430176
Batch 30, val loss:1.1635663509368896
Epoch 4, Train Loss:0.2785153065362702, Val loss:4.206923966606458
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27852
wandb:         Val Loss 4.20692
wandb:      train_batch 110
wandb: train_batch_loss 0.32212
wandb:        val_batch 30
wandb:   val_batch_loss 1.16357
wandb: 
wandb: üöÄ View run gallant-bird-350 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/zvn0om3c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_024627-zvn0om3c/logs
Seed completed execution! 1 0.8_5
------------------------------------------------------------------
Running for seed 42 of experiment 0.8_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_025421-7nt8e2w8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-violet-352
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/7nt8e2w8
Epoch 0, Batch 0, train loss:8.513291358947754, Elapsed time for epoch : 0.012332018216451008
Epoch 0, Batch 10, train loss:4.180898189544678, Elapsed time for epoch : 0.12826371987660726
Epoch 0, Batch 20, train loss:3.6267616748809814, Elapsed time for epoch : 0.2440379778544108
Epoch 0, Batch 30, train loss:3.2001330852508545, Elapsed time for epoch : 0.3598909099896749
Epoch 0, Batch 40, train loss:3.311025381088257, Elapsed time for epoch : 0.4763036568959554
Epoch 0, Batch 50, train loss:2.7424309253692627, Elapsed time for epoch : 0.592236590385437
Epoch 0, Batch 60, train loss:2.866588830947876, Elapsed time for epoch : 0.7084028164545695
Epoch 0, Batch 70, train loss:2.779675245285034, Elapsed time for epoch : 0.8245348771413167
Epoch 0, Batch 80, train loss:2.8326549530029297, Elapsed time for epoch : 0.9408554156621297
Epoch 0, Batch 90, train loss:2.6184070110321045, Elapsed time for epoch : 1.056922972202301
Epoch 0, Batch 100, train loss:2.2400755882263184, Elapsed time for epoch : 1.173285702864329
Epoch 0, Batch 110, train loss:2.1169261932373047, Elapsed time for epoch : 1.2896072665850322
Batch 0, val loss:6.8978776931762695
Batch 10, val loss:6.715299129486084
Batch 20, val loss:3.5479390621185303
Batch 30, val loss:9.957322120666504
Epoch 0, Train Loss:3.2569123776062674, Val loss:4.829406804508633
Epoch 1, Batch 0, train loss:2.138335943222046, Elapsed time for epoch : 0.01167826255162557
Epoch 1, Batch 10, train loss:1.9288142919540405, Elapsed time for epoch : 0.12810093959172567
Epoch 1, Batch 20, train loss:1.6226396560668945, Elapsed time for epoch : 0.244323734442393
Epoch 1, Batch 30, train loss:1.6346712112426758, Elapsed time for epoch : 0.36086683670679726
Epoch 1, Batch 40, train loss:1.698508381843567, Elapsed time for epoch : 0.47736859718958535
Epoch 1, Batch 50, train loss:1.5913894176483154, Elapsed time for epoch : 0.5938231150309244
Epoch 1, Batch 60, train loss:1.4703236818313599, Elapsed time for epoch : 0.7102989792823792
Epoch 1, Batch 70, train loss:1.4855797290802002, Elapsed time for epoch : 0.8265585978825887
Epoch 1, Batch 80, train loss:1.38066565990448, Elapsed time for epoch : 0.9425415595372518
Epoch 1, Batch 90, train loss:1.2863370180130005, Elapsed time for epoch : 1.059102181593577
Epoch 1, Batch 100, train loss:0.9612608551979065, Elapsed time for epoch : 1.1753289580345154
Epoch 1, Batch 110, train loss:1.0018376111984253, Elapsed time for epoch : 1.2914479653040567
Batch 0, val loss:2.8706533908843994
Batch 10, val loss:1.960553526878357
Batch 20, val loss:2.388474941253662
Batch 30, val loss:2.6824848651885986
Epoch 1, Train Loss:1.4753926572592362, Val loss:3.058097696966595
Epoch 2, Batch 0, train loss:0.9491936564445496, Elapsed time for epoch : 0.011720168590545654
Epoch 2, Batch 10, train loss:1.0148030519485474, Elapsed time for epoch : 0.12804030179977416
Epoch 2, Batch 20, train loss:1.0450317859649658, Elapsed time for epoch : 0.24457141558329265
Epoch 2, Batch 30, train loss:0.9534105062484741, Elapsed time for epoch : 0.3607597271601359
Epoch 2, Batch 40, train loss:0.9440765976905823, Elapsed time for epoch : 0.4769596219062805
Epoch 2, Batch 50, train loss:1.0704208612442017, Elapsed time for epoch : 0.5934154868125916
Epoch 2, Batch 60, train loss:0.9050626158714294, Elapsed time for epoch : 0.7094419042269389
Epoch 2, Batch 70, train loss:0.39578527212142944, Elapsed time for epoch : 0.8257154623667399
Epoch 2, Batch 80, train loss:0.8265743255615234, Elapsed time for epoch : 0.9418838898340861
Epoch 2, Batch 90, train loss:0.8194169998168945, Elapsed time for epoch : 1.0582927664120991
Epoch 2, Batch 100, train loss:0.9761618375778198, Elapsed time for epoch : 1.1749091347058613
Epoch 2, Batch 110, train loss:0.688895046710968, Elapsed time for epoch : 1.2923888961474101
Batch 0, val loss:2.956672430038452
Batch 10, val loss:3.1261372566223145
Batch 20, val loss:1.7011984586715698
Batch 30, val loss:2.5490190982818604
Epoch 2, Train Loss:0.8054561521696008, Val loss:3.8704316342870393
Epoch 3, Batch 0, train loss:0.6220940351486206, Elapsed time for epoch : 0.011692941188812256
Epoch 3, Batch 10, train loss:0.6383910775184631, Elapsed time for epoch : 0.12769599358240763
Epoch 3, Batch 20, train loss:0.8586597442626953, Elapsed time for epoch : 0.2442251682281494
Epoch 3, Batch 30, train loss:0.5649629831314087, Elapsed time for epoch : 0.36025524934132896
Epoch 3, Batch 40, train loss:0.613006591796875, Elapsed time for epoch : 0.4765886664390564
Epoch 3, Batch 50, train loss:0.17586924135684967, Elapsed time for epoch : 0.5926753163337708
Epoch 3, Batch 60, train loss:0.5325947403907776, Elapsed time for epoch : 0.7091529170672098
Epoch 3, Batch 70, train loss:0.6017717123031616, Elapsed time for epoch : 0.8253584225972493
Epoch 3, Batch 80, train loss:0.5630633234977722, Elapsed time for epoch : 0.9420904239018758
Epoch 3, Batch 90, train loss:0.4060295522212982, Elapsed time for epoch : 1.0583313743273417
Epoch 3, Batch 100, train loss:0.07474873960018158, Elapsed time for epoch : 1.1750681082407632
Epoch 3, Batch 110, train loss:0.4010932445526123, Elapsed time for epoch : 1.2914794325828551
Batch 0, val loss:10.053498268127441
Batch 10, val loss:17.06313705444336
Batch 20, val loss:15.598190307617188
Batch 30, val loss:8.04853630065918
Epoch 3, Train Loss:0.4924223325822664, Val loss:12.490536345375908
Epoch 4, Batch 0, train loss:0.4777776896953583, Elapsed time for epoch : 0.01168440580368042
Epoch 4, Batch 10, train loss:0.1042519360780716, Elapsed time for epoch : 0.1281357765197754
Epoch 4, Batch 20, train loss:0.06812852621078491, Elapsed time for epoch : 0.24422041177749634
Epoch 4, Batch 30, train loss:0.4026232063770294, Elapsed time for epoch : 0.36063859462738035
Epoch 4, Batch 40, train loss:0.27457016706466675, Elapsed time for epoch : 0.4772852142651876
Epoch 4, Batch 50, train loss:0.3712477385997772, Elapsed time for epoch : 0.593756377696991
Epoch 4, Batch 60, train loss:0.26613107323646545, Elapsed time for epoch : 0.709973414738973
Epoch 4, Batch 70, train loss:0.32098764181137085, Elapsed time for epoch : 0.8263283212979634
Epoch 4, Batch 80, train loss:0.0737694650888443, Elapsed time for epoch : 0.9424985448519388
Epoch 4, Batch 90, train loss:0.24202857911586761, Elapsed time for epoch : 1.058687396844228
Epoch 4, Batch 100, train loss:0.2784135639667511, Elapsed time for epoch : 1.175159486134847
Epoch 4, Batch 110, train loss:0.3221166729927063, Elapsed time for epoch : 1.2912944356600444
Batch 0, val loss:5.250369548797607
Batch 10, val loss:2.0428829193115234
Batch 20, val loss:3.6213650703430176
Batch 30, val loss:1.1635663509368896
Epoch 4, Train Loss:0.2785153065362702, Val loss:4.206923966606458
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27852
wandb:         Val Loss 4.20692
wandb:      train_batch 110
wandb: train_batch_loss 0.32212
wandb:        val_batch 30
wandb:   val_batch_loss 1.16357
wandb: 
wandb: üöÄ View run happy-violet-352 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/7nt8e2w8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_025421-7nt8e2w8/logs
Seed completed execution! 42 0.8_5
------------------------------------------------------------------
Running for seed 89 of experiment 0.8_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_030207-orvfq2nx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-resonance-354
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/orvfq2nx
Epoch 0, Batch 0, train loss:8.513291358947754, Elapsed time for epoch : 0.012925529479980468
Epoch 0, Batch 10, train loss:4.180898189544678, Elapsed time for epoch : 0.1289600173632304
Epoch 0, Batch 20, train loss:3.6267616748809814, Elapsed time for epoch : 0.2450009028116862
Epoch 0, Batch 30, train loss:3.2001330852508545, Elapsed time for epoch : 0.36118642886479696
Epoch 0, Batch 40, train loss:3.311025381088257, Elapsed time for epoch : 0.47728523015975954
Epoch 0, Batch 50, train loss:2.7424309253692627, Elapsed time for epoch : 0.5932262778282166
Epoch 0, Batch 60, train loss:2.866588830947876, Elapsed time for epoch : 0.7098256905873617
Epoch 0, Batch 70, train loss:2.779675245285034, Elapsed time for epoch : 0.8263600031534831
Epoch 0, Batch 80, train loss:2.8326549530029297, Elapsed time for epoch : 0.9427030523618062
Epoch 0, Batch 90, train loss:2.6184070110321045, Elapsed time for epoch : 1.0592502156893413
Epoch 0, Batch 100, train loss:2.2400755882263184, Elapsed time for epoch : 1.1756439050038656
Epoch 0, Batch 110, train loss:2.1169261932373047, Elapsed time for epoch : 1.2920969049135844
Batch 0, val loss:6.8978776931762695
Batch 10, val loss:6.715299129486084
Batch 20, val loss:3.5479390621185303
Batch 30, val loss:9.957322120666504
Epoch 0, Train Loss:3.2569123776062674, Val loss:4.829406804508633
Epoch 1, Batch 0, train loss:2.138335943222046, Elapsed time for epoch : 0.011646755536397298
Epoch 1, Batch 10, train loss:1.9288142919540405, Elapsed time for epoch : 0.12829033533732095
Epoch 1, Batch 20, train loss:1.6226396560668945, Elapsed time for epoch : 0.2447689414024353
Epoch 1, Batch 30, train loss:1.6346712112426758, Elapsed time for epoch : 0.3608733057975769
Epoch 1, Batch 40, train loss:1.698508381843567, Elapsed time for epoch : 0.47746909856796266
Epoch 1, Batch 50, train loss:1.5913894176483154, Elapsed time for epoch : 0.5936198512713114
Epoch 1, Batch 60, train loss:1.4703236818313599, Elapsed time for epoch : 0.7099977771441142
Epoch 1, Batch 70, train loss:1.4855797290802002, Elapsed time for epoch : 0.826212207476298
Epoch 1, Batch 80, train loss:1.38066565990448, Elapsed time for epoch : 0.9425927599271139
Epoch 1, Batch 90, train loss:1.2863370180130005, Elapsed time for epoch : 1.0588864843050638
Epoch 1, Batch 100, train loss:0.9612608551979065, Elapsed time for epoch : 1.17527547677358
Epoch 1, Batch 110, train loss:1.0018376111984253, Elapsed time for epoch : 1.2916903098424275
Batch 0, val loss:2.8706533908843994
Batch 10, val loss:1.960553526878357
Batch 20, val loss:2.388474941253662
Batch 30, val loss:2.6824848651885986
Epoch 1, Train Loss:1.4753926572592362, Val loss:3.058097696966595
Epoch 2, Batch 0, train loss:0.9491936564445496, Elapsed time for epoch : 0.011761864026387533
Epoch 2, Batch 10, train loss:1.0148030519485474, Elapsed time for epoch : 0.12825454076131185
Epoch 2, Batch 20, train loss:1.0450317859649658, Elapsed time for epoch : 0.2447413444519043
Epoch 2, Batch 30, train loss:0.9534105062484741, Elapsed time for epoch : 0.3610140363375346
Epoch 2, Batch 40, train loss:0.9440765976905823, Elapsed time for epoch : 0.4782067616780599
Epoch 2, Batch 50, train loss:1.0704208612442017, Elapsed time for epoch : 0.5943248470624288
Epoch 2, Batch 60, train loss:0.9050626158714294, Elapsed time for epoch : 0.7106601436932881
Epoch 2, Batch 70, train loss:0.39578527212142944, Elapsed time for epoch : 0.8269380966822306
Epoch 2, Batch 80, train loss:0.8265743255615234, Elapsed time for epoch : 0.9437024275461833
Epoch 2, Batch 90, train loss:0.8194169998168945, Elapsed time for epoch : 1.0598934531211852
Epoch 2, Batch 100, train loss:0.9761618375778198, Elapsed time for epoch : 1.176660140355428
Epoch 2, Batch 110, train loss:0.688895046710968, Elapsed time for epoch : 1.293015758196513
Batch 0, val loss:2.956672430038452
Batch 10, val loss:3.1261372566223145
Batch 20, val loss:1.7011984586715698
Batch 30, val loss:2.5490190982818604
Epoch 2, Train Loss:0.8054561521696008, Val loss:3.8704316342870393
Epoch 3, Batch 0, train loss:0.6220940351486206, Elapsed time for epoch : 0.011815404891967774
Epoch 3, Batch 10, train loss:0.6383910775184631, Elapsed time for epoch : 0.1283517797787984
Epoch 3, Batch 20, train loss:0.8586597442626953, Elapsed time for epoch : 0.24462912877400717
Epoch 3, Batch 30, train loss:0.5649629831314087, Elapsed time for epoch : 0.3606486757596334
Epoch 3, Batch 40, train loss:0.613006591796875, Elapsed time for epoch : 0.4770925521850586
Epoch 3, Batch 50, train loss:0.17586924135684967, Elapsed time for epoch : 0.5934013525644938
Epoch 3, Batch 60, train loss:0.5325947403907776, Elapsed time for epoch : 0.7094984014829
Epoch 3, Batch 70, train loss:0.6017717123031616, Elapsed time for epoch : 0.8258817513783773
Epoch 3, Batch 80, train loss:0.5630633234977722, Elapsed time for epoch : 0.9423126379648844
Epoch 3, Batch 90, train loss:0.4060295522212982, Elapsed time for epoch : 1.058366052309672
Epoch 3, Batch 100, train loss:0.07474873960018158, Elapsed time for epoch : 1.1745994726816813
Epoch 3, Batch 110, train loss:0.4010932445526123, Elapsed time for epoch : 1.29104483127594
Batch 0, val loss:10.053498268127441
Batch 10, val loss:17.06313705444336
Batch 20, val loss:15.598190307617188
Batch 30, val loss:8.04853630065918
Epoch 3, Train Loss:0.4924223325822664, Val loss:12.490536345375908
Epoch 4, Batch 0, train loss:0.4777776896953583, Elapsed time for epoch : 0.011739277839660644
Epoch 4, Batch 10, train loss:0.1042519360780716, Elapsed time for epoch : 0.12847759326299033
Epoch 4, Batch 20, train loss:0.06812852621078491, Elapsed time for epoch : 0.24450625578562418
Epoch 4, Batch 30, train loss:0.4026232063770294, Elapsed time for epoch : 0.3609606385231018
Epoch 4, Batch 40, train loss:0.27457016706466675, Elapsed time for epoch : 0.4774127840995789
Epoch 4, Batch 50, train loss:0.3712477385997772, Elapsed time for epoch : 0.5940690517425538
Epoch 4, Batch 60, train loss:0.26613107323646545, Elapsed time for epoch : 0.710647197564443
Epoch 4, Batch 70, train loss:0.32098764181137085, Elapsed time for epoch : 0.8270658413569133
Epoch 4, Batch 80, train loss:0.0737694650888443, Elapsed time for epoch : 0.9436410546302796
Epoch 4, Batch 90, train loss:0.24202857911586761, Elapsed time for epoch : 1.0601116339365642
Epoch 4, Batch 100, train loss:0.2784135639667511, Elapsed time for epoch : 1.1764302770296733
Epoch 4, Batch 110, train loss:0.3221166729927063, Elapsed time for epoch : 1.2925092140833536
Batch 0, val loss:5.250369548797607
Batch 10, val loss:2.0428829193115234
Batch 20, val loss:3.6213650703430176
Batch 30, val loss:1.1635663509368896
Epoch 4, Train Loss:0.2785153065362702, Val loss:4.206923966606458
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27852
wandb:         Val Loss 4.20692
wandb:      train_batch 110
wandb: train_batch_loss 0.32212
wandb:        val_batch 30
wandb:   val_batch_loss 1.16357
wandb: 
wandb: üöÄ View run upbeat-resonance-354 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/orvfq2nx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_030207-orvfq2nx/logs
Seed completed execution! 89 0.8_5
------------------------------------------------------------------
Running for seed 23 of experiment 0.8_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_030956-m3qcwmcw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-meadow-356
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m3qcwmcw
Epoch 0, Batch 0, train loss:8.513291358947754, Elapsed time for epoch : 0.013481656710306803
Epoch 0, Batch 10, train loss:4.180898189544678, Elapsed time for epoch : 0.1297267476717631
Epoch 0, Batch 20, train loss:3.6267616748809814, Elapsed time for epoch : 0.24566762844721476
Epoch 0, Batch 30, train loss:3.2001330852508545, Elapsed time for epoch : 0.36220744450887044
Epoch 0, Batch 40, train loss:3.311025381088257, Elapsed time for epoch : 0.4781625350316366
Epoch 0, Batch 50, train loss:2.7424309253692627, Elapsed time for epoch : 0.5941559235254924
Epoch 0, Batch 60, train loss:2.866588830947876, Elapsed time for epoch : 0.7103861212730408
Epoch 0, Batch 70, train loss:2.779675245285034, Elapsed time for epoch : 0.8266331553459167
Epoch 0, Batch 80, train loss:2.8326549530029297, Elapsed time for epoch : 0.9428102691968282
Epoch 0, Batch 90, train loss:2.6184070110321045, Elapsed time for epoch : 1.0590280652046205
Epoch 0, Batch 100, train loss:2.2400755882263184, Elapsed time for epoch : 1.1753474275271099
Epoch 0, Batch 110, train loss:2.1169261932373047, Elapsed time for epoch : 1.2914700428644816
Batch 0, val loss:6.8978776931762695
Batch 10, val loss:6.715299129486084
Batch 20, val loss:3.5479390621185303
Batch 30, val loss:9.957322120666504
Epoch 0, Train Loss:3.2569123776062674, Val loss:4.829406804508633
Epoch 1, Batch 0, train loss:2.138335943222046, Elapsed time for epoch : 0.011638832092285157
Epoch 1, Batch 10, train loss:1.9288142919540405, Elapsed time for epoch : 0.12775824069976807
Epoch 1, Batch 20, train loss:1.6226396560668945, Elapsed time for epoch : 0.24415326515833538
Epoch 1, Batch 30, train loss:1.6346712112426758, Elapsed time for epoch : 0.36072450478871665
Epoch 1, Batch 40, train loss:1.698508381843567, Elapsed time for epoch : 0.4771644830703735
Epoch 1, Batch 50, train loss:1.5913894176483154, Elapsed time for epoch : 0.5936455249786377
Epoch 1, Batch 60, train loss:1.4703236818313599, Elapsed time for epoch : 0.7100805600484212
Epoch 1, Batch 70, train loss:1.4855797290802002, Elapsed time for epoch : 0.826341986656189
Epoch 1, Batch 80, train loss:1.38066565990448, Elapsed time for epoch : 0.9425712784131368
Epoch 1, Batch 90, train loss:1.2863370180130005, Elapsed time for epoch : 1.0590882420539856
Epoch 1, Batch 100, train loss:0.9612608551979065, Elapsed time for epoch : 1.1753894408543906
Epoch 1, Batch 110, train loss:1.0018376111984253, Elapsed time for epoch : 1.2917924006779988
Batch 0, val loss:2.8706533908843994
Batch 10, val loss:1.960553526878357
Batch 20, val loss:2.388474941253662
Batch 30, val loss:2.6824848651885986
Epoch 1, Train Loss:1.4753926572592362, Val loss:3.058097696966595
Epoch 2, Batch 0, train loss:0.9491936564445496, Elapsed time for epoch : 0.01167067289352417
Epoch 2, Batch 10, train loss:1.0148030519485474, Elapsed time for epoch : 0.12814799944559732
Epoch 2, Batch 20, train loss:1.0450317859649658, Elapsed time for epoch : 0.24478485981623332
Epoch 2, Batch 30, train loss:0.9534105062484741, Elapsed time for epoch : 0.3609746336936951
Epoch 2, Batch 40, train loss:0.9440765976905823, Elapsed time for epoch : 0.47804535230000816
Epoch 2, Batch 50, train loss:1.0704208612442017, Elapsed time for epoch : 0.5947147170702617
Epoch 2, Batch 60, train loss:0.9050626158714294, Elapsed time for epoch : 0.7108510216077168
Epoch 2, Batch 70, train loss:0.39578527212142944, Elapsed time for epoch : 0.8272603472073873
Epoch 2, Batch 80, train loss:0.8265743255615234, Elapsed time for epoch : 0.9435672203699748
Epoch 2, Batch 90, train loss:0.8194169998168945, Elapsed time for epoch : 1.0598232309023539
Epoch 2, Batch 100, train loss:0.9761618375778198, Elapsed time for epoch : 1.1763046622276305
Epoch 2, Batch 110, train loss:0.688895046710968, Elapsed time for epoch : 1.2930298566818237
Batch 0, val loss:2.956672430038452
Batch 10, val loss:3.1261372566223145
Batch 20, val loss:1.7011984586715698
Batch 30, val loss:2.5490190982818604
Epoch 2, Train Loss:0.8054561521696008, Val loss:3.8704316342870393
Epoch 3, Batch 0, train loss:0.6220940351486206, Elapsed time for epoch : 0.0117919127146403
Epoch 3, Batch 10, train loss:0.6383910775184631, Elapsed time for epoch : 0.12830353180567425
Epoch 3, Batch 20, train loss:0.8586597442626953, Elapsed time for epoch : 0.24520243803660074
Epoch 3, Batch 30, train loss:0.5649629831314087, Elapsed time for epoch : 0.361575988928477
Epoch 3, Batch 40, train loss:0.613006591796875, Elapsed time for epoch : 0.4783582051595052
Epoch 3, Batch 50, train loss:0.17586924135684967, Elapsed time for epoch : 0.5948812246322632
Epoch 3, Batch 60, train loss:0.5325947403907776, Elapsed time for epoch : 0.7113884647687276
Epoch 3, Batch 70, train loss:0.6017717123031616, Elapsed time for epoch : 0.8283430298169454
Epoch 3, Batch 80, train loss:0.5630633234977722, Elapsed time for epoch : 0.9447336316108703
Epoch 3, Batch 90, train loss:0.4060295522212982, Elapsed time for epoch : 1.0618202368418375
Epoch 3, Batch 100, train loss:0.07474873960018158, Elapsed time for epoch : 1.1783161997795104
Epoch 3, Batch 110, train loss:0.4010932445526123, Elapsed time for epoch : 1.2952406088511148
Batch 0, val loss:10.053498268127441
Batch 10, val loss:17.06313705444336
Batch 20, val loss:15.598190307617188
Batch 30, val loss:8.04853630065918
Epoch 3, Train Loss:0.4924223325822664, Val loss:12.490536345375908
Epoch 4, Batch 0, train loss:0.4777776896953583, Elapsed time for epoch : 0.011774893601735432
Epoch 4, Batch 10, train loss:0.1042519360780716, Elapsed time for epoch : 0.12931159734725953
Epoch 4, Batch 20, train loss:0.06812852621078491, Elapsed time for epoch : 0.2468348979949951
Epoch 4, Batch 30, train loss:0.4026232063770294, Elapsed time for epoch : 0.36549493074417116
Epoch 4, Batch 40, train loss:0.27457016706466675, Elapsed time for epoch : 0.4831709901491801
Epoch 4, Batch 50, train loss:0.3712477385997772, Elapsed time for epoch : 0.6005348881085714
Epoch 4, Batch 60, train loss:0.26613107323646545, Elapsed time for epoch : 0.7183323542277018
Epoch 4, Batch 70, train loss:0.32098764181137085, Elapsed time for epoch : 0.836447274684906
Epoch 4, Batch 80, train loss:0.0737694650888443, Elapsed time for epoch : 0.9540696422259013
Epoch 4, Batch 90, train loss:0.24202857911586761, Elapsed time for epoch : 1.0711952646573384
Epoch 4, Batch 100, train loss:0.2784135639667511, Elapsed time for epoch : 1.1882904012997946
Epoch 4, Batch 110, train loss:0.3221166729927063, Elapsed time for epoch : 1.3057223717371622
Batch 0, val loss:5.250369548797607
Batch 10, val loss:2.0428829193115234
Batch 20, val loss:3.6213650703430176
Batch 30, val loss:1.1635663509368896
Epoch 4, Train Loss:0.2785153065362702, Val loss:4.206923966606458
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27852
wandb:         Val Loss 4.20692
wandb:      train_batch 110
wandb: train_batch_loss 0.32212
wandb:        val_batch 30
wandb:   val_batch_loss 1.16357
wandb: 
wandb: üöÄ View run logical-meadow-356 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m3qcwmcw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_030956-m3qcwmcw/logs
Seed completed execution! 23 0.8_5
------------------------------------------------------------------
Running for seed 113 of experiment 0.8_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_031744-el8vtx5y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sun-358
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/el8vtx5y
Epoch 0, Batch 0, train loss:8.513291358947754, Elapsed time for epoch : 0.013317735989888509
Epoch 0, Batch 10, train loss:4.180898189544678, Elapsed time for epoch : 0.12927149931589763
Epoch 0, Batch 20, train loss:3.6267616748809814, Elapsed time for epoch : 0.2450240135192871
Epoch 0, Batch 30, train loss:3.2001330852508545, Elapsed time for epoch : 0.36082229216893513
Epoch 0, Batch 40, train loss:3.311025381088257, Elapsed time for epoch : 0.47728612025578815
Epoch 0, Batch 50, train loss:2.7424309253692627, Elapsed time for epoch : 0.5931211948394776
Epoch 0, Batch 60, train loss:2.866588830947876, Elapsed time for epoch : 0.709290063381195
Epoch 0, Batch 70, train loss:2.779675245285034, Elapsed time for epoch : 0.8250957489013672
Epoch 0, Batch 80, train loss:2.8326549530029297, Elapsed time for epoch : 0.941535480817159
Epoch 0, Batch 90, train loss:2.6184070110321045, Elapsed time for epoch : 1.0575848380724588
Epoch 0, Batch 100, train loss:2.2400755882263184, Elapsed time for epoch : 1.1739305098851522
Epoch 0, Batch 110, train loss:2.1169261932373047, Elapsed time for epoch : 1.290095321337382
Batch 0, val loss:6.8978776931762695
Batch 10, val loss:6.715299129486084
Batch 20, val loss:3.5479390621185303
Batch 30, val loss:9.957322120666504
Epoch 0, Train Loss:3.2569123776062674, Val loss:4.829406804508633
Epoch 1, Batch 0, train loss:2.138335943222046, Elapsed time for epoch : 0.011638474464416505
Epoch 1, Batch 10, train loss:1.9288142919540405, Elapsed time for epoch : 0.12809736728668214
Epoch 1, Batch 20, train loss:1.6226396560668945, Elapsed time for epoch : 0.24440435965855917
Epoch 1, Batch 30, train loss:1.6346712112426758, Elapsed time for epoch : 0.36058876911799115
Epoch 1, Batch 40, train loss:1.698508381843567, Elapsed time for epoch : 0.4770639220873515
Epoch 1, Batch 50, train loss:1.5913894176483154, Elapsed time for epoch : 0.593286395072937
Epoch 1, Batch 60, train loss:1.4703236818313599, Elapsed time for epoch : 0.7100530505180359
Epoch 1, Batch 70, train loss:1.4855797290802002, Elapsed time for epoch : 0.8266436338424683
Epoch 1, Batch 80, train loss:1.38066565990448, Elapsed time for epoch : 0.9431492487589518
Epoch 1, Batch 90, train loss:1.2863370180130005, Elapsed time for epoch : 1.0599878390630086
Epoch 1, Batch 100, train loss:0.9612608551979065, Elapsed time for epoch : 1.176245625813802
Epoch 1, Batch 110, train loss:1.0018376111984253, Elapsed time for epoch : 1.2926074743270874
Batch 0, val loss:2.8706533908843994
Batch 10, val loss:1.960553526878357
Batch 20, val loss:2.388474941253662
Batch 30, val loss:2.6824848651885986
Epoch 1, Train Loss:1.4753926572592362, Val loss:3.058097696966595
Epoch 2, Batch 0, train loss:0.9491936564445496, Elapsed time for epoch : 0.011705752213795979
Epoch 2, Batch 10, train loss:1.0148030519485474, Elapsed time for epoch : 0.12787541151046752
Epoch 2, Batch 20, train loss:1.0450317859649658, Elapsed time for epoch : 0.2443858782450358
Epoch 2, Batch 30, train loss:0.9534105062484741, Elapsed time for epoch : 0.3608474493026733
Epoch 2, Batch 40, train loss:0.9440765976905823, Elapsed time for epoch : 0.47745064496994016
Epoch 2, Batch 50, train loss:1.0704208612442017, Elapsed time for epoch : 0.5939502755800883
Epoch 2, Batch 60, train loss:0.9050626158714294, Elapsed time for epoch : 0.7105276505152385
Epoch 2, Batch 70, train loss:0.39578527212142944, Elapsed time for epoch : 0.8267054835955302
Epoch 2, Batch 80, train loss:0.8265743255615234, Elapsed time for epoch : 0.943389642238617
Epoch 2, Batch 90, train loss:0.8194169998168945, Elapsed time for epoch : 1.0604220390319825
Epoch 2, Batch 100, train loss:0.9761618375778198, Elapsed time for epoch : 1.1770700931549072
Epoch 2, Batch 110, train loss:0.688895046710968, Elapsed time for epoch : 1.2934197147687276
Batch 0, val loss:2.956672430038452
Batch 10, val loss:3.1261372566223145
Batch 20, val loss:1.7011984586715698
Batch 30, val loss:2.5490190982818604
Epoch 2, Train Loss:0.8054561521696008, Val loss:3.8704316342870393
Epoch 3, Batch 0, train loss:0.6220940351486206, Elapsed time for epoch : 0.011666405200958251
Epoch 3, Batch 10, train loss:0.6383910775184631, Elapsed time for epoch : 0.1278328776359558
Epoch 3, Batch 20, train loss:0.8586597442626953, Elapsed time for epoch : 0.24420408407847086
Epoch 3, Batch 30, train loss:0.5649629831314087, Elapsed time for epoch : 0.3608304063479106
Epoch 3, Batch 40, train loss:0.613006591796875, Elapsed time for epoch : 0.47707630395889283
Epoch 3, Batch 50, train loss:0.17586924135684967, Elapsed time for epoch : 0.5936152696609497
Epoch 3, Batch 60, train loss:0.5325947403907776, Elapsed time for epoch : 0.7100006143252054
Epoch 3, Batch 70, train loss:0.6017717123031616, Elapsed time for epoch : 0.8267444610595703
Epoch 3, Batch 80, train loss:0.5630633234977722, Elapsed time for epoch : 0.9433174729347229
Epoch 3, Batch 90, train loss:0.4060295522212982, Elapsed time for epoch : 1.0599420070648193
Epoch 3, Batch 100, train loss:0.07474873960018158, Elapsed time for epoch : 1.176796833674113
Epoch 3, Batch 110, train loss:0.4010932445526123, Elapsed time for epoch : 1.2931150356928507
Batch 0, val loss:10.053498268127441
Batch 10, val loss:17.06313705444336
Batch 20, val loss:15.598190307617188
Batch 30, val loss:8.04853630065918
Epoch 3, Train Loss:0.4924223325822664, Val loss:12.490536345375908
Epoch 4, Batch 0, train loss:0.4777776896953583, Elapsed time for epoch : 0.011944218476613363
Epoch 4, Batch 10, train loss:0.1042519360780716, Elapsed time for epoch : 0.12849915822347005
Epoch 4, Batch 20, train loss:0.06812852621078491, Elapsed time for epoch : 0.24509084224700928
Epoch 4, Batch 30, train loss:0.4026232063770294, Elapsed time for epoch : 0.3618642052014669
Epoch 4, Batch 40, train loss:0.27457016706466675, Elapsed time for epoch : 0.4784820318222046
Epoch 4, Batch 50, train loss:0.3712477385997772, Elapsed time for epoch : 0.5953699588775635
Epoch 4, Batch 60, train loss:0.26613107323646545, Elapsed time for epoch : 0.7118095835049947
Epoch 4, Batch 70, train loss:0.32098764181137085, Elapsed time for epoch : 0.8288808782895406
Epoch 4, Batch 80, train loss:0.0737694650888443, Elapsed time for epoch : 0.9460326115290324
Epoch 4, Batch 90, train loss:0.24202857911586761, Elapsed time for epoch : 1.0633034388224283
Epoch 4, Batch 100, train loss:0.2784135639667511, Elapsed time for epoch : 1.1801025946935018
Epoch 4, Batch 110, train loss:0.3221166729927063, Elapsed time for epoch : 1.2965703010559082
Batch 0, val loss:5.250369548797607
Batch 10, val loss:2.0428829193115234
Batch 20, val loss:3.6213650703430176
Batch 30, val loss:1.1635663509368896
Epoch 4, Train Loss:0.2785153065362702, Val loss:4.206923966606458
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27852
wandb:         Val Loss 4.20692
wandb:      train_batch 110
wandb: train_batch_loss 0.32212
wandb:        val_batch 30
wandb:   val_batch_loss 1.16357
wandb: 
wandb: üöÄ View run rose-sun-358 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/el8vtx5y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_031744-el8vtx5y/logs
Seed completed execution! 113 0.8_5
------------------------------------------------------------------
Experiment complete 0.8_5
==========================================================================
Running experiment for setting 0.9_1
==========================================================================
Running for seed 1 of experiment 0.9_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_032532-wa08p29p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-darkness-360
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wa08p29p
Epoch 0, Batch 0, train loss:8.617968559265137, Elapsed time for epoch : 0.0135604461034139
Epoch 0, Batch 10, train loss:3.0437541007995605, Elapsed time for epoch : 0.1318384846051534
Epoch 0, Batch 20, train loss:2.8108584880828857, Elapsed time for epoch : 0.25014338493347166
Epoch 0, Batch 30, train loss:2.356714963912964, Elapsed time for epoch : 0.36858396530151366
Epoch 0, Batch 40, train loss:2.185683488845825, Elapsed time for epoch : 0.48704038858413695
Epoch 0, Batch 50, train loss:2.0015861988067627, Elapsed time for epoch : 0.606150225798289
Epoch 0, Batch 60, train loss:1.916375994682312, Elapsed time for epoch : 0.7242734630902609
Epoch 0, Batch 70, train loss:2.008574962615967, Elapsed time for epoch : 0.8420895099639892
Epoch 0, Batch 80, train loss:1.7371349334716797, Elapsed time for epoch : 0.9609273513158162
Epoch 0, Batch 90, train loss:1.71402108669281, Elapsed time for epoch : 1.0789875507354736
Epoch 0, Batch 100, train loss:1.6612708568572998, Elapsed time for epoch : 1.1971625089645386
Epoch 0, Batch 110, train loss:1.5555684566497803, Elapsed time for epoch : 1.314821187655131
Batch 0, val loss:3.2562832832336426
Batch 10, val loss:4.332774639129639
Batch 20, val loss:3.29187273979187
Batch 30, val loss:4.624602317810059
Epoch 0, Train Loss:3.2430748006571894, Val loss:4.492783493465847
Epoch 1, Batch 0, train loss:1.6219571828842163, Elapsed time for epoch : 0.011613837877909343
Epoch 1, Batch 10, train loss:1.5798577070236206, Elapsed time for epoch : 0.1274702270825704
Epoch 1, Batch 20, train loss:1.4731136560440063, Elapsed time for epoch : 0.24354934692382812
Epoch 1, Batch 30, train loss:1.5873140096664429, Elapsed time for epoch : 0.35972463687260947
Epoch 1, Batch 40, train loss:1.4943712949752808, Elapsed time for epoch : 0.4765462676684062
Epoch 1, Batch 50, train loss:1.4320224523544312, Elapsed time for epoch : 0.5926119327545166
Epoch 1, Batch 60, train loss:1.2909296751022339, Elapsed time for epoch : 0.7087975223859151
Epoch 1, Batch 70, train loss:1.3400611877441406, Elapsed time for epoch : 0.8250411907831828
Epoch 1, Batch 80, train loss:1.3458272218704224, Elapsed time for epoch : 0.9410909334818522
Epoch 1, Batch 90, train loss:1.2537199258804321, Elapsed time for epoch : 1.0571855028470358
Epoch 1, Batch 100, train loss:1.1532319784164429, Elapsed time for epoch : 1.17331307331721
Epoch 1, Batch 110, train loss:1.2100704908370972, Elapsed time for epoch : 1.2895707408587138
Batch 0, val loss:2.3024771213531494
Batch 10, val loss:1.648339867591858
Batch 20, val loss:2.618943691253662
Batch 30, val loss:2.586010217666626
Epoch 1, Train Loss:1.393745145590409, Val loss:8.679304503732258
Epoch 2, Batch 0, train loss:1.246351957321167, Elapsed time for epoch : 0.011618518829345703
Epoch 2, Batch 10, train loss:1.2061312198638916, Elapsed time for epoch : 0.12785597642262778
Epoch 2, Batch 20, train loss:1.2042337656021118, Elapsed time for epoch : 0.2439323623975118
Epoch 2, Batch 30, train loss:1.1385809183120728, Elapsed time for epoch : 0.36011955738067625
Epoch 2, Batch 40, train loss:1.1568511724472046, Elapsed time for epoch : 0.4763262669245402
Epoch 2, Batch 50, train loss:1.2039413452148438, Elapsed time for epoch : 0.5926831960678101
Epoch 2, Batch 60, train loss:1.1398364305496216, Elapsed time for epoch : 0.7088767210642497
Epoch 2, Batch 70, train loss:0.8792173266410828, Elapsed time for epoch : 0.8251554926236471
Epoch 2, Batch 80, train loss:1.1459378004074097, Elapsed time for epoch : 0.9418548266092936
Epoch 2, Batch 90, train loss:0.9982935786247253, Elapsed time for epoch : 1.0577953616778055
Epoch 2, Batch 100, train loss:1.0612632036209106, Elapsed time for epoch : 1.1743270397186278
Epoch 2, Batch 110, train loss:1.0928609371185303, Elapsed time for epoch : 1.2904501795768737
Batch 0, val loss:1.4890601634979248
Batch 10, val loss:2.23394513130188
Batch 20, val loss:2.8225789070129395
Batch 30, val loss:25.796688079833984
Epoch 2, Train Loss:1.1095941953037096, Val loss:7.502203981081645
Epoch 3, Batch 0, train loss:1.0691869258880615, Elapsed time for epoch : 0.011636912822723389
Epoch 3, Batch 10, train loss:1.0199440717697144, Elapsed time for epoch : 0.12785312334696453
Epoch 3, Batch 20, train loss:1.0874806642532349, Elapsed time for epoch : 0.24393947124481202
Epoch 3, Batch 30, train loss:1.0132533311843872, Elapsed time for epoch : 0.3604286511739095
Epoch 3, Batch 40, train loss:0.9909375309944153, Elapsed time for epoch : 0.4765046199162801
Epoch 3, Batch 50, train loss:0.727765679359436, Elapsed time for epoch : 0.5926928838094075
Epoch 3, Batch 60, train loss:0.9802255034446716, Elapsed time for epoch : 0.7089975873629252
Epoch 3, Batch 70, train loss:1.085822343826294, Elapsed time for epoch : 0.8251729846000672
Epoch 3, Batch 80, train loss:1.027030110359192, Elapsed time for epoch : 0.9409321228663127
Epoch 3, Batch 90, train loss:1.0379081964492798, Elapsed time for epoch : 1.0573419014612833
Epoch 3, Batch 100, train loss:0.8881585001945496, Elapsed time for epoch : 1.1740174611409506
Epoch 3, Batch 110, train loss:0.8704065680503845, Elapsed time for epoch : 1.2901084780693055
Batch 0, val loss:23.46611213684082
Batch 10, val loss:5.135997772216797
Batch 20, val loss:31.922208786010742
Batch 30, val loss:3.5266640186309814
Epoch 3, Train Loss:0.9951850429825161, Val loss:10.776046769486534
Epoch 4, Batch 0, train loss:1.057687520980835, Elapsed time for epoch : 0.011640699704488118
Epoch 4, Batch 10, train loss:0.8411617875099182, Elapsed time for epoch : 0.1276501178741455
Epoch 4, Batch 20, train loss:0.8223633766174316, Elapsed time for epoch : 0.24402788877487183
Epoch 4, Batch 30, train loss:0.894866943359375, Elapsed time for epoch : 0.36038597822189333
Epoch 4, Batch 40, train loss:1.0448665618896484, Elapsed time for epoch : 0.4768694798151652
Epoch 4, Batch 50, train loss:0.949726939201355, Elapsed time for epoch : 0.593149455388387
Epoch 4, Batch 60, train loss:0.9361362457275391, Elapsed time for epoch : 0.7095163981119792
Epoch 4, Batch 70, train loss:0.9825283885002136, Elapsed time for epoch : 0.8260250608126323
Epoch 4, Batch 80, train loss:0.902313768863678, Elapsed time for epoch : 0.9426602840423584
Epoch 4, Batch 90, train loss:0.9590429067611694, Elapsed time for epoch : 1.0586533188819884
Epoch 4, Batch 100, train loss:0.9202283024787903, Elapsed time for epoch : 1.1748792608579
Epoch 4, Batch 110, train loss:0.9223398566246033, Elapsed time for epoch : 1.291746433575948
Batch 0, val loss:2.744554042816162
Batch 10, val loss:1.7965399026870728
Batch 20, val loss:2.3988595008850098
Batch 30, val loss:2.356616497039795
Epoch 4, Train Loss:0.9302988933480304, Val loss:6.152291787995233
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.9303
wandb:         Val Loss 6.15229
wandb:      train_batch 110
wandb: train_batch_loss 0.92234
wandb:        val_batch 30
wandb:   val_batch_loss 2.35662
wandb: 
wandb: üöÄ View run earthy-darkness-360 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wa08p29p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_032532-wa08p29p/logs
Seed completed execution! 1 0.9_1
------------------------------------------------------------------
Running for seed 42 of experiment 0.9_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_033322-suv9cuyi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-donkey-362
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/suv9cuyi
Epoch 0, Batch 0, train loss:8.617968559265137, Elapsed time for epoch : 0.013197914759318034
Epoch 0, Batch 10, train loss:3.0437541007995605, Elapsed time for epoch : 0.12932549715042113
Epoch 0, Batch 20, train loss:2.8108584880828857, Elapsed time for epoch : 0.24517860412597656
Epoch 0, Batch 30, train loss:2.356714963912964, Elapsed time for epoch : 0.36101383368174234
Epoch 0, Batch 40, train loss:2.185683488845825, Elapsed time for epoch : 0.4769426981608073
Epoch 0, Batch 50, train loss:2.0015861988067627, Elapsed time for epoch : 0.5927316188812256
Epoch 0, Batch 60, train loss:1.916375994682312, Elapsed time for epoch : 0.7086605866750081
Epoch 0, Batch 70, train loss:2.008574962615967, Elapsed time for epoch : 0.8243016481399537
Epoch 0, Batch 80, train loss:1.7371349334716797, Elapsed time for epoch : 0.9406559626261394
Epoch 0, Batch 90, train loss:1.71402108669281, Elapsed time for epoch : 1.056363316377004
Epoch 0, Batch 100, train loss:1.6612708568572998, Elapsed time for epoch : 1.1731404622395833
Epoch 0, Batch 110, train loss:1.5555684566497803, Elapsed time for epoch : 1.2892202734947205
Batch 0, val loss:3.2562832832336426
Batch 10, val loss:4.332774639129639
Batch 20, val loss:3.29187273979187
Batch 30, val loss:4.624602317810059
Epoch 0, Train Loss:3.2430748006571894, Val loss:4.492783493465847
Epoch 1, Batch 0, train loss:1.6219571828842163, Elapsed time for epoch : 0.011815480391184489
Epoch 1, Batch 10, train loss:1.5798577070236206, Elapsed time for epoch : 0.1279425899187724
Epoch 1, Batch 20, train loss:1.4731136560440063, Elapsed time for epoch : 0.24422168731689453
Epoch 1, Batch 30, train loss:1.5873140096664429, Elapsed time for epoch : 0.3603681166966756
Epoch 1, Batch 40, train loss:1.4943712949752808, Elapsed time for epoch : 0.47630878289540607
Epoch 1, Batch 50, train loss:1.4320224523544312, Elapsed time for epoch : 0.5925655762354533
Epoch 1, Batch 60, train loss:1.2909296751022339, Elapsed time for epoch : 0.7087870438893636
Epoch 1, Batch 70, train loss:1.3400611877441406, Elapsed time for epoch : 0.825073528289795
Epoch 1, Batch 80, train loss:1.3458272218704224, Elapsed time for epoch : 0.9407729387283326
Epoch 1, Batch 90, train loss:1.2537199258804321, Elapsed time for epoch : 1.056852149963379
Epoch 1, Batch 100, train loss:1.1532319784164429, Elapsed time for epoch : 1.1729404131571453
Epoch 1, Batch 110, train loss:1.2100704908370972, Elapsed time for epoch : 1.2890302062034606
Batch 0, val loss:2.3024771213531494
Batch 10, val loss:1.648339867591858
Batch 20, val loss:2.618943691253662
Batch 30, val loss:2.586010217666626
Epoch 1, Train Loss:1.393745145590409, Val loss:8.679304503732258
Epoch 2, Batch 0, train loss:1.246351957321167, Elapsed time for epoch : 0.011746434370676677
Epoch 2, Batch 10, train loss:1.2061312198638916, Elapsed time for epoch : 0.12770621379216512
Epoch 2, Batch 20, train loss:1.2042337656021118, Elapsed time for epoch : 0.24402795632680258
Epoch 2, Batch 30, train loss:1.1385809183120728, Elapsed time for epoch : 0.36018507877985634
Epoch 2, Batch 40, train loss:1.1568511724472046, Elapsed time for epoch : 0.47662966648737587
Epoch 2, Batch 50, train loss:1.2039413452148438, Elapsed time for epoch : 0.5929224491119385
Epoch 2, Batch 60, train loss:1.1398364305496216, Elapsed time for epoch : 0.7088983098665873
Epoch 2, Batch 70, train loss:0.8792173266410828, Elapsed time for epoch : 0.8250638008117676
Epoch 2, Batch 80, train loss:1.1459378004074097, Elapsed time for epoch : 0.9413785219192505
Epoch 2, Batch 90, train loss:0.9982935786247253, Elapsed time for epoch : 1.0579380512237548
Epoch 2, Batch 100, train loss:1.0612632036209106, Elapsed time for epoch : 1.174464507897695
Epoch 2, Batch 110, train loss:1.0928609371185303, Elapsed time for epoch : 1.2905572017033895
Batch 0, val loss:1.4890601634979248
Batch 10, val loss:2.23394513130188
Batch 20, val loss:2.8225789070129395
Batch 30, val loss:25.796688079833984
Epoch 2, Train Loss:1.1095941953037096, Val loss:7.502203981081645
Epoch 3, Batch 0, train loss:1.0691869258880615, Elapsed time for epoch : 0.011624244848887126
Epoch 3, Batch 10, train loss:1.0199440717697144, Elapsed time for epoch : 0.12771662871042888
Epoch 3, Batch 20, train loss:1.0874806642532349, Elapsed time for epoch : 0.24401710033416749
Epoch 3, Batch 30, train loss:1.0132533311843872, Elapsed time for epoch : 0.3601413567860921
Epoch 3, Batch 40, train loss:0.9909375309944153, Elapsed time for epoch : 0.47616273164749146
Epoch 3, Batch 50, train loss:0.727765679359436, Elapsed time for epoch : 0.5923114895820618
Epoch 3, Batch 60, train loss:0.9802255034446716, Elapsed time for epoch : 0.7086332122484843
Epoch 3, Batch 70, train loss:1.085822343826294, Elapsed time for epoch : 0.8246853073438009
Epoch 3, Batch 80, train loss:1.027030110359192, Elapsed time for epoch : 0.9408939758936564
Epoch 3, Batch 90, train loss:1.0379081964492798, Elapsed time for epoch : 1.0569948514302572
Epoch 3, Batch 100, train loss:0.8881585001945496, Elapsed time for epoch : 1.172823158899943
Epoch 3, Batch 110, train loss:0.8704065680503845, Elapsed time for epoch : 1.2890055775642395
Batch 0, val loss:23.46611213684082
Batch 10, val loss:5.135997772216797
Batch 20, val loss:31.922208786010742
Batch 30, val loss:3.5266640186309814
Epoch 3, Train Loss:0.9951850429825161, Val loss:10.776046769486534
Epoch 4, Batch 0, train loss:1.057687520980835, Elapsed time for epoch : 0.011619814236958821
Epoch 4, Batch 10, train loss:0.8411617875099182, Elapsed time for epoch : 0.12805734078089395
Epoch 4, Batch 20, train loss:0.8223633766174316, Elapsed time for epoch : 0.24404675563176473
Epoch 4, Batch 30, train loss:0.894866943359375, Elapsed time for epoch : 0.36017544666926066
Epoch 4, Batch 40, train loss:1.0448665618896484, Elapsed time for epoch : 0.47701253096262614
Epoch 4, Batch 50, train loss:0.949726939201355, Elapsed time for epoch : 0.5931554198265075
Epoch 4, Batch 60, train loss:0.9361362457275391, Elapsed time for epoch : 0.709610625108083
Epoch 4, Batch 70, train loss:0.9825283885002136, Elapsed time for epoch : 0.8260906577110291
Epoch 4, Batch 80, train loss:0.902313768863678, Elapsed time for epoch : 0.9420875628789266
Epoch 4, Batch 90, train loss:0.9590429067611694, Elapsed time for epoch : 1.0584187507629395
Epoch 4, Batch 100, train loss:0.9202283024787903, Elapsed time for epoch : 1.1745761473973593
Epoch 4, Batch 110, train loss:0.9223398566246033, Elapsed time for epoch : 1.2906540036201477
Batch 0, val loss:2.744554042816162
Batch 10, val loss:1.7965399026870728
Batch 20, val loss:2.3988595008850098
Batch 30, val loss:2.356616497039795
Epoch 4, Train Loss:0.9302988933480304, Val loss:6.152291787995233
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.9303
wandb:         Val Loss 6.15229
wandb:      train_batch 110
wandb: train_batch_loss 0.92234
wandb:        val_batch 30
wandb:   val_batch_loss 2.35662
wandb: 
wandb: üöÄ View run dauntless-donkey-362 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/suv9cuyi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_033322-suv9cuyi/logs
Seed completed execution! 42 0.9_1
------------------------------------------------------------------
Running for seed 89 of experiment 0.9_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_034108-s9llzedl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-microwave-364
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/s9llzedl
Epoch 0, Batch 0, train loss:8.617968559265137, Elapsed time for epoch : 0.01345607042312622
Epoch 0, Batch 10, train loss:3.0437541007995605, Elapsed time for epoch : 0.12913663784662882
Epoch 0, Batch 20, train loss:2.8108584880828857, Elapsed time for epoch : 0.24541465044021607
Epoch 0, Batch 30, train loss:2.356714963912964, Elapsed time for epoch : 0.36133909225463867
Epoch 0, Batch 40, train loss:2.185683488845825, Elapsed time for epoch : 0.4775097370147705
Epoch 0, Batch 50, train loss:2.0015861988067627, Elapsed time for epoch : 0.5938335299491883
Epoch 0, Batch 60, train loss:1.916375994682312, Elapsed time for epoch : 0.7097130378087362
Epoch 0, Batch 70, train loss:2.008574962615967, Elapsed time for epoch : 0.8257462700208028
Epoch 0, Batch 80, train loss:1.7371349334716797, Elapsed time for epoch : 0.9418145815531412
Epoch 0, Batch 90, train loss:1.71402108669281, Elapsed time for epoch : 1.057891583442688
Epoch 0, Batch 100, train loss:1.6612708568572998, Elapsed time for epoch : 1.1741745114326476
Epoch 0, Batch 110, train loss:1.5555684566497803, Elapsed time for epoch : 1.2899479230244955
Batch 0, val loss:3.2562832832336426
Batch 10, val loss:4.332774639129639
Batch 20, val loss:3.29187273979187
Batch 30, val loss:4.624602317810059
Epoch 0, Train Loss:3.2430748006571894, Val loss:4.492783493465847
Epoch 1, Batch 0, train loss:1.6219571828842163, Elapsed time for epoch : 0.011626426378885906
Epoch 1, Batch 10, train loss:1.5798577070236206, Elapsed time for epoch : 0.12786177396774293
Epoch 1, Batch 20, train loss:1.4731136560440063, Elapsed time for epoch : 0.2445953925450643
Epoch 1, Batch 30, train loss:1.5873140096664429, Elapsed time for epoch : 0.36088215510050453
Epoch 1, Batch 40, train loss:1.4943712949752808, Elapsed time for epoch : 0.4771296779314677
Epoch 1, Batch 50, train loss:1.4320224523544312, Elapsed time for epoch : 0.5932721932729085
Epoch 1, Batch 60, train loss:1.2909296751022339, Elapsed time for epoch : 0.7093872229258219
Epoch 1, Batch 70, train loss:1.3400611877441406, Elapsed time for epoch : 0.8258179783821106
Epoch 1, Batch 80, train loss:1.3458272218704224, Elapsed time for epoch : 0.9421798944473266
Epoch 1, Batch 90, train loss:1.2537199258804321, Elapsed time for epoch : 1.0583449482917786
Epoch 1, Batch 100, train loss:1.1532319784164429, Elapsed time for epoch : 1.1745283802350361
Epoch 1, Batch 110, train loss:1.2100704908370972, Elapsed time for epoch : 1.2910907546679178
Batch 0, val loss:2.3024771213531494
Batch 10, val loss:1.648339867591858
Batch 20, val loss:2.618943691253662
Batch 30, val loss:2.586010217666626
Epoch 1, Train Loss:1.393745145590409, Val loss:8.679304503732258
Epoch 2, Batch 0, train loss:1.246351957321167, Elapsed time for epoch : 0.011630813280741373
Epoch 2, Batch 10, train loss:1.2061312198638916, Elapsed time for epoch : 0.12787553071975707
Epoch 2, Batch 20, train loss:1.2042337656021118, Elapsed time for epoch : 0.24423425197601317
Epoch 2, Batch 30, train loss:1.1385809183120728, Elapsed time for epoch : 0.36022611856460574
Epoch 2, Batch 40, train loss:1.1568511724472046, Elapsed time for epoch : 0.4771974047025045
Epoch 2, Batch 50, train loss:1.2039413452148438, Elapsed time for epoch : 0.5934186418851216
Epoch 2, Batch 60, train loss:1.1398364305496216, Elapsed time for epoch : 0.7095919648806254
Epoch 2, Batch 70, train loss:0.8792173266410828, Elapsed time for epoch : 0.8260251363118489
Epoch 2, Batch 80, train loss:1.1459378004074097, Elapsed time for epoch : 0.9423471212387085
Epoch 2, Batch 90, train loss:0.9982935786247253, Elapsed time for epoch : 1.0584945638974508
Epoch 2, Batch 100, train loss:1.0612632036209106, Elapsed time for epoch : 1.1749502857526144
Epoch 2, Batch 110, train loss:1.0928609371185303, Elapsed time for epoch : 1.2912645141283672
Batch 0, val loss:1.4890601634979248
Batch 10, val loss:2.23394513130188
Batch 20, val loss:2.8225789070129395
Batch 30, val loss:25.796688079833984
Epoch 2, Train Loss:1.1095941953037096, Val loss:7.502203981081645
Epoch 3, Batch 0, train loss:1.0691869258880615, Elapsed time for epoch : 0.011673510074615479
Epoch 3, Batch 10, train loss:1.0199440717697144, Elapsed time for epoch : 0.12820175091425579
Epoch 3, Batch 20, train loss:1.0874806642532349, Elapsed time for epoch : 0.24443171421686807
Epoch 3, Batch 30, train loss:1.0132533311843872, Elapsed time for epoch : 0.36063749392827354
Epoch 3, Batch 40, train loss:0.9909375309944153, Elapsed time for epoch : 0.47678645451863605
Epoch 3, Batch 50, train loss:0.727765679359436, Elapsed time for epoch : 0.5930522998174032
Epoch 3, Batch 60, train loss:0.9802255034446716, Elapsed time for epoch : 0.7091806729634603
Epoch 3, Batch 70, train loss:1.085822343826294, Elapsed time for epoch : 0.825520924727122
Epoch 3, Batch 80, train loss:1.027030110359192, Elapsed time for epoch : 0.9415819684664408
Epoch 3, Batch 90, train loss:1.0379081964492798, Elapsed time for epoch : 1.0580944935480754
Epoch 3, Batch 100, train loss:0.8881585001945496, Elapsed time for epoch : 1.1744584043820698
Epoch 3, Batch 110, train loss:0.8704065680503845, Elapsed time for epoch : 1.2916451573371888
Batch 0, val loss:23.46611213684082
Batch 10, val loss:5.135997772216797
Batch 20, val loss:31.922208786010742
Batch 30, val loss:3.5266640186309814
Epoch 3, Train Loss:0.9951850429825161, Val loss:10.776046769486534
Epoch 4, Batch 0, train loss:1.057687520980835, Elapsed time for epoch : 0.011621483167012532
Epoch 4, Batch 10, train loss:0.8411617875099182, Elapsed time for epoch : 0.12777434984842936
Epoch 4, Batch 20, train loss:0.8223633766174316, Elapsed time for epoch : 0.2437869111696879
Epoch 4, Batch 30, train loss:0.894866943359375, Elapsed time for epoch : 0.36005297501881917
Epoch 4, Batch 40, train loss:1.0448665618896484, Elapsed time for epoch : 0.4763299067815145
Epoch 4, Batch 50, train loss:0.949726939201355, Elapsed time for epoch : 0.5925418257713317
Epoch 4, Batch 60, train loss:0.9361362457275391, Elapsed time for epoch : 0.7087358832359314
Epoch 4, Batch 70, train loss:0.9825283885002136, Elapsed time for epoch : 0.8250545700391133
Epoch 4, Batch 80, train loss:0.902313768863678, Elapsed time for epoch : 0.9411770423253377
Epoch 4, Batch 90, train loss:0.9590429067611694, Elapsed time for epoch : 1.0574246962865195
Epoch 4, Batch 100, train loss:0.9202283024787903, Elapsed time for epoch : 1.1735763152440388
Epoch 4, Batch 110, train loss:0.9223398566246033, Elapsed time for epoch : 1.289619783560435
Batch 0, val loss:2.744554042816162
Batch 10, val loss:1.7965399026870728
Batch 20, val loss:2.3988595008850098
Batch 30, val loss:2.356616497039795
Epoch 4, Train Loss:0.9302988933480304, Val loss:6.152291787995233
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.9303
wandb:         Val Loss 6.15229
wandb:      train_batch 110
wandb: train_batch_loss 0.92234
wandb:        val_batch 30
wandb:   val_batch_loss 2.35662
wandb: 
wandb: üöÄ View run twilight-microwave-364 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/s9llzedl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_034108-s9llzedl/logs
Seed completed execution! 89 0.9_1
------------------------------------------------------------------
Running for seed 23 of experiment 0.9_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_034855-r9z1i1av
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-spaceship-366
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/r9z1i1av
Epoch 0, Batch 0, train loss:8.617968559265137, Elapsed time for epoch : 0.013434851169586181
Epoch 0, Batch 10, train loss:3.0437541007995605, Elapsed time for epoch : 0.12915734052658082
Epoch 0, Batch 20, train loss:2.8108584880828857, Elapsed time for epoch : 0.2449815551439921
Epoch 0, Batch 30, train loss:2.356714963912964, Elapsed time for epoch : 0.3609969933827718
Epoch 0, Batch 40, train loss:2.185683488845825, Elapsed time for epoch : 0.4774374723434448
Epoch 0, Batch 50, train loss:2.0015861988067627, Elapsed time for epoch : 0.5936035553614298
Epoch 0, Batch 60, train loss:1.916375994682312, Elapsed time for epoch : 0.7093539436658224
Epoch 0, Batch 70, train loss:2.008574962615967, Elapsed time for epoch : 0.825567102432251
Epoch 0, Batch 80, train loss:1.7371349334716797, Elapsed time for epoch : 0.941780670483907
Epoch 0, Batch 90, train loss:1.71402108669281, Elapsed time for epoch : 1.0579935590426126
Epoch 0, Batch 100, train loss:1.6612708568572998, Elapsed time for epoch : 1.1740164120992025
Epoch 0, Batch 110, train loss:1.5555684566497803, Elapsed time for epoch : 1.2899904012680055
Batch 0, val loss:3.2562832832336426
Batch 10, val loss:4.332774639129639
Batch 20, val loss:3.29187273979187
Batch 30, val loss:4.624602317810059
Epoch 0, Train Loss:3.2430748006571894, Val loss:4.492783493465847
Epoch 1, Batch 0, train loss:1.6219571828842163, Elapsed time for epoch : 0.0116256316502889
Epoch 1, Batch 10, train loss:1.5798577070236206, Elapsed time for epoch : 0.1277957042058309
Epoch 1, Batch 20, train loss:1.4731136560440063, Elapsed time for epoch : 0.24395056168238322
Epoch 1, Batch 30, train loss:1.5873140096664429, Elapsed time for epoch : 0.36017119089762367
Epoch 1, Batch 40, train loss:1.4943712949752808, Elapsed time for epoch : 0.4764452656110128
Epoch 1, Batch 50, train loss:1.4320224523544312, Elapsed time for epoch : 0.5928017377853394
Epoch 1, Batch 60, train loss:1.2909296751022339, Elapsed time for epoch : 0.7092084527015686
Epoch 1, Batch 70, train loss:1.3400611877441406, Elapsed time for epoch : 0.8253693024317423
Epoch 1, Batch 80, train loss:1.3458272218704224, Elapsed time for epoch : 0.9412695407867432
Epoch 1, Batch 90, train loss:1.2537199258804321, Elapsed time for epoch : 1.0581721584002177
Epoch 1, Batch 100, train loss:1.1532319784164429, Elapsed time for epoch : 1.1740314364433289
Epoch 1, Batch 110, train loss:1.2100704908370972, Elapsed time for epoch : 1.2903452277183534
Batch 0, val loss:2.3024771213531494
Batch 10, val loss:1.648339867591858
Batch 20, val loss:2.618943691253662
Batch 30, val loss:2.586010217666626
Epoch 1, Train Loss:1.393745145590409, Val loss:8.679304503732258
Epoch 2, Batch 0, train loss:1.246351957321167, Elapsed time for epoch : 0.011615411440531413
Epoch 2, Batch 10, train loss:1.2061312198638916, Elapsed time for epoch : 0.12760881980260214
Epoch 2, Batch 20, train loss:1.2042337656021118, Elapsed time for epoch : 0.24417238632837932
Epoch 2, Batch 30, train loss:1.1385809183120728, Elapsed time for epoch : 0.3603842655817668
Epoch 2, Batch 40, train loss:1.1568511724472046, Elapsed time for epoch : 0.476856271425883
Epoch 2, Batch 50, train loss:1.2039413452148438, Elapsed time for epoch : 0.5930213093757629
Epoch 2, Batch 60, train loss:1.1398364305496216, Elapsed time for epoch : 0.7092537879943848
Epoch 2, Batch 70, train loss:0.8792173266410828, Elapsed time for epoch : 0.8254831393559774
Epoch 2, Batch 80, train loss:1.1459378004074097, Elapsed time for epoch : 0.9418110688527425
Epoch 2, Batch 90, train loss:0.9982935786247253, Elapsed time for epoch : 1.0578421870867412
Epoch 2, Batch 100, train loss:1.0612632036209106, Elapsed time for epoch : 1.1740160544713338
Epoch 2, Batch 110, train loss:1.0928609371185303, Elapsed time for epoch : 1.2903019626935324
Batch 0, val loss:1.4890601634979248
Batch 10, val loss:2.23394513130188
Batch 20, val loss:2.8225789070129395
Batch 30, val loss:25.796688079833984
Epoch 2, Train Loss:1.1095941953037096, Val loss:7.502203981081645
Epoch 3, Batch 0, train loss:1.0691869258880615, Elapsed time for epoch : 0.011624276638031006
Epoch 3, Batch 10, train loss:1.0199440717697144, Elapsed time for epoch : 0.12767231861750286
Epoch 3, Batch 20, train loss:1.0874806642532349, Elapsed time for epoch : 0.24392691850662232
Epoch 3, Batch 30, train loss:1.0132533311843872, Elapsed time for epoch : 0.3600862463315328
Epoch 3, Batch 40, train loss:0.9909375309944153, Elapsed time for epoch : 0.47665658791859944
Epoch 3, Batch 50, train loss:0.727765679359436, Elapsed time for epoch : 0.5927077531814575
Epoch 3, Batch 60, train loss:0.9802255034446716, Elapsed time for epoch : 0.7088738441467285
Epoch 3, Batch 70, train loss:1.085822343826294, Elapsed time for epoch : 0.8254863063494364
Epoch 3, Batch 80, train loss:1.027030110359192, Elapsed time for epoch : 0.941695769627889
Epoch 3, Batch 90, train loss:1.0379081964492798, Elapsed time for epoch : 1.0576904892921448
Epoch 3, Batch 100, train loss:0.8881585001945496, Elapsed time for epoch : 1.1741277694702148
Epoch 3, Batch 110, train loss:0.8704065680503845, Elapsed time for epoch : 1.2900911808013915
Batch 0, val loss:23.46611213684082
Batch 10, val loss:5.135997772216797
Batch 20, val loss:31.922208786010742
Batch 30, val loss:3.5266640186309814
Epoch 3, Train Loss:0.9951850429825161, Val loss:10.776046769486534
Epoch 4, Batch 0, train loss:1.057687520980835, Elapsed time for epoch : 0.01164009173711141
Epoch 4, Batch 10, train loss:0.8411617875099182, Elapsed time for epoch : 0.12850061257680256
Epoch 4, Batch 20, train loss:0.8223633766174316, Elapsed time for epoch : 0.2450217366218567
Epoch 4, Batch 30, train loss:0.894866943359375, Elapsed time for epoch : 0.3618752360343933
Epoch 4, Batch 40, train loss:1.0448665618896484, Elapsed time for epoch : 0.47831043004989626
Epoch 4, Batch 50, train loss:0.949726939201355, Elapsed time for epoch : 0.5947891036669414
Epoch 4, Batch 60, train loss:0.9361362457275391, Elapsed time for epoch : 0.7112161755561829
Epoch 4, Batch 70, train loss:0.9825283885002136, Elapsed time for epoch : 0.8280787587165832
Epoch 4, Batch 80, train loss:0.902313768863678, Elapsed time for epoch : 0.9444915453592936
Epoch 4, Batch 90, train loss:0.9590429067611694, Elapsed time for epoch : 1.061162531375885
Epoch 4, Batch 100, train loss:0.9202283024787903, Elapsed time for epoch : 1.177422046661377
Epoch 4, Batch 110, train loss:0.9223398566246033, Elapsed time for epoch : 1.2935296932856242
Batch 0, val loss:2.744554042816162
Batch 10, val loss:1.7965399026870728
Batch 20, val loss:2.3988595008850098
Batch 30, val loss:2.356616497039795
Epoch 4, Train Loss:0.9302988933480304, Val loss:6.152291787995233
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.9303
wandb:         Val Loss 6.15229
wandb:      train_batch 110
wandb: train_batch_loss 0.92234
wandb:        val_batch 30
wandb:   val_batch_loss 2.35662
wandb: 
wandb: üöÄ View run happy-spaceship-366 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/r9z1i1av
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_034855-r9z1i1av/logs
Seed completed execution! 23 0.9_1
------------------------------------------------------------------
Running for seed 113 of experiment 0.9_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_035641-xfawhyrz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-waterfall-368
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xfawhyrz
Epoch 0, Batch 0, train loss:8.617968559265137, Elapsed time for epoch : 0.013536600271860759
Epoch 0, Batch 10, train loss:3.0437541007995605, Elapsed time for epoch : 0.1303691864013672
Epoch 0, Batch 20, train loss:2.8108584880828857, Elapsed time for epoch : 0.24745163917541504
Epoch 0, Batch 30, train loss:2.356714963912964, Elapsed time for epoch : 0.36494152148564657
Epoch 0, Batch 40, train loss:2.185683488845825, Elapsed time for epoch : 0.48233746687571205
Epoch 0, Batch 50, train loss:2.0015861988067627, Elapsed time for epoch : 0.5993390917778015
Epoch 0, Batch 60, train loss:1.916375994682312, Elapsed time for epoch : 0.7162040710449219
Epoch 0, Batch 70, train loss:2.008574962615967, Elapsed time for epoch : 0.833071768283844
Epoch 0, Batch 80, train loss:1.7371349334716797, Elapsed time for epoch : 0.9501094341278076
Epoch 0, Batch 90, train loss:1.71402108669281, Elapsed time for epoch : 1.067168152332306
Epoch 0, Batch 100, train loss:1.6612708568572998, Elapsed time for epoch : 1.1841162204742433
Epoch 0, Batch 110, train loss:1.5555684566497803, Elapsed time for epoch : 1.3017325798670452
Batch 0, val loss:3.2562832832336426
Batch 10, val loss:4.332774639129639
Batch 20, val loss:3.29187273979187
Batch 30, val loss:4.624602317810059
Epoch 0, Train Loss:3.2430748006571894, Val loss:4.492783493465847
Epoch 1, Batch 0, train loss:1.6219571828842163, Elapsed time for epoch : 0.01159902811050415
Epoch 1, Batch 10, train loss:1.5798577070236206, Elapsed time for epoch : 0.12742635409037273
Epoch 1, Batch 20, train loss:1.4731136560440063, Elapsed time for epoch : 0.24370085795720417
Epoch 1, Batch 30, train loss:1.5873140096664429, Elapsed time for epoch : 0.35971734126408894
Epoch 1, Batch 40, train loss:1.4943712949752808, Elapsed time for epoch : 0.4759225050608317
Epoch 1, Batch 50, train loss:1.4320224523544312, Elapsed time for epoch : 0.5917456785837809
Epoch 1, Batch 60, train loss:1.2909296751022339, Elapsed time for epoch : 0.7079236268997192
Epoch 1, Batch 70, train loss:1.3400611877441406, Elapsed time for epoch : 0.8239319841066997
Epoch 1, Batch 80, train loss:1.3458272218704224, Elapsed time for epoch : 0.9403084357579549
Epoch 1, Batch 90, train loss:1.2537199258804321, Elapsed time for epoch : 1.056359593073527
Epoch 1, Batch 100, train loss:1.1532319784164429, Elapsed time for epoch : 1.172473700841268
Epoch 1, Batch 110, train loss:1.2100704908370972, Elapsed time for epoch : 1.2886833508809408
Batch 0, val loss:2.3024771213531494
Batch 10, val loss:1.648339867591858
Batch 20, val loss:2.618943691253662
Batch 30, val loss:2.586010217666626
Epoch 1, Train Loss:1.393745145590409, Val loss:8.679304503732258
Epoch 2, Batch 0, train loss:1.246351957321167, Elapsed time for epoch : 0.011572174231211345
Epoch 2, Batch 10, train loss:1.2061312198638916, Elapsed time for epoch : 0.1276641329129537
Epoch 2, Batch 20, train loss:1.2042337656021118, Elapsed time for epoch : 0.2437432050704956
Epoch 2, Batch 30, train loss:1.1385809183120728, Elapsed time for epoch : 0.35979169607162476
Epoch 2, Batch 40, train loss:1.1568511724472046, Elapsed time for epoch : 0.4766274174054464
Epoch 2, Batch 50, train loss:1.2039413452148438, Elapsed time for epoch : 0.5924773772557577
Epoch 2, Batch 60, train loss:1.1398364305496216, Elapsed time for epoch : 0.7088161389033
Epoch 2, Batch 70, train loss:0.8792173266410828, Elapsed time for epoch : 0.8253365715344747
Epoch 2, Batch 80, train loss:1.1459378004074097, Elapsed time for epoch : 0.941515306631724
Epoch 2, Batch 90, train loss:0.9982935786247253, Elapsed time for epoch : 1.0575018803278604
Epoch 2, Batch 100, train loss:1.0612632036209106, Elapsed time for epoch : 1.174061906337738
Epoch 2, Batch 110, train loss:1.0928609371185303, Elapsed time for epoch : 1.2901574810345968
Batch 0, val loss:1.4890601634979248
Batch 10, val loss:2.23394513130188
Batch 20, val loss:2.8225789070129395
Batch 30, val loss:25.796688079833984
Epoch 2, Train Loss:1.1095941953037096, Val loss:7.502203981081645
Epoch 3, Batch 0, train loss:1.0691869258880615, Elapsed time for epoch : 0.011605751514434815
Epoch 3, Batch 10, train loss:1.0199440717697144, Elapsed time for epoch : 0.1279836138089498
Epoch 3, Batch 20, train loss:1.0874806642532349, Elapsed time for epoch : 0.24422696034113567
Epoch 3, Batch 30, train loss:1.0132533311843872, Elapsed time for epoch : 0.36047221422195436
Epoch 3, Batch 40, train loss:0.9909375309944153, Elapsed time for epoch : 0.47665433088938397
Epoch 3, Batch 50, train loss:0.727765679359436, Elapsed time for epoch : 0.5924336632092794
Epoch 3, Batch 60, train loss:0.9802255034446716, Elapsed time for epoch : 0.7087117433547974
Epoch 3, Batch 70, train loss:1.085822343826294, Elapsed time for epoch : 0.825188942750295
Epoch 3, Batch 80, train loss:1.027030110359192, Elapsed time for epoch : 0.9416684508323669
Epoch 3, Batch 90, train loss:1.0379081964492798, Elapsed time for epoch : 1.0579169789950054
Epoch 3, Batch 100, train loss:0.8881585001945496, Elapsed time for epoch : 1.1742741545041402
Epoch 3, Batch 110, train loss:0.8704065680503845, Elapsed time for epoch : 1.2902271191279093
Batch 0, val loss:23.46611213684082
Batch 10, val loss:5.135997772216797
Batch 20, val loss:31.922208786010742
Batch 30, val loss:3.5266640186309814
Epoch 3, Train Loss:0.9951850429825161, Val loss:10.776046769486534
Epoch 4, Batch 0, train loss:1.057687520980835, Elapsed time for epoch : 0.011702779928843181
Epoch 4, Batch 10, train loss:0.8411617875099182, Elapsed time for epoch : 0.12783113320668538
Epoch 4, Batch 20, train loss:0.8223633766174316, Elapsed time for epoch : 0.24458331267038982
Epoch 4, Batch 30, train loss:0.894866943359375, Elapsed time for epoch : 0.3605557401974996
Epoch 4, Batch 40, train loss:1.0448665618896484, Elapsed time for epoch : 0.47707332372665406
Epoch 4, Batch 50, train loss:0.949726939201355, Elapsed time for epoch : 0.5933526515960693
Epoch 4, Batch 60, train loss:0.9361362457275391, Elapsed time for epoch : 0.7096706787745158
Epoch 4, Batch 70, train loss:0.9825283885002136, Elapsed time for epoch : 0.8259402553240458
Epoch 4, Batch 80, train loss:0.902313768863678, Elapsed time for epoch : 0.9423193653424581
Epoch 4, Batch 90, train loss:0.9590429067611694, Elapsed time for epoch : 1.0586331009864807
Epoch 4, Batch 100, train loss:0.9202283024787903, Elapsed time for epoch : 1.1749868830045065
Epoch 4, Batch 110, train loss:0.9223398566246033, Elapsed time for epoch : 1.2918599327405293
Batch 0, val loss:2.744554042816162
Batch 10, val loss:1.7965399026870728
Batch 20, val loss:2.3988595008850098
Batch 30, val loss:2.356616497039795
Epoch 4, Train Loss:0.9302988933480304, Val loss:6.152291787995233
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.171 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.9303
wandb:         Val Loss 6.15229
wandb:      train_batch 110
wandb: train_batch_loss 0.92234
wandb:        val_batch 30
wandb:   val_batch_loss 2.35662
wandb: 
wandb: üöÄ View run drawn-waterfall-368 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xfawhyrz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_035641-xfawhyrz/logs
Seed completed execution! 113 0.9_1
------------------------------------------------------------------
Experiment complete 0.9_1
==========================================================================
Running experiment for setting 0.9_2
==========================================================================
Running for seed 1 of experiment 0.9_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_040427-bsrcd6o1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-glitter-370
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bsrcd6o1
Epoch 0, Batch 0, train loss:8.377289772033691, Elapsed time for epoch : 0.013528462251027424
Epoch 0, Batch 10, train loss:3.962484359741211, Elapsed time for epoch : 0.1360052188237508
Epoch 0, Batch 20, train loss:2.6361939907073975, Elapsed time for epoch : 0.25963940223058063
Epoch 0, Batch 30, train loss:2.4307284355163574, Elapsed time for epoch : 0.38530251185099285
Epoch 0, Batch 40, train loss:2.249875068664551, Elapsed time for epoch : 0.5096357782681783
Epoch 0, Batch 50, train loss:2.109753370285034, Elapsed time for epoch : 0.63369699716568
Epoch 0, Batch 60, train loss:2.0186474323272705, Elapsed time for epoch : 0.7568672219912211
Epoch 0, Batch 70, train loss:2.032378673553467, Elapsed time for epoch : 0.8799540877342225
Epoch 0, Batch 80, train loss:1.8564453125, Elapsed time for epoch : 1.005161960919698
Epoch 0, Batch 90, train loss:1.6993939876556396, Elapsed time for epoch : 1.1295880317687987
Epoch 0, Batch 100, train loss:1.6150197982788086, Elapsed time for epoch : 1.2547049840291342
Epoch 0, Batch 110, train loss:1.4343911409378052, Elapsed time for epoch : 1.3782005230585734
Batch 0, val loss:5.3561224937438965
Batch 10, val loss:4.999115467071533
Batch 20, val loss:4.017043113708496
Batch 30, val loss:10.606844902038574
Epoch 0, Train Loss:2.9322607942249466, Val loss:4.938405063417223
Epoch 1, Batch 0, train loss:1.5842889547348022, Elapsed time for epoch : 0.011615351835886637
Epoch 1, Batch 10, train loss:1.4762827157974243, Elapsed time for epoch : 0.12777402003606161
Epoch 1, Batch 20, train loss:1.3357079029083252, Elapsed time for epoch : 0.24394516547520956
Epoch 1, Batch 30, train loss:1.350999116897583, Elapsed time for epoch : 0.3602287967999776
Epoch 1, Batch 40, train loss:1.2711725234985352, Elapsed time for epoch : 0.47656458218892417
Epoch 1, Batch 50, train loss:1.4199365377426147, Elapsed time for epoch : 0.592647401491801
Epoch 1, Batch 60, train loss:1.2077184915542603, Elapsed time for epoch : 0.7085941672325134
Epoch 1, Batch 70, train loss:1.195571780204773, Elapsed time for epoch : 0.8250767906506856
Epoch 1, Batch 80, train loss:1.2362747192382812, Elapsed time for epoch : 0.9417820612589518
Epoch 1, Batch 90, train loss:1.138885498046875, Elapsed time for epoch : 1.0578467488288879
Epoch 1, Batch 100, train loss:1.0107471942901611, Elapsed time for epoch : 1.174304684003194
Epoch 1, Batch 110, train loss:1.0664209127426147, Elapsed time for epoch : 1.290582044919332
Batch 0, val loss:2.132316827774048
Batch 10, val loss:5.68829345703125
Batch 20, val loss:2.4403531551361084
Batch 30, val loss:2.3382129669189453
Epoch 1, Train Loss:1.2546125857726387, Val loss:7.281129128403133
Epoch 2, Batch 0, train loss:1.0181320905685425, Elapsed time for epoch : 0.011642916997273763
Epoch 2, Batch 10, train loss:1.0428186655044556, Elapsed time for epoch : 0.12811750570933025
Epoch 2, Batch 20, train loss:1.0189348459243774, Elapsed time for epoch : 0.24443370898564656
Epoch 2, Batch 30, train loss:1.014293909072876, Elapsed time for epoch : 0.36073340972264606
Epoch 2, Batch 40, train loss:1.042671799659729, Elapsed time for epoch : 0.4772769808769226
Epoch 2, Batch 50, train loss:1.07573664188385, Elapsed time for epoch : 0.5937253912289937
Epoch 2, Batch 60, train loss:0.9814371466636658, Elapsed time for epoch : 0.7101495703061421
Epoch 2, Batch 70, train loss:0.8408387303352356, Elapsed time for epoch : 0.8269762714703878
Epoch 2, Batch 80, train loss:1.0270781517028809, Elapsed time for epoch : 0.9432788054148357
Epoch 2, Batch 90, train loss:0.9347370862960815, Elapsed time for epoch : 1.059852902094523
Epoch 2, Batch 100, train loss:1.0027772188186646, Elapsed time for epoch : 1.1768921812375386
Epoch 2, Batch 110, train loss:0.9569674134254456, Elapsed time for epoch : 1.2935729384422303
Batch 0, val loss:4.811682224273682
Batch 10, val loss:1.8210285902023315
Batch 20, val loss:1.8893451690673828
Batch 30, val loss:10.577445983886719
Epoch 2, Train Loss:0.9697124854378079, Val loss:6.640671604209476
Epoch 3, Batch 0, train loss:0.8640637397766113, Elapsed time for epoch : 0.011714752515157063
Epoch 3, Batch 10, train loss:0.912526547908783, Elapsed time for epoch : 0.12794437805811565
Epoch 3, Batch 20, train loss:0.8838380575180054, Elapsed time for epoch : 0.24424108266830444
Epoch 3, Batch 30, train loss:0.9399908781051636, Elapsed time for epoch : 0.3608468095461527
Epoch 3, Batch 40, train loss:0.9303320050239563, Elapsed time for epoch : 0.47708870967229206
Epoch 3, Batch 50, train loss:0.6557415127754211, Elapsed time for epoch : 0.5932145277659099
Epoch 3, Batch 60, train loss:0.7994666695594788, Elapsed time for epoch : 0.7094954093297322
Epoch 3, Batch 70, train loss:0.9219720959663391, Elapsed time for epoch : 0.8266684492429097
Epoch 3, Batch 80, train loss:0.828762412071228, Elapsed time for epoch : 0.942899747689565
Epoch 3, Batch 90, train loss:0.7741662263870239, Elapsed time for epoch : 1.0594672600428263
Epoch 3, Batch 100, train loss:0.5153716206550598, Elapsed time for epoch : 1.175764767328898
Epoch 3, Batch 110, train loss:0.7662330269813538, Elapsed time for epoch : 1.292336678504944
Batch 0, val loss:8.705588340759277
Batch 10, val loss:2.8443477153778076
Batch 20, val loss:11.62418270111084
Batch 30, val loss:4.849358558654785
Epoch 3, Train Loss:0.8227806179419808, Val loss:8.620897804697355
Epoch 4, Batch 0, train loss:0.7212005853652954, Elapsed time for epoch : 0.011620879173278809
Epoch 4, Batch 10, train loss:0.576321005821228, Elapsed time for epoch : 0.1277840813000997
Epoch 4, Batch 20, train loss:0.4909040927886963, Elapsed time for epoch : 0.24434178670247395
Epoch 4, Batch 30, train loss:0.6309860348701477, Elapsed time for epoch : 0.36114409764607747
Epoch 4, Batch 40, train loss:0.6711864471435547, Elapsed time for epoch : 0.4773585875829061
Epoch 4, Batch 50, train loss:0.8385125994682312, Elapsed time for epoch : 0.5936950445175171
Epoch 4, Batch 60, train loss:0.6458786129951477, Elapsed time for epoch : 0.7099395394325256
Epoch 4, Batch 70, train loss:0.7409620881080627, Elapsed time for epoch : 0.8263830701510112
Epoch 4, Batch 80, train loss:0.5205360651016235, Elapsed time for epoch : 0.9428344607353211
Epoch 4, Batch 90, train loss:0.6814155578613281, Elapsed time for epoch : 1.058924432595571
Epoch 4, Batch 100, train loss:0.6477091908454895, Elapsed time for epoch : 1.175069018205007
Epoch 4, Batch 110, train loss:0.6803750991821289, Elapsed time for epoch : 1.2912231047948202
Batch 0, val loss:2.6878490447998047
Batch 10, val loss:2.631075143814087
Batch 20, val loss:4.974578380584717
Batch 30, val loss:3.9325108528137207
Epoch 4, Train Loss:0.6455215775448343, Val loss:6.029855425159137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.64552
wandb:         Val Loss 6.02986
wandb:      train_batch 110
wandb: train_batch_loss 0.68038
wandb:        val_batch 30
wandb:   val_batch_loss 3.93251
wandb: 
wandb: üöÄ View run rare-glitter-370 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bsrcd6o1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_040427-bsrcd6o1/logs
Seed completed execution! 1 0.9_2
------------------------------------------------------------------
Running for seed 42 of experiment 0.9_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_041221-fut6d5wh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-river-372
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/fut6d5wh
Epoch 0, Batch 0, train loss:8.377289772033691, Elapsed time for epoch : 0.01334817409515381
Epoch 0, Batch 10, train loss:3.962484359741211, Elapsed time for epoch : 0.12978710730870566
Epoch 0, Batch 20, train loss:2.6361939907073975, Elapsed time for epoch : 0.2456317663192749
Epoch 0, Batch 30, train loss:2.4307284355163574, Elapsed time for epoch : 0.3616165518760681
Epoch 0, Batch 40, train loss:2.249875068664551, Elapsed time for epoch : 0.47763516505559284
Epoch 0, Batch 50, train loss:2.109753370285034, Elapsed time for epoch : 0.5936674237251282
Epoch 0, Batch 60, train loss:2.0186474323272705, Elapsed time for epoch : 0.7093916614850362
Epoch 0, Batch 70, train loss:2.032378673553467, Elapsed time for epoch : 0.8252632697423299
Epoch 0, Batch 80, train loss:1.8564453125, Elapsed time for epoch : 0.9418464660644531
Epoch 0, Batch 90, train loss:1.6993939876556396, Elapsed time for epoch : 1.057633670171102
Epoch 0, Batch 100, train loss:1.6150197982788086, Elapsed time for epoch : 1.1737516840298972
Epoch 0, Batch 110, train loss:1.4343911409378052, Elapsed time for epoch : 1.2904268701871235
Batch 0, val loss:5.3561224937438965
Batch 10, val loss:4.999115467071533
Batch 20, val loss:4.017043113708496
Batch 30, val loss:10.606844902038574
Epoch 0, Train Loss:2.9322607942249466, Val loss:4.938405063417223
Epoch 1, Batch 0, train loss:1.5842889547348022, Elapsed time for epoch : 0.011623076597849528
Epoch 1, Batch 10, train loss:1.4762827157974243, Elapsed time for epoch : 0.128219469388326
Epoch 1, Batch 20, train loss:1.3357079029083252, Elapsed time for epoch : 0.2442926009496053
Epoch 1, Batch 30, train loss:1.350999116897583, Elapsed time for epoch : 0.3605855425198873
Epoch 1, Batch 40, train loss:1.2711725234985352, Elapsed time for epoch : 0.4767760713895162
Epoch 1, Batch 50, train loss:1.4199365377426147, Elapsed time for epoch : 0.5927995602289836
Epoch 1, Batch 60, train loss:1.2077184915542603, Elapsed time for epoch : 0.7092884023984273
Epoch 1, Batch 70, train loss:1.195571780204773, Elapsed time for epoch : 0.8255745609601338
Epoch 1, Batch 80, train loss:1.2362747192382812, Elapsed time for epoch : 0.9419130762418111
Epoch 1, Batch 90, train loss:1.138885498046875, Elapsed time for epoch : 1.0579755942026774
Epoch 1, Batch 100, train loss:1.0107471942901611, Elapsed time for epoch : 1.1745152513186137
Epoch 1, Batch 110, train loss:1.0664209127426147, Elapsed time for epoch : 1.2905221223831176
Batch 0, val loss:2.132316827774048
Batch 10, val loss:5.68829345703125
Batch 20, val loss:2.4403531551361084
Batch 30, val loss:2.3382129669189453
Epoch 1, Train Loss:1.2546125857726387, Val loss:7.281129128403133
Epoch 2, Batch 0, train loss:1.0181320905685425, Elapsed time for epoch : 0.011758689085642498
Epoch 2, Batch 10, train loss:1.0428186655044556, Elapsed time for epoch : 0.1279926856358846
Epoch 2, Batch 20, train loss:1.0189348459243774, Elapsed time for epoch : 0.24434706767400105
Epoch 2, Batch 30, train loss:1.014293909072876, Elapsed time for epoch : 0.36058499415715534
Epoch 2, Batch 40, train loss:1.042671799659729, Elapsed time for epoch : 0.4765933076540629
Epoch 2, Batch 50, train loss:1.07573664188385, Elapsed time for epoch : 0.5930282473564148
Epoch 2, Batch 60, train loss:0.9814371466636658, Elapsed time for epoch : 0.7094810167948405
Epoch 2, Batch 70, train loss:0.8408387303352356, Elapsed time for epoch : 0.8260879874229431
Epoch 2, Batch 80, train loss:1.0270781517028809, Elapsed time for epoch : 0.9430264671643575
Epoch 2, Batch 90, train loss:0.9347370862960815, Elapsed time for epoch : 1.0593941410382588
Epoch 2, Batch 100, train loss:1.0027772188186646, Elapsed time for epoch : 1.175696563720703
Epoch 2, Batch 110, train loss:0.9569674134254456, Elapsed time for epoch : 1.2926573952039082
Batch 0, val loss:4.811682224273682
Batch 10, val loss:1.8210285902023315
Batch 20, val loss:1.8893451690673828
Batch 30, val loss:10.577445983886719
Epoch 2, Train Loss:0.9697124854378079, Val loss:6.640671604209476
Epoch 3, Batch 0, train loss:0.8640637397766113, Elapsed time for epoch : 0.011885090668996175
Epoch 3, Batch 10, train loss:0.912526547908783, Elapsed time for epoch : 0.1279656171798706
Epoch 3, Batch 20, train loss:0.8838380575180054, Elapsed time for epoch : 0.2443897048632304
Epoch 3, Batch 30, train loss:0.9399908781051636, Elapsed time for epoch : 0.36062104304631554
Epoch 3, Batch 40, train loss:0.9303320050239563, Elapsed time for epoch : 0.47714451551437376
Epoch 3, Batch 50, train loss:0.6557415127754211, Elapsed time for epoch : 0.5931280811627706
Epoch 3, Batch 60, train loss:0.7994666695594788, Elapsed time for epoch : 0.7091303904851277
Epoch 3, Batch 70, train loss:0.9219720959663391, Elapsed time for epoch : 0.8253023028373718
Epoch 3, Batch 80, train loss:0.828762412071228, Elapsed time for epoch : 0.9415266633033752
Epoch 3, Batch 90, train loss:0.7741662263870239, Elapsed time for epoch : 1.0578582485516865
Epoch 3, Batch 100, train loss:0.5153716206550598, Elapsed time for epoch : 1.1743159770965577
Epoch 3, Batch 110, train loss:0.7662330269813538, Elapsed time for epoch : 1.2906717896461486
Batch 0, val loss:8.705588340759277
Batch 10, val loss:2.8443477153778076
Batch 20, val loss:11.62418270111084
Batch 30, val loss:4.849358558654785
Epoch 3, Train Loss:0.8227806179419808, Val loss:8.620897804697355
Epoch 4, Batch 0, train loss:0.7212005853652954, Elapsed time for epoch : 0.011605759461720785
Epoch 4, Batch 10, train loss:0.576321005821228, Elapsed time for epoch : 0.12791775862375895
Epoch 4, Batch 20, train loss:0.4909040927886963, Elapsed time for epoch : 0.24432994922002158
Epoch 4, Batch 30, train loss:0.6309860348701477, Elapsed time for epoch : 0.3605185031890869
Epoch 4, Batch 40, train loss:0.6711864471435547, Elapsed time for epoch : 0.47640836238861084
Epoch 4, Batch 50, train loss:0.8385125994682312, Elapsed time for epoch : 0.5925362745920817
Epoch 4, Batch 60, train loss:0.6458786129951477, Elapsed time for epoch : 0.7086241881052653
Epoch 4, Batch 70, train loss:0.7409620881080627, Elapsed time for epoch : 0.8249681711196899
Epoch 4, Batch 80, train loss:0.5205360651016235, Elapsed time for epoch : 0.9409837404886882
Epoch 4, Batch 90, train loss:0.6814155578613281, Elapsed time for epoch : 1.0567750493685404
Epoch 4, Batch 100, train loss:0.6477091908454895, Elapsed time for epoch : 1.1729776859283447
Epoch 4, Batch 110, train loss:0.6803750991821289, Elapsed time for epoch : 1.2891746441523233
Batch 0, val loss:2.6878490447998047
Batch 10, val loss:2.631075143814087
Batch 20, val loss:4.974578380584717
Batch 30, val loss:3.9325108528137207
Epoch 4, Train Loss:0.6455215775448343, Val loss:6.029855425159137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.64552
wandb:         Val Loss 6.02986
wandb:      train_batch 110
wandb: train_batch_loss 0.68038
wandb:        val_batch 30
wandb:   val_batch_loss 3.93251
wandb: 
wandb: üöÄ View run dry-river-372 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/fut6d5wh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_041221-fut6d5wh/logs
Seed completed execution! 42 0.9_2
------------------------------------------------------------------
Running for seed 89 of experiment 0.9_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_042007-kwctn3a7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-monkey-374
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/kwctn3a7
Epoch 0, Batch 0, train loss:8.377289772033691, Elapsed time for epoch : 0.012346494197845458
Epoch 0, Batch 10, train loss:3.962484359741211, Elapsed time for epoch : 0.12869724829991658
Epoch 0, Batch 20, train loss:2.6361939907073975, Elapsed time for epoch : 0.24469658533732097
Epoch 0, Batch 30, train loss:2.4307284355163574, Elapsed time for epoch : 0.36074687242507936
Epoch 0, Batch 40, train loss:2.249875068664551, Elapsed time for epoch : 0.47665678660074867
Epoch 0, Batch 50, train loss:2.109753370285034, Elapsed time for epoch : 0.592831019560496
Epoch 0, Batch 60, train loss:2.0186474323272705, Elapsed time for epoch : 0.7086122671763102
Epoch 0, Batch 70, train loss:2.032378673553467, Elapsed time for epoch : 0.8248291412989298
Epoch 0, Batch 80, train loss:1.8564453125, Elapsed time for epoch : 0.9414730032285055
Epoch 0, Batch 90, train loss:1.6993939876556396, Elapsed time for epoch : 1.0580664157867432
Epoch 0, Batch 100, train loss:1.6150197982788086, Elapsed time for epoch : 1.1750040849049885
Epoch 0, Batch 110, train loss:1.4343911409378052, Elapsed time for epoch : 1.2919766902923584
Batch 0, val loss:5.3561224937438965
Batch 10, val loss:4.999115467071533
Batch 20, val loss:4.017043113708496
Batch 30, val loss:10.606844902038574
Epoch 0, Train Loss:2.9322607942249466, Val loss:4.938405063417223
Epoch 1, Batch 0, train loss:1.5842889547348022, Elapsed time for epoch : 0.011733023325602214
Epoch 1, Batch 10, train loss:1.4762827157974243, Elapsed time for epoch : 0.12862897316614788
Epoch 1, Batch 20, train loss:1.3357079029083252, Elapsed time for epoch : 0.2453888456026713
Epoch 1, Batch 30, train loss:1.350999116897583, Elapsed time for epoch : 0.3619083841641744
Epoch 1, Batch 40, train loss:1.2711725234985352, Elapsed time for epoch : 0.47823440631230674
Epoch 1, Batch 50, train loss:1.4199365377426147, Elapsed time for epoch : 0.5944986899693807
Epoch 1, Batch 60, train loss:1.2077184915542603, Elapsed time for epoch : 0.7110473871231079
Epoch 1, Batch 70, train loss:1.195571780204773, Elapsed time for epoch : 0.8275249520937602
Epoch 1, Batch 80, train loss:1.2362747192382812, Elapsed time for epoch : 0.944531528155009
Epoch 1, Batch 90, train loss:1.138885498046875, Elapsed time for epoch : 1.0609617988268534
Epoch 1, Batch 100, train loss:1.0107471942901611, Elapsed time for epoch : 1.1773460268974305
Epoch 1, Batch 110, train loss:1.0664209127426147, Elapsed time for epoch : 1.2936625083287556
Batch 0, val loss:2.132316827774048
Batch 10, val loss:5.68829345703125
Batch 20, val loss:2.4403531551361084
Batch 30, val loss:2.3382129669189453
Epoch 1, Train Loss:1.2546125857726387, Val loss:7.281129128403133
Epoch 2, Batch 0, train loss:1.0181320905685425, Elapsed time for epoch : 0.011652151743570963
Epoch 2, Batch 10, train loss:1.0428186655044556, Elapsed time for epoch : 0.12804325819015502
Epoch 2, Batch 20, train loss:1.0189348459243774, Elapsed time for epoch : 0.24441372553507487
Epoch 2, Batch 30, train loss:1.014293909072876, Elapsed time for epoch : 0.36057075659434
Epoch 2, Batch 40, train loss:1.042671799659729, Elapsed time for epoch : 0.4774294137954712
Epoch 2, Batch 50, train loss:1.07573664188385, Elapsed time for epoch : 0.5938646952311198
Epoch 2, Batch 60, train loss:0.9814371466636658, Elapsed time for epoch : 0.710208801428477
Epoch 2, Batch 70, train loss:0.8408387303352356, Elapsed time for epoch : 0.8264949719111124
Epoch 2, Batch 80, train loss:1.0270781517028809, Elapsed time for epoch : 0.9428825894991557
Epoch 2, Batch 90, train loss:0.9347370862960815, Elapsed time for epoch : 1.0591489791870117
Epoch 2, Batch 100, train loss:1.0027772188186646, Elapsed time for epoch : 1.1754831075668335
Epoch 2, Batch 110, train loss:0.9569674134254456, Elapsed time for epoch : 1.291801909605662
Batch 0, val loss:4.811682224273682
Batch 10, val loss:1.8210285902023315
Batch 20, val loss:1.8893451690673828
Batch 30, val loss:10.577445983886719
Epoch 2, Train Loss:0.9697124854378079, Val loss:6.640671604209476
Epoch 3, Batch 0, train loss:0.8640637397766113, Elapsed time for epoch : 0.011655787626902262
Epoch 3, Batch 10, train loss:0.912526547908783, Elapsed time for epoch : 0.1280627131462097
Epoch 3, Batch 20, train loss:0.8838380575180054, Elapsed time for epoch : 0.24418865442276
Epoch 3, Batch 30, train loss:0.9399908781051636, Elapsed time for epoch : 0.36082642475763954
Epoch 3, Batch 40, train loss:0.9303320050239563, Elapsed time for epoch : 0.4774037837982178
Epoch 3, Batch 50, train loss:0.6557415127754211, Elapsed time for epoch : 0.5937319238980611
Epoch 3, Batch 60, train loss:0.7994666695594788, Elapsed time for epoch : 0.7102313280105591
Epoch 3, Batch 70, train loss:0.9219720959663391, Elapsed time for epoch : 0.827058752377828
Epoch 3, Batch 80, train loss:0.828762412071228, Elapsed time for epoch : 0.9432688236236573
Epoch 3, Batch 90, train loss:0.7741662263870239, Elapsed time for epoch : 1.0595809896787007
Epoch 3, Batch 100, train loss:0.5153716206550598, Elapsed time for epoch : 1.1759347041447958
Epoch 3, Batch 110, train loss:0.7662330269813538, Elapsed time for epoch : 1.2922734181086222
Batch 0, val loss:8.705588340759277
Batch 10, val loss:2.8443477153778076
Batch 20, val loss:11.62418270111084
Batch 30, val loss:4.849358558654785
Epoch 3, Train Loss:0.8227806179419808, Val loss:8.620897804697355
Epoch 4, Batch 0, train loss:0.7212005853652954, Elapsed time for epoch : 0.011816986401875814
Epoch 4, Batch 10, train loss:0.576321005821228, Elapsed time for epoch : 0.12806103626887003
Epoch 4, Batch 20, train loss:0.4909040927886963, Elapsed time for epoch : 0.24438843727111817
Epoch 4, Batch 30, train loss:0.6309860348701477, Elapsed time for epoch : 0.3608802835146586
Epoch 4, Batch 40, train loss:0.6711864471435547, Elapsed time for epoch : 0.47731075684229535
Epoch 4, Batch 50, train loss:0.8385125994682312, Elapsed time for epoch : 0.593828558921814
Epoch 4, Batch 60, train loss:0.6458786129951477, Elapsed time for epoch : 0.7103923002878825
Epoch 4, Batch 70, train loss:0.7409620881080627, Elapsed time for epoch : 0.8267838557561239
Epoch 4, Batch 80, train loss:0.5205360651016235, Elapsed time for epoch : 0.9437199672063191
Epoch 4, Batch 90, train loss:0.6814155578613281, Elapsed time for epoch : 1.0602714657783507
Epoch 4, Batch 100, train loss:0.6477091908454895, Elapsed time for epoch : 1.1766156991322836
Epoch 4, Batch 110, train loss:0.6803750991821289, Elapsed time for epoch : 1.2932227770487468
Batch 0, val loss:2.6878490447998047
Batch 10, val loss:2.631075143814087
Batch 20, val loss:4.974578380584717
Batch 30, val loss:3.9325108528137207
Epoch 4, Train Loss:0.6455215775448343, Val loss:6.029855425159137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.64552
wandb:         Val Loss 6.02986
wandb:      train_batch 110
wandb: train_batch_loss 0.68038
wandb:        val_batch 30
wandb:   val_batch_loss 3.93251
wandb: 
wandb: üöÄ View run swept-monkey-374 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/kwctn3a7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_042007-kwctn3a7/logs
Seed completed execution! 89 0.9_2
------------------------------------------------------------------
Running for seed 23 of experiment 0.9_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_042756-hpb9r9z4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-sponge-376
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hpb9r9z4
Epoch 0, Batch 0, train loss:8.377289772033691, Elapsed time for epoch : 0.012096742788950602
Epoch 0, Batch 10, train loss:3.962484359741211, Elapsed time for epoch : 0.12805939515431722
Epoch 0, Batch 20, train loss:2.6361939907073975, Elapsed time for epoch : 0.2442253033320109
Epoch 0, Batch 30, train loss:2.4307284355163574, Elapsed time for epoch : 0.360257093111674
Epoch 0, Batch 40, train loss:2.249875068664551, Elapsed time for epoch : 0.4768621603647868
Epoch 0, Batch 50, train loss:2.109753370285034, Elapsed time for epoch : 0.5928447167078654
Epoch 0, Batch 60, train loss:2.0186474323272705, Elapsed time for epoch : 0.709043550491333
Epoch 0, Batch 70, train loss:2.032378673553467, Elapsed time for epoch : 0.8250537037849426
Epoch 0, Batch 80, train loss:1.8564453125, Elapsed time for epoch : 0.941332979996999
Epoch 0, Batch 90, train loss:1.6993939876556396, Elapsed time for epoch : 1.057574780782064
Epoch 0, Batch 100, train loss:1.6150197982788086, Elapsed time for epoch : 1.1740556955337524
Epoch 0, Batch 110, train loss:1.4343911409378052, Elapsed time for epoch : 1.2903267343839009
Batch 0, val loss:5.3561224937438965
Batch 10, val loss:4.999115467071533
Batch 20, val loss:4.017043113708496
Batch 30, val loss:10.606844902038574
Epoch 0, Train Loss:2.9322607942249466, Val loss:4.938405063417223
Epoch 1, Batch 0, train loss:1.5842889547348022, Elapsed time for epoch : 0.011608354250590007
Epoch 1, Batch 10, train loss:1.4762827157974243, Elapsed time for epoch : 0.1279249866803487
Epoch 1, Batch 20, train loss:1.3357079029083252, Elapsed time for epoch : 0.24457775751749675
Epoch 1, Batch 30, train loss:1.350999116897583, Elapsed time for epoch : 0.36070838769276936
Epoch 1, Batch 40, train loss:1.2711725234985352, Elapsed time for epoch : 0.4771069010098775
Epoch 1, Batch 50, train loss:1.4199365377426147, Elapsed time for epoch : 0.5933058063189188
Epoch 1, Batch 60, train loss:1.2077184915542603, Elapsed time for epoch : 0.7097049554189047
Epoch 1, Batch 70, train loss:1.195571780204773, Elapsed time for epoch : 0.8260762254397075
Epoch 1, Batch 80, train loss:1.2362747192382812, Elapsed time for epoch : 0.9423922181129456
Epoch 1, Batch 90, train loss:1.138885498046875, Elapsed time for epoch : 1.0587580720583598
Epoch 1, Batch 100, train loss:1.0107471942901611, Elapsed time for epoch : 1.175292686621348
Epoch 1, Batch 110, train loss:1.0664209127426147, Elapsed time for epoch : 1.2916525920232138
Batch 0, val loss:2.132316827774048
Batch 10, val loss:5.68829345703125
Batch 20, val loss:2.4403531551361084
Batch 30, val loss:2.3382129669189453
Epoch 1, Train Loss:1.2546125857726387, Val loss:7.281129128403133
Epoch 2, Batch 0, train loss:1.0181320905685425, Elapsed time for epoch : 0.011761542161305745
Epoch 2, Batch 10, train loss:1.0428186655044556, Elapsed time for epoch : 0.1281802733739217
Epoch 2, Batch 20, train loss:1.0189348459243774, Elapsed time for epoch : 0.2445429007212321
Epoch 2, Batch 30, train loss:1.014293909072876, Elapsed time for epoch : 0.3607891718546549
Epoch 2, Batch 40, train loss:1.042671799659729, Elapsed time for epoch : 0.4771948258082072
Epoch 2, Batch 50, train loss:1.07573664188385, Elapsed time for epoch : 0.5935280839602153
Epoch 2, Batch 60, train loss:0.9814371466636658, Elapsed time for epoch : 0.709825849533081
Epoch 2, Batch 70, train loss:0.8408387303352356, Elapsed time for epoch : 0.8262041211128235
Epoch 2, Batch 80, train loss:1.0270781517028809, Elapsed time for epoch : 0.9431861758232116
Epoch 2, Batch 90, train loss:0.9347370862960815, Elapsed time for epoch : 1.0600612004597982
Epoch 2, Batch 100, train loss:1.0027772188186646, Elapsed time for epoch : 1.1766292969385783
Epoch 2, Batch 110, train loss:0.9569674134254456, Elapsed time for epoch : 1.2930163582166037
Batch 0, val loss:4.811682224273682
Batch 10, val loss:1.8210285902023315
Batch 20, val loss:1.8893451690673828
Batch 30, val loss:10.577445983886719
Epoch 2, Train Loss:0.9697124854378079, Val loss:6.640671604209476
Epoch 3, Batch 0, train loss:0.8640637397766113, Elapsed time for epoch : 0.011664776007334392
Epoch 3, Batch 10, train loss:0.912526547908783, Elapsed time for epoch : 0.12861276467641194
Epoch 3, Batch 20, train loss:0.8838380575180054, Elapsed time for epoch : 0.2450395623842875
Epoch 3, Batch 30, train loss:0.9399908781051636, Elapsed time for epoch : 0.3614324688911438
Epoch 3, Batch 40, train loss:0.9303320050239563, Elapsed time for epoch : 0.4781785170237223
Epoch 3, Batch 50, train loss:0.6557415127754211, Elapsed time for epoch : 0.5946948806444804
Epoch 3, Batch 60, train loss:0.7994666695594788, Elapsed time for epoch : 0.7111750443776449
Epoch 3, Batch 70, train loss:0.9219720959663391, Elapsed time for epoch : 0.8277655005455017
Epoch 3, Batch 80, train loss:0.828762412071228, Elapsed time for epoch : 0.9442232648531595
Epoch 3, Batch 90, train loss:0.7741662263870239, Elapsed time for epoch : 1.0606937209765117
Epoch 3, Batch 100, train loss:0.5153716206550598, Elapsed time for epoch : 1.1770259459813437
Epoch 3, Batch 110, train loss:0.7662330269813538, Elapsed time for epoch : 1.2938848813374837
Batch 0, val loss:8.705588340759277
Batch 10, val loss:2.8443477153778076
Batch 20, val loss:11.62418270111084
Batch 30, val loss:4.849358558654785
Epoch 3, Train Loss:0.8227806179419808, Val loss:8.620897804697355
Epoch 4, Batch 0, train loss:0.7212005853652954, Elapsed time for epoch : 0.011708394686381022
Epoch 4, Batch 10, train loss:0.576321005821228, Elapsed time for epoch : 0.12976131836573282
Epoch 4, Batch 20, train loss:0.4909040927886963, Elapsed time for epoch : 0.24738336404164632
Epoch 4, Batch 30, train loss:0.6309860348701477, Elapsed time for epoch : 0.3641225457191467
Epoch 4, Batch 40, train loss:0.6711864471435547, Elapsed time for epoch : 0.4822953939437866
Epoch 4, Batch 50, train loss:0.8385125994682312, Elapsed time for epoch : 0.6008963664372762
Epoch 4, Batch 60, train loss:0.6458786129951477, Elapsed time for epoch : 0.717825452486674
Epoch 4, Batch 70, train loss:0.7409620881080627, Elapsed time for epoch : 0.8361946622530619
Epoch 4, Batch 80, train loss:0.5205360651016235, Elapsed time for epoch : 0.9538253347078959
Epoch 4, Batch 90, train loss:0.6814155578613281, Elapsed time for epoch : 1.0708942453066508
Epoch 4, Batch 100, train loss:0.6477091908454895, Elapsed time for epoch : 1.1888319333394368
Epoch 4, Batch 110, train loss:0.6803750991821289, Elapsed time for epoch : 1.306188460191091
Batch 0, val loss:2.6878490447998047
Batch 10, val loss:2.631075143814087
Batch 20, val loss:4.974578380584717
Batch 30, val loss:3.9325108528137207
Epoch 4, Train Loss:0.6455215775448343, Val loss:6.029855425159137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.64552
wandb:         Val Loss 6.02986
wandb:      train_batch 110
wandb: train_batch_loss 0.68038
wandb:        val_batch 30
wandb:   val_batch_loss 3.93251
wandb: 
wandb: üöÄ View run glorious-sponge-376 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hpb9r9z4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_042756-hpb9r9z4/logs
Seed completed execution! 23 0.9_2
------------------------------------------------------------------
Running for seed 113 of experiment 0.9_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_043544-2pk0zhww
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-meadow-378
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/2pk0zhww
Epoch 0, Batch 0, train loss:8.377289772033691, Elapsed time for epoch : 0.012827245394388835
Epoch 0, Batch 10, train loss:3.962484359741211, Elapsed time for epoch : 0.12895110448201497
Epoch 0, Batch 20, train loss:2.6361939907073975, Elapsed time for epoch : 0.24475034475326538
Epoch 0, Batch 30, train loss:2.4307284355163574, Elapsed time for epoch : 0.36041324138641356
Epoch 0, Batch 40, train loss:2.249875068664551, Elapsed time for epoch : 0.4763222138086955
Epoch 0, Batch 50, train loss:2.109753370285034, Elapsed time for epoch : 0.5919895887374877
Epoch 0, Batch 60, train loss:2.0186474323272705, Elapsed time for epoch : 0.7078157623608907
Epoch 0, Batch 70, train loss:2.032378673553467, Elapsed time for epoch : 0.823944890499115
Epoch 0, Batch 80, train loss:1.8564453125, Elapsed time for epoch : 0.9400085926055908
Epoch 0, Batch 90, train loss:1.6993939876556396, Elapsed time for epoch : 1.0556604981422424
Epoch 0, Batch 100, train loss:1.6150197982788086, Elapsed time for epoch : 1.1717769106229146
Epoch 0, Batch 110, train loss:1.4343911409378052, Elapsed time for epoch : 1.2878882010777792
Batch 0, val loss:5.3561224937438965
Batch 10, val loss:4.999115467071533
Batch 20, val loss:4.017043113708496
Batch 30, val loss:10.606844902038574
Epoch 0, Train Loss:2.9322607942249466, Val loss:4.938405063417223
Epoch 1, Batch 0, train loss:1.5842889547348022, Elapsed time for epoch : 0.011626803874969482
Epoch 1, Batch 10, train loss:1.4762827157974243, Elapsed time for epoch : 0.12793869574864705
Epoch 1, Batch 20, train loss:1.3357079029083252, Elapsed time for epoch : 0.24426381985346476
Epoch 1, Batch 30, train loss:1.350999116897583, Elapsed time for epoch : 0.36052428086598715
Epoch 1, Batch 40, train loss:1.2711725234985352, Elapsed time for epoch : 0.47650439341863
Epoch 1, Batch 50, train loss:1.4199365377426147, Elapsed time for epoch : 0.5927520473798116
Epoch 1, Batch 60, train loss:1.2077184915542603, Elapsed time for epoch : 0.7090892632802327
Epoch 1, Batch 70, train loss:1.195571780204773, Elapsed time for epoch : 0.8259959657986958
Epoch 1, Batch 80, train loss:1.2362747192382812, Elapsed time for epoch : 0.9421858469645182
Epoch 1, Batch 90, train loss:1.138885498046875, Elapsed time for epoch : 1.0585057298342386
Epoch 1, Batch 100, train loss:1.0107471942901611, Elapsed time for epoch : 1.1746620217959085
Epoch 1, Batch 110, train loss:1.0664209127426147, Elapsed time for epoch : 1.2908109744389853
Batch 0, val loss:2.132316827774048
Batch 10, val loss:5.68829345703125
Batch 20, val loss:2.4403531551361084
Batch 30, val loss:2.3382129669189453
Epoch 1, Train Loss:1.2546125857726387, Val loss:7.281129128403133
Epoch 2, Batch 0, train loss:1.0181320905685425, Elapsed time for epoch : 0.011649394035339355
Epoch 2, Batch 10, train loss:1.0428186655044556, Elapsed time for epoch : 0.12794686158498128
Epoch 2, Batch 20, train loss:1.0189348459243774, Elapsed time for epoch : 0.24406288067499796
Epoch 2, Batch 30, train loss:1.014293909072876, Elapsed time for epoch : 0.3599077224731445
Epoch 2, Batch 40, train loss:1.042671799659729, Elapsed time for epoch : 0.476295538743337
Epoch 2, Batch 50, train loss:1.07573664188385, Elapsed time for epoch : 0.5924578626950582
Epoch 2, Batch 60, train loss:0.9814371466636658, Elapsed time for epoch : 0.7086686372756958
Epoch 2, Batch 70, train loss:0.8408387303352356, Elapsed time for epoch : 0.8249481360117594
Epoch 2, Batch 80, train loss:1.0270781517028809, Elapsed time for epoch : 0.9415958364804585
Epoch 2, Batch 90, train loss:0.9347370862960815, Elapsed time for epoch : 1.0573549270629883
Epoch 2, Batch 100, train loss:1.0027772188186646, Elapsed time for epoch : 1.1734439373016357
Epoch 2, Batch 110, train loss:0.9569674134254456, Elapsed time for epoch : 1.2898393114407858
Batch 0, val loss:4.811682224273682
Batch 10, val loss:1.8210285902023315
Batch 20, val loss:1.8893451690673828
Batch 30, val loss:10.577445983886719
Epoch 2, Train Loss:0.9697124854378079, Val loss:6.640671604209476
Epoch 3, Batch 0, train loss:0.8640637397766113, Elapsed time for epoch : 0.011642825603485108
Epoch 3, Batch 10, train loss:0.912526547908783, Elapsed time for epoch : 0.1279106060663859
Epoch 3, Batch 20, train loss:0.8838380575180054, Elapsed time for epoch : 0.24395956993103027
Epoch 3, Batch 30, train loss:0.9399908781051636, Elapsed time for epoch : 0.35999950965245564
Epoch 3, Batch 40, train loss:0.9303320050239563, Elapsed time for epoch : 0.4764009793599447
Epoch 3, Batch 50, train loss:0.6557415127754211, Elapsed time for epoch : 0.5922496120134989
Epoch 3, Batch 60, train loss:0.7994666695594788, Elapsed time for epoch : 0.7084969242413839
Epoch 3, Batch 70, train loss:0.9219720959663391, Elapsed time for epoch : 0.8248382170995077
Epoch 3, Batch 80, train loss:0.828762412071228, Elapsed time for epoch : 0.9411055326461792
Epoch 3, Batch 90, train loss:0.7741662263870239, Elapsed time for epoch : 1.0571848471959433
Epoch 3, Batch 100, train loss:0.5153716206550598, Elapsed time for epoch : 1.1734938740730285
Epoch 3, Batch 110, train loss:0.7662330269813538, Elapsed time for epoch : 1.2897945801417032
Batch 0, val loss:8.705588340759277
Batch 10, val loss:2.8443477153778076
Batch 20, val loss:11.62418270111084
Batch 30, val loss:4.849358558654785
Epoch 3, Train Loss:0.8227806179419808, Val loss:8.620897804697355
Epoch 4, Batch 0, train loss:0.7212005853652954, Elapsed time for epoch : 0.011961464087168376
Epoch 4, Batch 10, train loss:0.576321005821228, Elapsed time for epoch : 0.12823807795842487
Epoch 4, Batch 20, train loss:0.4909040927886963, Elapsed time for epoch : 0.24441977739334106
Epoch 4, Batch 30, train loss:0.6309860348701477, Elapsed time for epoch : 0.36068277756373085
Epoch 4, Batch 40, train loss:0.6711864471435547, Elapsed time for epoch : 0.4773809393246969
Epoch 4, Batch 50, train loss:0.8385125994682312, Elapsed time for epoch : 0.5941121856371562
Epoch 4, Batch 60, train loss:0.6458786129951477, Elapsed time for epoch : 0.7106136163075765
Epoch 4, Batch 70, train loss:0.7409620881080627, Elapsed time for epoch : 0.8267376701037089
Epoch 4, Batch 80, train loss:0.5205360651016235, Elapsed time for epoch : 0.9430612643559774
Epoch 4, Batch 90, train loss:0.6814155578613281, Elapsed time for epoch : 1.0593682050704956
Epoch 4, Batch 100, train loss:0.6477091908454895, Elapsed time for epoch : 1.1755144278208414
Epoch 4, Batch 110, train loss:0.6803750991821289, Elapsed time for epoch : 1.2918628295262655
Batch 0, val loss:2.6878490447998047
Batch 10, val loss:2.631075143814087
Batch 20, val loss:4.974578380584717
Batch 30, val loss:3.9325108528137207
Epoch 4, Train Loss:0.6455215775448343, Val loss:6.029855425159137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÑ‚ñà‚ñÉ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.64552
wandb:         Val Loss 6.02986
wandb:      train_batch 110
wandb: train_batch_loss 0.68038
wandb:        val_batch 30
wandb:   val_batch_loss 3.93251
wandb: 
wandb: üöÄ View run grateful-meadow-378 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/2pk0zhww
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_043544-2pk0zhww/logs
Seed completed execution! 113 0.9_2
------------------------------------------------------------------
Experiment complete 0.9_2
==========================================================================
Running experiment for setting 0.9_3
==========================================================================
Running for seed 1 of experiment 0.9_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_044330-f4bgx3oo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-cloud-380
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/f4bgx3oo
Epoch 0, Batch 0, train loss:8.916215896606445, Elapsed time for epoch : 0.013670198122660319
Epoch 0, Batch 10, train loss:4.691886901855469, Elapsed time for epoch : 0.13745479981104533
Epoch 0, Batch 20, train loss:3.4133141040802, Elapsed time for epoch : 0.2621185024579366
Epoch 0, Batch 30, train loss:3.8749122619628906, Elapsed time for epoch : 0.38663256565729776
Epoch 0, Batch 40, train loss:3.1858108043670654, Elapsed time for epoch : 0.510822073618571
Epoch 0, Batch 50, train loss:2.67423677444458, Elapsed time for epoch : 0.6331975777943929
Epoch 0, Batch 60, train loss:2.9132046699523926, Elapsed time for epoch : 0.7565047820409139
Epoch 0, Batch 70, train loss:2.7376558780670166, Elapsed time for epoch : 0.8802078445752461
Epoch 0, Batch 80, train loss:2.549260377883911, Elapsed time for epoch : 1.003382138411204
Epoch 0, Batch 90, train loss:2.1031641960144043, Elapsed time for epoch : 1.12777632077535
Epoch 0, Batch 100, train loss:2.013733148574829, Elapsed time for epoch : 1.2515940229098002
Epoch 0, Batch 110, train loss:2.0248303413391113, Elapsed time for epoch : 1.3756037433942159
Batch 0, val loss:4.854668140411377
Batch 10, val loss:7.857265949249268
Batch 20, val loss:5.066330909729004
Batch 30, val loss:4.950678825378418
Epoch 0, Train Loss:3.1017348579738453, Val loss:3.9476868642701044
Epoch 1, Batch 0, train loss:1.7459460496902466, Elapsed time for epoch : 0.01167284647623698
Epoch 1, Batch 10, train loss:1.7370903491973877, Elapsed time for epoch : 0.1278068741162618
Epoch 1, Batch 20, train loss:1.5127769708633423, Elapsed time for epoch : 0.2445621371269226
Epoch 1, Batch 30, train loss:1.612865686416626, Elapsed time for epoch : 0.3608806888262431
Epoch 1, Batch 40, train loss:1.3191410303115845, Elapsed time for epoch : 0.47714760303497317
Epoch 1, Batch 50, train loss:1.4145313501358032, Elapsed time for epoch : 0.5937222441037496
Epoch 1, Batch 60, train loss:1.2694171667099, Elapsed time for epoch : 0.7100926796595256
Epoch 1, Batch 70, train loss:1.2578967809677124, Elapsed time for epoch : 0.826878039042155
Epoch 1, Batch 80, train loss:1.241041898727417, Elapsed time for epoch : 0.9432332356770833
Epoch 1, Batch 90, train loss:1.096875786781311, Elapsed time for epoch : 1.059560736020406
Epoch 1, Batch 100, train loss:0.8814944624900818, Elapsed time for epoch : 1.176218581199646
Epoch 1, Batch 110, train loss:0.9430862069129944, Elapsed time for epoch : 1.2925158699353536
Batch 0, val loss:2.3173210620880127
Batch 10, val loss:6.8642964363098145
Batch 20, val loss:3.5577175617218018
Batch 30, val loss:3.0470669269561768
Epoch 1, Train Loss:1.3673465573269388, Val loss:6.962231169144313
Epoch 2, Batch 0, train loss:0.9352282285690308, Elapsed time for epoch : 0.01168672243754069
Epoch 2, Batch 10, train loss:0.9964489340782166, Elapsed time for epoch : 0.12838604052861533
Epoch 2, Batch 20, train loss:0.9475878477096558, Elapsed time for epoch : 0.24455621242523193
Epoch 2, Batch 30, train loss:0.867854118347168, Elapsed time for epoch : 0.3611118793487549
Epoch 2, Batch 40, train loss:0.9422040581703186, Elapsed time for epoch : 0.4774403889973958
Epoch 2, Batch 50, train loss:0.9292062520980835, Elapsed time for epoch : 0.5938860297203064
Epoch 2, Batch 60, train loss:0.8810221552848816, Elapsed time for epoch : 0.7105291525522868
Epoch 2, Batch 70, train loss:0.5249241590499878, Elapsed time for epoch : 0.8273105104764302
Epoch 2, Batch 80, train loss:0.8964123725891113, Elapsed time for epoch : 0.9440810481707255
Epoch 2, Batch 90, train loss:0.7912834882736206, Elapsed time for epoch : 1.0604480067888895
Epoch 2, Batch 100, train loss:0.8737157583236694, Elapsed time for epoch : 1.1771333972613016
Epoch 2, Batch 110, train loss:0.7265523672103882, Elapsed time for epoch : 1.2936008294423422
Batch 0, val loss:12.338790893554688
Batch 10, val loss:6.204894542694092
Batch 20, val loss:3.331613063812256
Batch 30, val loss:15.59557056427002
Epoch 2, Train Loss:0.8654877701531286, Val loss:8.179983387390772
Epoch 3, Batch 0, train loss:0.8485713601112366, Elapsed time for epoch : 0.011702430248260499
Epoch 3, Batch 10, train loss:0.7687476277351379, Elapsed time for epoch : 0.12800916035970053
Epoch 3, Batch 20, train loss:0.838746964931488, Elapsed time for epoch : 0.24468930959701538
Epoch 3, Batch 30, train loss:0.683708131313324, Elapsed time for epoch : 0.3613797227541606
Epoch 3, Batch 40, train loss:0.6609269976615906, Elapsed time for epoch : 0.4776322642962138
Epoch 3, Batch 50, train loss:0.32870087027549744, Elapsed time for epoch : 0.5940275073051453
Epoch 3, Batch 60, train loss:0.6658461689949036, Elapsed time for epoch : 0.7106357296307881
Epoch 3, Batch 70, train loss:0.7150405645370483, Elapsed time for epoch : 0.826945980389913
Epoch 3, Batch 80, train loss:0.6529267430305481, Elapsed time for epoch : 0.9431809266408284
Epoch 3, Batch 90, train loss:0.5216923952102661, Elapsed time for epoch : 1.05926517645518
Epoch 3, Batch 100, train loss:0.2518913149833679, Elapsed time for epoch : 1.1755966265996298
Epoch 3, Batch 110, train loss:0.5662657022476196, Elapsed time for epoch : 1.2921083370844524
Batch 0, val loss:7.458957195281982
Batch 10, val loss:15.259635925292969
Batch 20, val loss:10.379746437072754
Batch 30, val loss:3.2154595851898193
Epoch 3, Train Loss:0.6086542269457942, Val loss:15.071709089808994
Epoch 4, Batch 0, train loss:0.6726137399673462, Elapsed time for epoch : 0.011666603883107503
Epoch 4, Batch 10, train loss:0.2890586256980896, Elapsed time for epoch : 0.12861301898956298
Epoch 4, Batch 20, train loss:0.18333227932453156, Elapsed time for epoch : 0.2458346128463745
Epoch 4, Batch 30, train loss:0.6059917211532593, Elapsed time for epoch : 0.36277003288269044
Epoch 4, Batch 40, train loss:0.4491386115550995, Elapsed time for epoch : 0.47933867772420247
Epoch 4, Batch 50, train loss:0.4621746838092804, Elapsed time for epoch : 0.5962263385454813
Epoch 4, Batch 60, train loss:0.4045080840587616, Elapsed time for epoch : 0.712376081943512
Epoch 4, Batch 70, train loss:0.45058760046958923, Elapsed time for epoch : 0.8289697885513305
Epoch 4, Batch 80, train loss:0.17809638381004333, Elapsed time for epoch : 0.9455209294954936
Epoch 4, Batch 90, train loss:0.47426387667655945, Elapsed time for epoch : 1.0620374878247578
Epoch 4, Batch 100, train loss:0.379985511302948, Elapsed time for epoch : 1.1786661028862
Epoch 4, Batch 110, train loss:0.432110071182251, Elapsed time for epoch : 1.2951211651166281
Batch 0, val loss:9.006107330322266
Batch 10, val loss:3.1340649127960205
Batch 20, val loss:8.553224563598633
Batch 30, val loss:3.242004156112671
Epoch 4, Train Loss:0.4057252436876297, Val loss:8.369107441769707
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.40573
wandb:         Val Loss 8.36911
wandb:      train_batch 110
wandb: train_batch_loss 0.43211
wandb:        val_batch 30
wandb:   val_batch_loss 3.242
wandb: 
wandb: üöÄ View run solar-cloud-380 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/f4bgx3oo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_044330-f4bgx3oo/logs
Seed completed execution! 1 0.9_3
------------------------------------------------------------------
Running for seed 42 of experiment 0.9_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_045124-xjgmsfm2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-wildflower-382
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xjgmsfm2
Epoch 0, Batch 0, train loss:8.916215896606445, Elapsed time for epoch : 0.013575482368469238
Epoch 0, Batch 10, train loss:4.691886901855469, Elapsed time for epoch : 0.12977519035339355
Epoch 0, Batch 20, train loss:3.4133141040802, Elapsed time for epoch : 0.24603596925735474
Epoch 0, Batch 30, train loss:3.8749122619628906, Elapsed time for epoch : 0.362344491481781
Epoch 0, Batch 40, train loss:3.1858108043670654, Elapsed time for epoch : 0.478591787815094
Epoch 0, Batch 50, train loss:2.67423677444458, Elapsed time for epoch : 0.5950699845949808
Epoch 0, Batch 60, train loss:2.9132046699523926, Elapsed time for epoch : 0.7114111065864563
Epoch 0, Batch 70, train loss:2.7376558780670166, Elapsed time for epoch : 0.8276732285817464
Epoch 0, Batch 80, train loss:2.549260377883911, Elapsed time for epoch : 0.9443660418192545
Epoch 0, Batch 90, train loss:2.1031641960144043, Elapsed time for epoch : 1.0609715461730957
Epoch 0, Batch 100, train loss:2.013733148574829, Elapsed time for epoch : 1.1772274017333983
Epoch 0, Batch 110, train loss:2.0248303413391113, Elapsed time for epoch : 1.2936024904251098
Batch 0, val loss:4.854668140411377
Batch 10, val loss:7.857265949249268
Batch 20, val loss:5.066330909729004
Batch 30, val loss:4.950678825378418
Epoch 0, Train Loss:3.1017348579738453, Val loss:3.9476868642701044
Epoch 1, Batch 0, train loss:1.7459460496902466, Elapsed time for epoch : 0.011753980318705242
Epoch 1, Batch 10, train loss:1.7370903491973877, Elapsed time for epoch : 0.1282188375790914
Epoch 1, Batch 20, train loss:1.5127769708633423, Elapsed time for epoch : 0.24481465419133505
Epoch 1, Batch 30, train loss:1.612865686416626, Elapsed time for epoch : 0.36127593119939166
Epoch 1, Batch 40, train loss:1.3191410303115845, Elapsed time for epoch : 0.4775241374969482
Epoch 1, Batch 50, train loss:1.4145313501358032, Elapsed time for epoch : 0.5942790110905966
Epoch 1, Batch 60, train loss:1.2694171667099, Elapsed time for epoch : 0.7105880498886108
Epoch 1, Batch 70, train loss:1.2578967809677124, Elapsed time for epoch : 0.8270103534062704
Epoch 1, Batch 80, train loss:1.241041898727417, Elapsed time for epoch : 0.9432945569356282
Epoch 1, Batch 90, train loss:1.096875786781311, Elapsed time for epoch : 1.059793265660604
Epoch 1, Batch 100, train loss:0.8814944624900818, Elapsed time for epoch : 1.1763315717379252
Epoch 1, Batch 110, train loss:0.9430862069129944, Elapsed time for epoch : 1.2928871909777324
Batch 0, val loss:2.3173210620880127
Batch 10, val loss:6.8642964363098145
Batch 20, val loss:3.5577175617218018
Batch 30, val loss:3.0470669269561768
Epoch 1, Train Loss:1.3673465573269388, Val loss:6.962231169144313
Epoch 2, Batch 0, train loss:0.9352282285690308, Elapsed time for epoch : 0.011721535523732503
Epoch 2, Batch 10, train loss:0.9964489340782166, Elapsed time for epoch : 0.12834552526474
Epoch 2, Batch 20, train loss:0.9475878477096558, Elapsed time for epoch : 0.24496594270070393
Epoch 2, Batch 30, train loss:0.867854118347168, Elapsed time for epoch : 0.3614972233772278
Epoch 2, Batch 40, train loss:0.9422040581703186, Elapsed time for epoch : 0.47797408103942873
Epoch 2, Batch 50, train loss:0.9292062520980835, Elapsed time for epoch : 0.5944260040918986
Epoch 2, Batch 60, train loss:0.8810221552848816, Elapsed time for epoch : 0.7110500017801921
Epoch 2, Batch 70, train loss:0.5249241590499878, Elapsed time for epoch : 0.8275218486785889
Epoch 2, Batch 80, train loss:0.8964123725891113, Elapsed time for epoch : 0.9442191123962402
Epoch 2, Batch 90, train loss:0.7912834882736206, Elapsed time for epoch : 1.0607932647069296
Epoch 2, Batch 100, train loss:0.8737157583236694, Elapsed time for epoch : 1.1773663282394409
Epoch 2, Batch 110, train loss:0.7265523672103882, Elapsed time for epoch : 1.29375319480896
Batch 0, val loss:12.338790893554688
Batch 10, val loss:6.204894542694092
Batch 20, val loss:3.331613063812256
Batch 30, val loss:15.59557056427002
Epoch 2, Train Loss:0.8654877701531286, Val loss:8.179983387390772
Epoch 3, Batch 0, train loss:0.8485713601112366, Elapsed time for epoch : 0.011709634462992351
Epoch 3, Batch 10, train loss:0.7687476277351379, Elapsed time for epoch : 0.12798234224319457
Epoch 3, Batch 20, train loss:0.838746964931488, Elapsed time for epoch : 0.24462032715479534
Epoch 3, Batch 30, train loss:0.683708131313324, Elapsed time for epoch : 0.3607901732126872
Epoch 3, Batch 40, train loss:0.6609269976615906, Elapsed time for epoch : 0.4770640254020691
Epoch 3, Batch 50, train loss:0.32870087027549744, Elapsed time for epoch : 0.5933815161387126
Epoch 3, Batch 60, train loss:0.6658461689949036, Elapsed time for epoch : 0.7100835124651591
Epoch 3, Batch 70, train loss:0.7150405645370483, Elapsed time for epoch : 0.8263657132784525
Epoch 3, Batch 80, train loss:0.6529267430305481, Elapsed time for epoch : 0.9431637803713481
Epoch 3, Batch 90, train loss:0.5216923952102661, Elapsed time for epoch : 1.0598264892896017
Epoch 3, Batch 100, train loss:0.2518913149833679, Elapsed time for epoch : 1.1762858947118124
Epoch 3, Batch 110, train loss:0.5662657022476196, Elapsed time for epoch : 1.292990779876709
Batch 0, val loss:7.458957195281982
Batch 10, val loss:15.259635925292969
Batch 20, val loss:10.379746437072754
Batch 30, val loss:3.2154595851898193
Epoch 3, Train Loss:0.6086542269457942, Val loss:15.071709089808994
Epoch 4, Batch 0, train loss:0.6726137399673462, Elapsed time for epoch : 0.011703876654307048
Epoch 4, Batch 10, train loss:0.2890586256980896, Elapsed time for epoch : 0.1285154382387797
Epoch 4, Batch 20, train loss:0.18333227932453156, Elapsed time for epoch : 0.2449213663736979
Epoch 4, Batch 30, train loss:0.6059917211532593, Elapsed time for epoch : 0.36083097457885743
Epoch 4, Batch 40, train loss:0.4491386115550995, Elapsed time for epoch : 0.47734525203704836
Epoch 4, Batch 50, train loss:0.4621746838092804, Elapsed time for epoch : 0.5935744682947794
Epoch 4, Batch 60, train loss:0.4045080840587616, Elapsed time for epoch : 0.7098723173141479
Epoch 4, Batch 70, train loss:0.45058760046958923, Elapsed time for epoch : 0.8260843475659688
Epoch 4, Batch 80, train loss:0.17809638381004333, Elapsed time for epoch : 0.9425593495368958
Epoch 4, Batch 90, train loss:0.47426387667655945, Elapsed time for epoch : 1.0589988231658936
Epoch 4, Batch 100, train loss:0.379985511302948, Elapsed time for epoch : 1.1751841306686401
Epoch 4, Batch 110, train loss:0.432110071182251, Elapsed time for epoch : 1.2912121415138245
Batch 0, val loss:9.006107330322266
Batch 10, val loss:3.1340649127960205
Batch 20, val loss:8.553224563598633
Batch 30, val loss:3.242004156112671
Epoch 4, Train Loss:0.4057252436876297, Val loss:8.369107441769707
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.40573
wandb:         Val Loss 8.36911
wandb:      train_batch 110
wandb: train_batch_loss 0.43211
wandb:        val_batch 30
wandb:   val_batch_loss 3.242
wandb: 
wandb: üöÄ View run scarlet-wildflower-382 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xjgmsfm2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_045124-xjgmsfm2/logs
Seed completed execution! 42 0.9_3
------------------------------------------------------------------
Running for seed 89 of experiment 0.9_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_045912-xm9pn4zy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-dream-384
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xm9pn4zy
Epoch 0, Batch 0, train loss:8.916215896606445, Elapsed time for epoch : 0.01347490946451823
Epoch 0, Batch 10, train loss:4.691886901855469, Elapsed time for epoch : 0.12952262163162231
Epoch 0, Batch 20, train loss:3.4133141040802, Elapsed time for epoch : 0.24544300238291422
Epoch 0, Batch 30, train loss:3.8749122619628906, Elapsed time for epoch : 0.3616527477900187
Epoch 0, Batch 40, train loss:3.1858108043670654, Elapsed time for epoch : 0.47782710790634153
Epoch 0, Batch 50, train loss:2.67423677444458, Elapsed time for epoch : 0.5940172433853149
Epoch 0, Batch 60, train loss:2.9132046699523926, Elapsed time for epoch : 0.7106398224830628
Epoch 0, Batch 70, train loss:2.7376558780670166, Elapsed time for epoch : 0.82738329966863
Epoch 0, Batch 80, train loss:2.549260377883911, Elapsed time for epoch : 0.9436820864677429
Epoch 0, Batch 90, train loss:2.1031641960144043, Elapsed time for epoch : 1.0600967327753703
Epoch 0, Batch 100, train loss:2.013733148574829, Elapsed time for epoch : 1.1764669259389242
Epoch 0, Batch 110, train loss:2.0248303413391113, Elapsed time for epoch : 1.293091098467509
Batch 0, val loss:4.854668140411377
Batch 10, val loss:7.857265949249268
Batch 20, val loss:5.066330909729004
Batch 30, val loss:4.950678825378418
Epoch 0, Train Loss:3.1017348579738453, Val loss:3.9476868642701044
Epoch 1, Batch 0, train loss:1.7459460496902466, Elapsed time for epoch : 0.012102838357289631
Epoch 1, Batch 10, train loss:1.7370903491973877, Elapsed time for epoch : 0.12842450539271036
Epoch 1, Batch 20, train loss:1.5127769708633423, Elapsed time for epoch : 0.24519824186960856
Epoch 1, Batch 30, train loss:1.612865686416626, Elapsed time for epoch : 0.3619463562965393
Epoch 1, Batch 40, train loss:1.3191410303115845, Elapsed time for epoch : 0.47879249254862466
Epoch 1, Batch 50, train loss:1.4145313501358032, Elapsed time for epoch : 0.5953931689262391
Epoch 1, Batch 60, train loss:1.2694171667099, Elapsed time for epoch : 0.7117559274037679
Epoch 1, Batch 70, train loss:1.2578967809677124, Elapsed time for epoch : 0.8283265113830567
Epoch 1, Batch 80, train loss:1.241041898727417, Elapsed time for epoch : 0.9449248870213827
Epoch 1, Batch 90, train loss:1.096875786781311, Elapsed time for epoch : 1.061461579799652
Epoch 1, Batch 100, train loss:0.8814944624900818, Elapsed time for epoch : 1.1779597600301106
Epoch 1, Batch 110, train loss:0.9430862069129944, Elapsed time for epoch : 1.2946065743764241
Batch 0, val loss:2.3173210620880127
Batch 10, val loss:6.8642964363098145
Batch 20, val loss:3.5577175617218018
Batch 30, val loss:3.0470669269561768
Epoch 1, Train Loss:1.3673465573269388, Val loss:6.962231169144313
Epoch 2, Batch 0, train loss:0.9352282285690308, Elapsed time for epoch : 0.011682116985321045
Epoch 2, Batch 10, train loss:0.9964489340782166, Elapsed time for epoch : 0.12854115962982177
Epoch 2, Batch 20, train loss:0.9475878477096558, Elapsed time for epoch : 0.24539507627487184
Epoch 2, Batch 30, train loss:0.867854118347168, Elapsed time for epoch : 0.36173888444900515
Epoch 2, Batch 40, train loss:0.9422040581703186, Elapsed time for epoch : 0.4784483114878337
Epoch 2, Batch 50, train loss:0.9292062520980835, Elapsed time for epoch : 0.594881824652354
Epoch 2, Batch 60, train loss:0.8810221552848816, Elapsed time for epoch : 0.7113292256991068
Epoch 2, Batch 70, train loss:0.5249241590499878, Elapsed time for epoch : 0.8280773282051086
Epoch 2, Batch 80, train loss:0.8964123725891113, Elapsed time for epoch : 0.944517457485199
Epoch 2, Batch 90, train loss:0.7912834882736206, Elapsed time for epoch : 1.06100035905838
Epoch 2, Batch 100, train loss:0.8737157583236694, Elapsed time for epoch : 1.1775483210881552
Epoch 2, Batch 110, train loss:0.7265523672103882, Elapsed time for epoch : 1.2940251231193542
Batch 0, val loss:12.338790893554688
Batch 10, val loss:6.204894542694092
Batch 20, val loss:3.331613063812256
Batch 30, val loss:15.59557056427002
Epoch 2, Train Loss:0.8654877701531286, Val loss:8.179983387390772
Epoch 3, Batch 0, train loss:0.8485713601112366, Elapsed time for epoch : 0.011699914932250977
Epoch 3, Batch 10, train loss:0.7687476277351379, Elapsed time for epoch : 0.128475821018219
Epoch 3, Batch 20, train loss:0.838746964931488, Elapsed time for epoch : 0.24495630661646525
Epoch 3, Batch 30, train loss:0.683708131313324, Elapsed time for epoch : 0.3613293965657552
Epoch 3, Batch 40, train loss:0.6609269976615906, Elapsed time for epoch : 0.4778117656707764
Epoch 3, Batch 50, train loss:0.32870087027549744, Elapsed time for epoch : 0.5942648450533549
Epoch 3, Batch 60, train loss:0.6658461689949036, Elapsed time for epoch : 0.710877788066864
Epoch 3, Batch 70, train loss:0.7150405645370483, Elapsed time for epoch : 0.8275193373362223
Epoch 3, Batch 80, train loss:0.6529267430305481, Elapsed time for epoch : 0.9437103589375814
Epoch 3, Batch 90, train loss:0.5216923952102661, Elapsed time for epoch : 1.0600158413251242
Epoch 3, Batch 100, train loss:0.2518913149833679, Elapsed time for epoch : 1.1762567202250163
Epoch 3, Batch 110, train loss:0.5662657022476196, Elapsed time for epoch : 1.2930137236913046
Batch 0, val loss:7.458957195281982
Batch 10, val loss:15.259635925292969
Batch 20, val loss:10.379746437072754
Batch 30, val loss:3.2154595851898193
Epoch 3, Train Loss:0.6086542269457942, Val loss:15.071709089808994
Epoch 4, Batch 0, train loss:0.6726137399673462, Elapsed time for epoch : 0.011671872933705647
Epoch 4, Batch 10, train loss:0.2890586256980896, Elapsed time for epoch : 0.12811227242151896
Epoch 4, Batch 20, train loss:0.18333227932453156, Elapsed time for epoch : 0.24456604719161987
Epoch 4, Batch 30, train loss:0.6059917211532593, Elapsed time for epoch : 0.361129903793335
Epoch 4, Batch 40, train loss:0.4491386115550995, Elapsed time for epoch : 0.4775547782580058
Epoch 4, Batch 50, train loss:0.4621746838092804, Elapsed time for epoch : 0.5939346035321553
Epoch 4, Batch 60, train loss:0.4045080840587616, Elapsed time for epoch : 0.7104574759801229
Epoch 4, Batch 70, train loss:0.45058760046958923, Elapsed time for epoch : 0.8268383979797364
Epoch 4, Batch 80, train loss:0.17809638381004333, Elapsed time for epoch : 0.9434094429016113
Epoch 4, Batch 90, train loss:0.47426387667655945, Elapsed time for epoch : 1.05981631676356
Epoch 4, Batch 100, train loss:0.379985511302948, Elapsed time for epoch : 1.1762551665306091
Epoch 4, Batch 110, train loss:0.432110071182251, Elapsed time for epoch : 1.292688218752543
Batch 0, val loss:9.006107330322266
Batch 10, val loss:3.1340649127960205
Batch 20, val loss:8.553224563598633
Batch 30, val loss:3.242004156112671
Epoch 4, Train Loss:0.4057252436876297, Val loss:8.369107441769707
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.40573
wandb:         Val Loss 8.36911
wandb:      train_batch 110
wandb: train_batch_loss 0.43211
wandb:        val_batch 30
wandb:   val_batch_loss 3.242
wandb: 
wandb: üöÄ View run earnest-dream-384 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xm9pn4zy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_045912-xm9pn4zy/logs
Seed completed execution! 89 0.9_3
------------------------------------------------------------------
Running for seed 23 of experiment 0.9_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_050700-vxti8tyn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-donkey-386
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/vxti8tyn
Epoch 0, Batch 0, train loss:8.916215896606445, Elapsed time for epoch : 0.012187520662943522
Epoch 0, Batch 10, train loss:4.691886901855469, Elapsed time for epoch : 0.12865519523620605
Epoch 0, Batch 20, train loss:3.4133141040802, Elapsed time for epoch : 0.24460879961649576
Epoch 0, Batch 30, train loss:3.8749122619628906, Elapsed time for epoch : 0.36072105964024864
Epoch 0, Batch 40, train loss:3.1858108043670654, Elapsed time for epoch : 0.476713760693868
Epoch 0, Batch 50, train loss:2.67423677444458, Elapsed time for epoch : 0.5931695103645325
Epoch 0, Batch 60, train loss:2.9132046699523926, Elapsed time for epoch : 0.7093226830164592
Epoch 0, Batch 70, train loss:2.7376558780670166, Elapsed time for epoch : 0.8255584438641866
Epoch 0, Batch 80, train loss:2.549260377883911, Elapsed time for epoch : 0.9424569010734558
Epoch 0, Batch 90, train loss:2.1031641960144043, Elapsed time for epoch : 1.0584976116816203
Epoch 0, Batch 100, train loss:2.013733148574829, Elapsed time for epoch : 1.175040853023529
Epoch 0, Batch 110, train loss:2.0248303413391113, Elapsed time for epoch : 1.2919437925020854
Batch 0, val loss:4.854668140411377
Batch 10, val loss:7.857265949249268
Batch 20, val loss:5.066330909729004
Batch 30, val loss:4.950678825378418
Epoch 0, Train Loss:3.1017348579738453, Val loss:3.9476868642701044
Epoch 1, Batch 0, train loss:1.7459460496902466, Elapsed time for epoch : 0.011714577674865723
Epoch 1, Batch 10, train loss:1.7370903491973877, Elapsed time for epoch : 0.12823376655578614
Epoch 1, Batch 20, train loss:1.5127769708633423, Elapsed time for epoch : 0.24461020628611246
Epoch 1, Batch 30, train loss:1.612865686416626, Elapsed time for epoch : 0.3609081308046977
Epoch 1, Batch 40, train loss:1.3191410303115845, Elapsed time for epoch : 0.4775908629099528
Epoch 1, Batch 50, train loss:1.4145313501358032, Elapsed time for epoch : 0.5940084457397461
Epoch 1, Batch 60, train loss:1.2694171667099, Elapsed time for epoch : 0.7102333426475524
Epoch 1, Batch 70, train loss:1.2578967809677124, Elapsed time for epoch : 0.8266387184460958
Epoch 1, Batch 80, train loss:1.241041898727417, Elapsed time for epoch : 0.9435409029324849
Epoch 1, Batch 90, train loss:1.096875786781311, Elapsed time for epoch : 1.0602622906366983
Epoch 1, Batch 100, train loss:0.8814944624900818, Elapsed time for epoch : 1.176855238278707
Epoch 1, Batch 110, train loss:0.9430862069129944, Elapsed time for epoch : 1.2935301025708517
Batch 0, val loss:2.3173210620880127
Batch 10, val loss:6.8642964363098145
Batch 20, val loss:3.5577175617218018
Batch 30, val loss:3.0470669269561768
Epoch 1, Train Loss:1.3673465573269388, Val loss:6.962231169144313
Epoch 2, Batch 0, train loss:0.9352282285690308, Elapsed time for epoch : 0.0119163711865743
Epoch 2, Batch 10, train loss:0.9964489340782166, Elapsed time for epoch : 0.12839864095052084
Epoch 2, Batch 20, train loss:0.9475878477096558, Elapsed time for epoch : 0.2450733224550883
Epoch 2, Batch 30, train loss:0.867854118347168, Elapsed time for epoch : 0.361345100402832
Epoch 2, Batch 40, train loss:0.9422040581703186, Elapsed time for epoch : 0.47795997858047484
Epoch 2, Batch 50, train loss:0.9292062520980835, Elapsed time for epoch : 0.5944700002670288
Epoch 2, Batch 60, train loss:0.8810221552848816, Elapsed time for epoch : 0.7109287937482198
Epoch 2, Batch 70, train loss:0.5249241590499878, Elapsed time for epoch : 0.8275769392649333
Epoch 2, Batch 80, train loss:0.8964123725891113, Elapsed time for epoch : 0.94476797580719
Epoch 2, Batch 90, train loss:0.7912834882736206, Elapsed time for epoch : 1.06137349208196
Epoch 2, Batch 100, train loss:0.8737157583236694, Elapsed time for epoch : 1.1779790043830871
Epoch 2, Batch 110, train loss:0.7265523672103882, Elapsed time for epoch : 1.2946121136347453
Batch 0, val loss:12.338790893554688
Batch 10, val loss:6.204894542694092
Batch 20, val loss:3.331613063812256
Batch 30, val loss:15.59557056427002
Epoch 2, Train Loss:0.8654877701531286, Val loss:8.179983387390772
Epoch 3, Batch 0, train loss:0.8485713601112366, Elapsed time for epoch : 0.011717073122660319
Epoch 3, Batch 10, train loss:0.7687476277351379, Elapsed time for epoch : 0.12832636038462322
Epoch 3, Batch 20, train loss:0.838746964931488, Elapsed time for epoch : 0.24488866329193115
Epoch 3, Batch 30, train loss:0.683708131313324, Elapsed time for epoch : 0.3616800983746847
Epoch 3, Batch 40, train loss:0.6609269976615906, Elapsed time for epoch : 0.4785792867342631
Epoch 3, Batch 50, train loss:0.32870087027549744, Elapsed time for epoch : 0.5953509132067363
Epoch 3, Batch 60, train loss:0.6658461689949036, Elapsed time for epoch : 0.712035330136617
Epoch 3, Batch 70, train loss:0.7150405645370483, Elapsed time for epoch : 0.828580911954244
Epoch 3, Batch 80, train loss:0.6529267430305481, Elapsed time for epoch : 0.9457663655281067
Epoch 3, Batch 90, train loss:0.5216923952102661, Elapsed time for epoch : 1.0625840107599893
Epoch 3, Batch 100, train loss:0.2518913149833679, Elapsed time for epoch : 1.179369866847992
Epoch 3, Batch 110, train loss:0.5662657022476196, Elapsed time for epoch : 1.2961408813794455
Batch 0, val loss:7.458957195281982
Batch 10, val loss:15.259635925292969
Batch 20, val loss:10.379746437072754
Batch 30, val loss:3.2154595851898193
Epoch 3, Train Loss:0.6086542269457942, Val loss:15.071709089808994
Epoch 4, Batch 0, train loss:0.6726137399673462, Elapsed time for epoch : 0.011830703417460123
Epoch 4, Batch 10, train loss:0.2890586256980896, Elapsed time for epoch : 0.12994687557220458
Epoch 4, Batch 20, train loss:0.18333227932453156, Elapsed time for epoch : 0.2473379651705424
Epoch 4, Batch 30, train loss:0.6059917211532593, Elapsed time for epoch : 0.3649137576421102
Epoch 4, Batch 40, train loss:0.4491386115550995, Elapsed time for epoch : 0.4828515807787577
Epoch 4, Batch 50, train loss:0.4621746838092804, Elapsed time for epoch : 0.6005589604377747
Epoch 4, Batch 60, train loss:0.4045080840587616, Elapsed time for epoch : 0.7183180093765259
Epoch 4, Batch 70, train loss:0.45058760046958923, Elapsed time for epoch : 0.8362823923428854
Epoch 4, Batch 80, train loss:0.17809638381004333, Elapsed time for epoch : 0.9540332118670146
Epoch 4, Batch 90, train loss:0.47426387667655945, Elapsed time for epoch : 1.071706219514211
Epoch 4, Batch 100, train loss:0.379985511302948, Elapsed time for epoch : 1.1894273241360982
Epoch 4, Batch 110, train loss:0.432110071182251, Elapsed time for epoch : 1.306786596775055
Batch 0, val loss:9.006107330322266
Batch 10, val loss:3.1340649127960205
Batch 20, val loss:8.553224563598633
Batch 30, val loss:3.242004156112671
Epoch 4, Train Loss:0.4057252436876297, Val loss:8.369107441769707
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.40573
wandb:         Val Loss 8.36911
wandb:      train_batch 110
wandb: train_batch_loss 0.43211
wandb:        val_batch 30
wandb:   val_batch_loss 3.242
wandb: 
wandb: üöÄ View run crisp-donkey-386 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/vxti8tyn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_050700-vxti8tyn/logs
Seed completed execution! 23 0.9_3
------------------------------------------------------------------
Running for seed 113 of experiment 0.9_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_051448-318mp7ct
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-snowball-388
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/318mp7ct
Epoch 0, Batch 0, train loss:8.916215896606445, Elapsed time for epoch : 0.0130834698677063
Epoch 0, Batch 10, train loss:4.691886901855469, Elapsed time for epoch : 0.1304922342300415
Epoch 0, Batch 20, train loss:3.4133141040802, Elapsed time for epoch : 0.24635623296101888
Epoch 0, Batch 30, train loss:3.8749122619628906, Elapsed time for epoch : 0.36232875982920326
Epoch 0, Batch 40, train loss:3.1858108043670654, Elapsed time for epoch : 0.47828432321548464
Epoch 0, Batch 50, train loss:2.67423677444458, Elapsed time for epoch : 0.5949027180671692
Epoch 0, Batch 60, train loss:2.9132046699523926, Elapsed time for epoch : 0.7107615788777669
Epoch 0, Batch 70, train loss:2.7376558780670166, Elapsed time for epoch : 0.8272117177645365
Epoch 0, Batch 80, train loss:2.549260377883911, Elapsed time for epoch : 0.9443050861358643
Epoch 0, Batch 90, train loss:2.1031641960144043, Elapsed time for epoch : 1.0606740196545918
Epoch 0, Batch 100, train loss:2.013733148574829, Elapsed time for epoch : 1.1769163886706033
Epoch 0, Batch 110, train loss:2.0248303413391113, Elapsed time for epoch : 1.293279778957367
Batch 0, val loss:4.854668140411377
Batch 10, val loss:7.857265949249268
Batch 20, val loss:5.066330909729004
Batch 30, val loss:4.950678825378418
Epoch 0, Train Loss:3.1017348579738453, Val loss:3.9476868642701044
Epoch 1, Batch 0, train loss:1.7459460496902466, Elapsed time for epoch : 0.011682434876759847
Epoch 1, Batch 10, train loss:1.7370903491973877, Elapsed time for epoch : 0.12793973286946614
Epoch 1, Batch 20, train loss:1.5127769708633423, Elapsed time for epoch : 0.24437209367752075
Epoch 1, Batch 30, train loss:1.612865686416626, Elapsed time for epoch : 0.36067586739857993
Epoch 1, Batch 40, train loss:1.3191410303115845, Elapsed time for epoch : 0.47713274955749513
Epoch 1, Batch 50, train loss:1.4145313501358032, Elapsed time for epoch : 0.5931479016939799
Epoch 1, Batch 60, train loss:1.2694171667099, Elapsed time for epoch : 0.7096335967381795
Epoch 1, Batch 70, train loss:1.2578967809677124, Elapsed time for epoch : 0.8262251774470012
Epoch 1, Batch 80, train loss:1.241041898727417, Elapsed time for epoch : 0.9431745727856954
Epoch 1, Batch 90, train loss:1.096875786781311, Elapsed time for epoch : 1.059454091389974
Epoch 1, Batch 100, train loss:0.8814944624900818, Elapsed time for epoch : 1.1759119828542073
Epoch 1, Batch 110, train loss:0.9430862069129944, Elapsed time for epoch : 1.2924463351567586
Batch 0, val loss:2.3173210620880127
Batch 10, val loss:6.8642964363098145
Batch 20, val loss:3.5577175617218018
Batch 30, val loss:3.0470669269561768
Epoch 1, Train Loss:1.3673465573269388, Val loss:6.962231169144313
Epoch 2, Batch 0, train loss:0.9352282285690308, Elapsed time for epoch : 0.011690266927083333
Epoch 2, Batch 10, train loss:0.9964489340782166, Elapsed time for epoch : 0.12807990709940592
Epoch 2, Batch 20, train loss:0.9475878477096558, Elapsed time for epoch : 0.2450130820274353
Epoch 2, Batch 30, train loss:0.867854118347168, Elapsed time for epoch : 0.36167065699895223
Epoch 2, Batch 40, train loss:0.9422040581703186, Elapsed time for epoch : 0.4782909234364828
Epoch 2, Batch 50, train loss:0.9292062520980835, Elapsed time for epoch : 0.5951701521873474
Epoch 2, Batch 60, train loss:0.8810221552848816, Elapsed time for epoch : 0.7116620341936747
Epoch 2, Batch 70, train loss:0.5249241590499878, Elapsed time for epoch : 0.8279075980186462
Epoch 2, Batch 80, train loss:0.8964123725891113, Elapsed time for epoch : 0.944666306177775
Epoch 2, Batch 90, train loss:0.7912834882736206, Elapsed time for epoch : 1.0612413009007773
Epoch 2, Batch 100, train loss:0.8737157583236694, Elapsed time for epoch : 1.1774853626887003
Epoch 2, Batch 110, train loss:0.7265523672103882, Elapsed time for epoch : 1.2944646914800009
Batch 0, val loss:12.338790893554688
Batch 10, val loss:6.204894542694092
Batch 20, val loss:3.331613063812256
Batch 30, val loss:15.59557056427002
Epoch 2, Train Loss:0.8654877701531286, Val loss:8.179983387390772
Epoch 3, Batch 0, train loss:0.8485713601112366, Elapsed time for epoch : 0.011664946873982748
Epoch 3, Batch 10, train loss:0.7687476277351379, Elapsed time for epoch : 0.1279718557993571
Epoch 3, Batch 20, train loss:0.838746964931488, Elapsed time for epoch : 0.24420798619588216
Epoch 3, Batch 30, train loss:0.683708131313324, Elapsed time for epoch : 0.36059937477111814
Epoch 3, Batch 40, train loss:0.6609269976615906, Elapsed time for epoch : 0.4775505224863688
Epoch 3, Batch 50, train loss:0.32870087027549744, Elapsed time for epoch : 0.5942947109540303
Epoch 3, Batch 60, train loss:0.6658461689949036, Elapsed time for epoch : 0.7106889684995016
Epoch 3, Batch 70, train loss:0.7150405645370483, Elapsed time for epoch : 0.8273654540379842
Epoch 3, Batch 80, train loss:0.6529267430305481, Elapsed time for epoch : 0.9437586108843485
Epoch 3, Batch 90, train loss:0.5216923952102661, Elapsed time for epoch : 1.0602097074190775
Epoch 3, Batch 100, train loss:0.2518913149833679, Elapsed time for epoch : 1.1768101374308269
Epoch 3, Batch 110, train loss:0.5662657022476196, Elapsed time for epoch : 1.293317985534668
Batch 0, val loss:7.458957195281982
Batch 10, val loss:15.259635925292969
Batch 20, val loss:10.379746437072754
Batch 30, val loss:3.2154595851898193
Epoch 3, Train Loss:0.6086542269457942, Val loss:15.071709089808994
Epoch 4, Batch 0, train loss:0.6726137399673462, Elapsed time for epoch : 0.011668590704600017
Epoch 4, Batch 10, train loss:0.2890586256980896, Elapsed time for epoch : 0.12812163829803466
Epoch 4, Batch 20, train loss:0.18333227932453156, Elapsed time for epoch : 0.2444971164067586
Epoch 4, Batch 30, train loss:0.6059917211532593, Elapsed time for epoch : 0.36092634598414103
Epoch 4, Batch 40, train loss:0.4491386115550995, Elapsed time for epoch : 0.4777528087298075
Epoch 4, Batch 50, train loss:0.4621746838092804, Elapsed time for epoch : 0.5944772521654765
Epoch 4, Batch 60, train loss:0.4045080840587616, Elapsed time for epoch : 0.71067373752594
Epoch 4, Batch 70, train loss:0.45058760046958923, Elapsed time for epoch : 0.827072807153066
Epoch 4, Batch 80, train loss:0.17809638381004333, Elapsed time for epoch : 0.9436874111493428
Epoch 4, Batch 90, train loss:0.47426387667655945, Elapsed time for epoch : 1.0600444237391153
Epoch 4, Batch 100, train loss:0.379985511302948, Elapsed time for epoch : 1.1762477477391562
Epoch 4, Batch 110, train loss:0.432110071182251, Elapsed time for epoch : 1.2928406437238058
Batch 0, val loss:9.006107330322266
Batch 10, val loss:3.1340649127960205
Batch 20, val loss:8.553224563598633
Batch 30, val loss:3.242004156112671
Epoch 4, Train Loss:0.4057252436876297, Val loss:8.369107441769707
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÉ‚ñÇ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.40573
wandb:         Val Loss 8.36911
wandb:      train_batch 110
wandb: train_batch_loss 0.43211
wandb:        val_batch 30
wandb:   val_batch_loss 3.242
wandb: 
wandb: üöÄ View run deep-snowball-388 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/318mp7ct
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_051448-318mp7ct/logs
Seed completed execution! 113 0.9_3
------------------------------------------------------------------
Experiment complete 0.9_3
==========================================================================
Running experiment for setting 0.9_4
==========================================================================
Running for seed 1 of experiment 0.9_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_052236-9byslt8t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-morning-390
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9byslt8t
Epoch 0, Batch 0, train loss:8.419449806213379, Elapsed time for epoch : 0.013740233580271403
Epoch 0, Batch 10, train loss:3.493495225906372, Elapsed time for epoch : 0.13870654106140137
Epoch 0, Batch 20, train loss:3.828256130218506, Elapsed time for epoch : 0.26330766280492146
Epoch 0, Batch 30, train loss:3.880758047103882, Elapsed time for epoch : 0.38922667503356934
Epoch 0, Batch 40, train loss:2.939873456954956, Elapsed time for epoch : 0.5132927934328715
Epoch 0, Batch 50, train loss:2.818828821182251, Elapsed time for epoch : 0.6387971997261047
Epoch 0, Batch 60, train loss:2.6535091400146484, Elapsed time for epoch : 0.7621189077695211
Epoch 0, Batch 70, train loss:2.5204989910125732, Elapsed time for epoch : 0.8865435481071472
Epoch 0, Batch 80, train loss:2.5866663455963135, Elapsed time for epoch : 1.0096906344095866
Epoch 0, Batch 90, train loss:2.3998920917510986, Elapsed time for epoch : 1.1327397624651592
Epoch 0, Batch 100, train loss:1.9801827669143677, Elapsed time for epoch : 1.2574327786763508
Epoch 0, Batch 110, train loss:1.8266518115997314, Elapsed time for epoch : 1.3814016779263814
Batch 0, val loss:5.101893901824951
Batch 10, val loss:5.321146488189697
Batch 20, val loss:3.7825424671173096
Batch 30, val loss:5.774630546569824
Epoch 0, Train Loss:3.1996469694635143, Val loss:3.691681186358134
Epoch 1, Batch 0, train loss:2.0064072608947754, Elapsed time for epoch : 0.011634854475657146
Epoch 1, Batch 10, train loss:1.883485198020935, Elapsed time for epoch : 0.12824105421702067
Epoch 1, Batch 20, train loss:1.5403168201446533, Elapsed time for epoch : 0.24470152854919433
Epoch 1, Batch 30, train loss:1.6122928857803345, Elapsed time for epoch : 0.3610575318336487
Epoch 1, Batch 40, train loss:1.638841986656189, Elapsed time for epoch : 0.4774489482243856
Epoch 1, Batch 50, train loss:1.5332133769989014, Elapsed time for epoch : 0.5941001097361247
Epoch 1, Batch 60, train loss:1.4844045639038086, Elapsed time for epoch : 0.71121693054835
Epoch 1, Batch 70, train loss:1.466153860092163, Elapsed time for epoch : 0.8282469789187114
Epoch 1, Batch 80, train loss:1.3535823822021484, Elapsed time for epoch : 0.9449764529863993
Epoch 1, Batch 90, train loss:1.2530028820037842, Elapsed time for epoch : 1.0613003492355346
Epoch 1, Batch 100, train loss:0.9387345910072327, Elapsed time for epoch : 1.1778927087783813
Epoch 1, Batch 110, train loss:1.096382975578308, Elapsed time for epoch : 1.294586149851481
Batch 0, val loss:4.5310564041137695
Batch 10, val loss:2.2095932960510254
Batch 20, val loss:4.145381927490234
Batch 30, val loss:4.325518608093262
Epoch 1, Train Loss:1.4314123703085857, Val loss:3.911394132508172
Epoch 2, Batch 0, train loss:1.1224453449249268, Elapsed time for epoch : 0.01166229248046875
Epoch 2, Batch 10, train loss:1.0865859985351562, Elapsed time for epoch : 0.12821016709009805
Epoch 2, Batch 20, train loss:1.1935430765151978, Elapsed time for epoch : 0.24482680559158326
Epoch 2, Batch 30, train loss:1.1900898218154907, Elapsed time for epoch : 0.36128397782643634
Epoch 2, Batch 40, train loss:1.1322451829910278, Elapsed time for epoch : 0.4778907338778178
Epoch 2, Batch 50, train loss:1.2344053983688354, Elapsed time for epoch : 0.5945973912874858
Epoch 2, Batch 60, train loss:1.129814624786377, Elapsed time for epoch : 0.7113642970720927
Epoch 2, Batch 70, train loss:0.6422963738441467, Elapsed time for epoch : 0.8279781977335612
Epoch 2, Batch 80, train loss:1.081626296043396, Elapsed time for epoch : 0.9447046041488647
Epoch 2, Batch 90, train loss:1.0678811073303223, Elapsed time for epoch : 1.0610953172047932
Epoch 2, Batch 100, train loss:1.0798594951629639, Elapsed time for epoch : 1.177612566947937
Epoch 2, Batch 110, train loss:0.8969236612319946, Elapsed time for epoch : 1.2943394343058268
Batch 0, val loss:2.1620330810546875
Batch 10, val loss:2.2868030071258545
Batch 20, val loss:2.4357693195343018
Batch 30, val loss:3.5917718410491943
Epoch 2, Train Loss:1.0166485783846482, Val loss:4.019706098569764
Epoch 3, Batch 0, train loss:0.958504855632782, Elapsed time for epoch : 0.011675651868184407
Epoch 3, Batch 10, train loss:1.024553894996643, Elapsed time for epoch : 0.1284846305847168
Epoch 3, Batch 20, train loss:0.9811776876449585, Elapsed time for epoch : 0.24509472846984864
Epoch 3, Batch 30, train loss:0.9428118467330933, Elapsed time for epoch : 0.3619363784790039
Epoch 3, Batch 40, train loss:0.8780783414840698, Elapsed time for epoch : 0.4787006497383118
Epoch 3, Batch 50, train loss:0.5216779112815857, Elapsed time for epoch : 0.5951020042101542
Epoch 3, Batch 60, train loss:0.8914651870727539, Elapsed time for epoch : 0.7115409890810649
Epoch 3, Batch 70, train loss:0.9181913137435913, Elapsed time for epoch : 0.8285140077273051
Epoch 3, Batch 80, train loss:0.8114147186279297, Elapsed time for epoch : 0.9450701793034871
Epoch 3, Batch 90, train loss:0.7811036109924316, Elapsed time for epoch : 1.0619441866874695
Epoch 3, Batch 100, train loss:0.3788732886314392, Elapsed time for epoch : 1.1784347653388978
Epoch 3, Batch 110, train loss:0.8263855576515198, Elapsed time for epoch : 1.2951228976249696
Batch 0, val loss:5.598031044006348
Batch 10, val loss:9.341019630432129
Batch 20, val loss:13.789640426635742
Batch 30, val loss:3.6481270790100098
Epoch 3, Train Loss:0.8162530702093374, Val loss:7.393756601545546
Epoch 4, Batch 0, train loss:1.103658676147461, Elapsed time for epoch : 0.011693708101908366
Epoch 4, Batch 10, train loss:0.3839494287967682, Elapsed time for epoch : 0.12835468451182047
Epoch 4, Batch 20, train loss:0.34669479727745056, Elapsed time for epoch : 0.24508896668752034
Epoch 4, Batch 30, train loss:0.7389479875564575, Elapsed time for epoch : 0.361967945098877
Epoch 4, Batch 40, train loss:0.7051209211349487, Elapsed time for epoch : 0.4785331726074219
Epoch 4, Batch 50, train loss:0.6566702723503113, Elapsed time for epoch : 0.5949380755424499
Epoch 4, Batch 60, train loss:0.6828315854072571, Elapsed time for epoch : 0.7116011500358581
Epoch 4, Batch 70, train loss:0.6731645464897156, Elapsed time for epoch : 0.8283461213111878
Epoch 4, Batch 80, train loss:0.319559782743454, Elapsed time for epoch : 0.9447099645932515
Epoch 4, Batch 90, train loss:0.8586187362670898, Elapsed time for epoch : 1.0613782445589701
Epoch 4, Batch 100, train loss:0.6514291763305664, Elapsed time for epoch : 1.178428840637207
Epoch 4, Batch 110, train loss:0.6082725524902344, Elapsed time for epoch : 1.2958474556605022
Batch 0, val loss:4.1254377365112305
Batch 10, val loss:2.405317783355713
Batch 20, val loss:3.3379099369049072
Batch 30, val loss:2.2862589359283447
Epoch 4, Train Loss:0.6137877040583154, Val loss:4.357947644260195
wandb: - 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61379
wandb:         Val Loss 4.35795
wandb:      train_batch 110
wandb: train_batch_loss 0.60827
wandb:        val_batch 30
wandb:   val_batch_loss 2.28626
wandb: 
wandb: üöÄ View run magic-morning-390 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9byslt8t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_052236-9byslt8t/logs
Seed completed execution! 1 0.9_4
------------------------------------------------------------------
Running for seed 42 of experiment 0.9_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_053030-m6xuz6sp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-aardvark-392
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m6xuz6sp
Epoch 0, Batch 0, train loss:8.419449806213379, Elapsed time for epoch : 0.0134842316309611
Epoch 0, Batch 10, train loss:3.493495225906372, Elapsed time for epoch : 0.12989732027053832
Epoch 0, Batch 20, train loss:3.828256130218506, Elapsed time for epoch : 0.2464548667271932
Epoch 0, Batch 30, train loss:3.880758047103882, Elapsed time for epoch : 0.36235780318578087
Epoch 0, Batch 40, train loss:2.939873456954956, Elapsed time for epoch : 0.47922720114390055
Epoch 0, Batch 50, train loss:2.818828821182251, Elapsed time for epoch : 0.5954571962356567
Epoch 0, Batch 60, train loss:2.6535091400146484, Elapsed time for epoch : 0.7116039951642354
Epoch 0, Batch 70, train loss:2.5204989910125732, Elapsed time for epoch : 0.8277526497840881
Epoch 0, Batch 80, train loss:2.5866663455963135, Elapsed time for epoch : 0.9443777839342753
Epoch 0, Batch 90, train loss:2.3998920917510986, Elapsed time for epoch : 1.0608122626940408
Epoch 0, Batch 100, train loss:1.9801827669143677, Elapsed time for epoch : 1.177437456448873
Epoch 0, Batch 110, train loss:1.8266518115997314, Elapsed time for epoch : 1.2939868728319803
Batch 0, val loss:5.101893901824951
Batch 10, val loss:5.321146488189697
Batch 20, val loss:3.7825424671173096
Batch 30, val loss:5.774630546569824
Epoch 0, Train Loss:3.1996469694635143, Val loss:3.691681186358134
Epoch 1, Batch 0, train loss:2.0064072608947754, Elapsed time for epoch : 0.011760397752126058
Epoch 1, Batch 10, train loss:1.883485198020935, Elapsed time for epoch : 0.12806864976882934
Epoch 1, Batch 20, train loss:1.5403168201446533, Elapsed time for epoch : 0.24489298661549885
Epoch 1, Batch 30, train loss:1.6122928857803345, Elapsed time for epoch : 0.3611976106961568
Epoch 1, Batch 40, train loss:1.638841986656189, Elapsed time for epoch : 0.4782059907913208
Epoch 1, Batch 50, train loss:1.5332133769989014, Elapsed time for epoch : 0.594495956103007
Epoch 1, Batch 60, train loss:1.4844045639038086, Elapsed time for epoch : 0.7112375736236572
Epoch 1, Batch 70, train loss:1.466153860092163, Elapsed time for epoch : 0.8278734723726908
Epoch 1, Batch 80, train loss:1.3535823822021484, Elapsed time for epoch : 0.94410400390625
Epoch 1, Batch 90, train loss:1.2530028820037842, Elapsed time for epoch : 1.0607144435246785
Epoch 1, Batch 100, train loss:0.9387345910072327, Elapsed time for epoch : 1.1772863149642945
Epoch 1, Batch 110, train loss:1.096382975578308, Elapsed time for epoch : 1.2936853806177775
Batch 0, val loss:4.5310564041137695
Batch 10, val loss:2.2095932960510254
Batch 20, val loss:4.145381927490234
Batch 30, val loss:4.325518608093262
Epoch 1, Train Loss:1.4314123703085857, Val loss:3.911394132508172
Epoch 2, Batch 0, train loss:1.1224453449249268, Elapsed time for epoch : 0.011690310637156169
Epoch 2, Batch 10, train loss:1.0865859985351562, Elapsed time for epoch : 0.12855008045832317
Epoch 2, Batch 20, train loss:1.1935430765151978, Elapsed time for epoch : 0.24520092805226643
Epoch 2, Batch 30, train loss:1.1900898218154907, Elapsed time for epoch : 0.361936100323995
Epoch 2, Batch 40, train loss:1.1322451829910278, Elapsed time for epoch : 0.478471044699351
Epoch 2, Batch 50, train loss:1.2344053983688354, Elapsed time for epoch : 0.5952411770820618
Epoch 2, Batch 60, train loss:1.129814624786377, Elapsed time for epoch : 0.7119429628054301
Epoch 2, Batch 70, train loss:0.6422963738441467, Elapsed time for epoch : 0.8284543712933858
Epoch 2, Batch 80, train loss:1.081626296043396, Elapsed time for epoch : 0.9455025871594747
Epoch 2, Batch 90, train loss:1.0678811073303223, Elapsed time for epoch : 1.0616612752278647
Epoch 2, Batch 100, train loss:1.0798594951629639, Elapsed time for epoch : 1.1784024079640707
Epoch 2, Batch 110, train loss:0.8969236612319946, Elapsed time for epoch : 1.295658242702484
Batch 0, val loss:2.1620330810546875
Batch 10, val loss:2.2868030071258545
Batch 20, val loss:2.4357693195343018
Batch 30, val loss:3.5917718410491943
Epoch 2, Train Loss:1.0166485783846482, Val loss:4.019706098569764
Epoch 3, Batch 0, train loss:0.958504855632782, Elapsed time for epoch : 0.011668272813161214
Epoch 3, Batch 10, train loss:1.024553894996643, Elapsed time for epoch : 0.1284601132074992
Epoch 3, Batch 20, train loss:0.9811776876449585, Elapsed time for epoch : 0.24520211617151896
Epoch 3, Batch 30, train loss:0.9428118467330933, Elapsed time for epoch : 0.3616662859916687
Epoch 3, Batch 40, train loss:0.8780783414840698, Elapsed time for epoch : 0.47840078274408976
Epoch 3, Batch 50, train loss:0.5216779112815857, Elapsed time for epoch : 0.5947748939196269
Epoch 3, Batch 60, train loss:0.8914651870727539, Elapsed time for epoch : 0.711411448319753
Epoch 3, Batch 70, train loss:0.9181913137435913, Elapsed time for epoch : 0.828076155980428
Epoch 3, Batch 80, train loss:0.8114147186279297, Elapsed time for epoch : 0.9449567516644796
Epoch 3, Batch 90, train loss:0.7811036109924316, Elapsed time for epoch : 1.0622615694999695
Epoch 3, Batch 100, train loss:0.3788732886314392, Elapsed time for epoch : 1.1787795980771383
Epoch 3, Batch 110, train loss:0.8263855576515198, Elapsed time for epoch : 1.2952312191327413
Batch 0, val loss:5.598031044006348
Batch 10, val loss:9.341019630432129
Batch 20, val loss:13.789640426635742
Batch 30, val loss:3.6481270790100098
Epoch 3, Train Loss:0.8162530702093374, Val loss:7.393756601545546
Epoch 4, Batch 0, train loss:1.103658676147461, Elapsed time for epoch : 0.011684254805246989
Epoch 4, Batch 10, train loss:0.3839494287967682, Elapsed time for epoch : 0.1283536434173584
Epoch 4, Batch 20, train loss:0.34669479727745056, Elapsed time for epoch : 0.2448766311009725
Epoch 4, Batch 30, train loss:0.7389479875564575, Elapsed time for epoch : 0.3613994558652242
Epoch 4, Batch 40, train loss:0.7051209211349487, Elapsed time for epoch : 0.47775110801060994
Epoch 4, Batch 50, train loss:0.6566702723503113, Elapsed time for epoch : 0.5944578250249227
Epoch 4, Batch 60, train loss:0.6828315854072571, Elapsed time for epoch : 0.7110235929489136
Epoch 4, Batch 70, train loss:0.6731645464897156, Elapsed time for epoch : 0.8274290243784587
Epoch 4, Batch 80, train loss:0.319559782743454, Elapsed time for epoch : 0.9441688497861226
Epoch 4, Batch 90, train loss:0.8586187362670898, Elapsed time for epoch : 1.0606419603029886
Epoch 4, Batch 100, train loss:0.6514291763305664, Elapsed time for epoch : 1.177521550655365
Epoch 4, Batch 110, train loss:0.6082725524902344, Elapsed time for epoch : 1.2938815752665203
Batch 0, val loss:4.1254377365112305
Batch 10, val loss:2.405317783355713
Batch 20, val loss:3.3379099369049072
Batch 30, val loss:2.2862589359283447
Epoch 4, Train Loss:0.6137877040583154, Val loss:4.357947644260195
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61379
wandb:         Val Loss 4.35795
wandb:      train_batch 110
wandb: train_batch_loss 0.60827
wandb:        val_batch 30
wandb:   val_batch_loss 2.28626
wandb: 
wandb: üöÄ View run lyric-aardvark-392 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m6xuz6sp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_053030-m6xuz6sp/logs
Seed completed execution! 42 0.9_4
------------------------------------------------------------------
Running for seed 89 of experiment 0.9_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_053818-e0yc7mt9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-cloud-394
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e0yc7mt9
Epoch 0, Batch 0, train loss:8.419449806213379, Elapsed time for epoch : 0.013321423530578613
Epoch 0, Batch 10, train loss:3.493495225906372, Elapsed time for epoch : 0.1293898900349935
Epoch 0, Batch 20, train loss:3.828256130218506, Elapsed time for epoch : 0.24639249642690023
Epoch 0, Batch 30, train loss:3.880758047103882, Elapsed time for epoch : 0.3636122465133667
Epoch 0, Batch 40, train loss:2.939873456954956, Elapsed time for epoch : 0.48033443291982014
Epoch 0, Batch 50, train loss:2.818828821182251, Elapsed time for epoch : 0.5965393304824829
Epoch 0, Batch 60, train loss:2.6535091400146484, Elapsed time for epoch : 0.7132185498873392
Epoch 0, Batch 70, train loss:2.5204989910125732, Elapsed time for epoch : 0.829639732837677
Epoch 0, Batch 80, train loss:2.5866663455963135, Elapsed time for epoch : 0.9468016664187113
Epoch 0, Batch 90, train loss:2.3998920917510986, Elapsed time for epoch : 1.0639461437861124
Epoch 0, Batch 100, train loss:1.9801827669143677, Elapsed time for epoch : 1.1807275533676147
Epoch 0, Batch 110, train loss:1.8266518115997314, Elapsed time for epoch : 1.2979898730913797
Batch 0, val loss:5.101893901824951
Batch 10, val loss:5.321146488189697
Batch 20, val loss:3.7825424671173096
Batch 30, val loss:5.774630546569824
Epoch 0, Train Loss:3.1996469694635143, Val loss:3.691681186358134
Epoch 1, Batch 0, train loss:2.0064072608947754, Elapsed time for epoch : 0.011688049634297688
Epoch 1, Batch 10, train loss:1.883485198020935, Elapsed time for epoch : 0.1281238913536072
Epoch 1, Batch 20, train loss:1.5403168201446533, Elapsed time for epoch : 0.24492551485697428
Epoch 1, Batch 30, train loss:1.6122928857803345, Elapsed time for epoch : 0.36154301166534425
Epoch 1, Batch 40, train loss:1.638841986656189, Elapsed time for epoch : 0.4779663642247518
Epoch 1, Batch 50, train loss:1.5332133769989014, Elapsed time for epoch : 0.594978924592336
Epoch 1, Batch 60, train loss:1.4844045639038086, Elapsed time for epoch : 0.7122529308001201
Epoch 1, Batch 70, train loss:1.466153860092163, Elapsed time for epoch : 0.8295904954274496
Epoch 1, Batch 80, train loss:1.3535823822021484, Elapsed time for epoch : 0.9464176376660665
Epoch 1, Batch 90, train loss:1.2530028820037842, Elapsed time for epoch : 1.0631511489550272
Epoch 1, Batch 100, train loss:0.9387345910072327, Elapsed time for epoch : 1.1797507564226786
Epoch 1, Batch 110, train loss:1.096382975578308, Elapsed time for epoch : 1.2963780283927917
Batch 0, val loss:4.5310564041137695
Batch 10, val loss:2.2095932960510254
Batch 20, val loss:4.145381927490234
Batch 30, val loss:4.325518608093262
Epoch 1, Train Loss:1.4314123703085857, Val loss:3.911394132508172
Epoch 2, Batch 0, train loss:1.1224453449249268, Elapsed time for epoch : 0.011725258827209473
Epoch 2, Batch 10, train loss:1.0865859985351562, Elapsed time for epoch : 0.12840236028035482
Epoch 2, Batch 20, train loss:1.1935430765151978, Elapsed time for epoch : 0.24520055055618287
Epoch 2, Batch 30, train loss:1.1900898218154907, Elapsed time for epoch : 0.36142903566360474
Epoch 2, Batch 40, train loss:1.1322451829910278, Elapsed time for epoch : 0.4778772234916687
Epoch 2, Batch 50, train loss:1.2344053983688354, Elapsed time for epoch : 0.5941025376319885
Epoch 2, Batch 60, train loss:1.129814624786377, Elapsed time for epoch : 0.7108339190483093
Epoch 2, Batch 70, train loss:0.6422963738441467, Elapsed time for epoch : 0.8272220134735108
Epoch 2, Batch 80, train loss:1.081626296043396, Elapsed time for epoch : 0.9439651091893514
Epoch 2, Batch 90, train loss:1.0678811073303223, Elapsed time for epoch : 1.060780656337738
Epoch 2, Batch 100, train loss:1.0798594951629639, Elapsed time for epoch : 1.1772978107134502
Epoch 2, Batch 110, train loss:0.8969236612319946, Elapsed time for epoch : 1.2940303087234497
Batch 0, val loss:2.1620330810546875
Batch 10, val loss:2.2868030071258545
Batch 20, val loss:2.4357693195343018
Batch 30, val loss:3.5917718410491943
Epoch 2, Train Loss:1.0166485783846482, Val loss:4.019706098569764
Epoch 3, Batch 0, train loss:0.958504855632782, Elapsed time for epoch : 0.011694773038228353
Epoch 3, Batch 10, train loss:1.024553894996643, Elapsed time for epoch : 0.1281938870747884
Epoch 3, Batch 20, train loss:0.9811776876449585, Elapsed time for epoch : 0.2445949673652649
Epoch 3, Batch 30, train loss:0.9428118467330933, Elapsed time for epoch : 0.3612005392710368
Epoch 3, Batch 40, train loss:0.8780783414840698, Elapsed time for epoch : 0.4778136412302653
Epoch 3, Batch 50, train loss:0.5216779112815857, Elapsed time for epoch : 0.594007651011149
Epoch 3, Batch 60, train loss:0.8914651870727539, Elapsed time for epoch : 0.7105859279632568
Epoch 3, Batch 70, train loss:0.9181913137435913, Elapsed time for epoch : 0.8275739113489787
Epoch 3, Batch 80, train loss:0.8114147186279297, Elapsed time for epoch : 0.9441503167152405
Epoch 3, Batch 90, train loss:0.7811036109924316, Elapsed time for epoch : 1.0608295281728108
Epoch 3, Batch 100, train loss:0.3788732886314392, Elapsed time for epoch : 1.177441652615865
Epoch 3, Batch 110, train loss:0.8263855576515198, Elapsed time for epoch : 1.2941587408383688
Batch 0, val loss:5.598031044006348
Batch 10, val loss:9.341019630432129
Batch 20, val loss:13.789640426635742
Batch 30, val loss:3.6481270790100098
Epoch 3, Train Loss:0.8162530702093374, Val loss:7.393756601545546
Epoch 4, Batch 0, train loss:1.103658676147461, Elapsed time for epoch : 0.011819259325663248
Epoch 4, Batch 10, train loss:0.3839494287967682, Elapsed time for epoch : 0.12838699420293173
Epoch 4, Batch 20, train loss:0.34669479727745056, Elapsed time for epoch : 0.24496299425760906
Epoch 4, Batch 30, train loss:0.7389479875564575, Elapsed time for epoch : 0.36145359675089517
Epoch 4, Batch 40, train loss:0.7051209211349487, Elapsed time for epoch : 0.47786290645599366
Epoch 4, Batch 50, train loss:0.6566702723503113, Elapsed time for epoch : 0.5943291266759236
Epoch 4, Batch 60, train loss:0.6828315854072571, Elapsed time for epoch : 0.7107906540234884
Epoch 4, Batch 70, train loss:0.6731645464897156, Elapsed time for epoch : 0.8273005803426107
Epoch 4, Batch 80, train loss:0.319559782743454, Elapsed time for epoch : 0.9438550233840942
Epoch 4, Batch 90, train loss:0.8586187362670898, Elapsed time for epoch : 1.0604103485743204
Epoch 4, Batch 100, train loss:0.6514291763305664, Elapsed time for epoch : 1.1768170475959778
Epoch 4, Batch 110, train loss:0.6082725524902344, Elapsed time for epoch : 1.2935421387354533
Batch 0, val loss:4.1254377365112305
Batch 10, val loss:2.405317783355713
Batch 20, val loss:3.3379099369049072
Batch 30, val loss:2.2862589359283447
Epoch 4, Train Loss:0.6137877040583154, Val loss:4.357947644260195
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61379
wandb:         Val Loss 4.35795
wandb:      train_batch 110
wandb: train_batch_loss 0.60827
wandb:        val_batch 30
wandb:   val_batch_loss 2.28626
wandb: 
wandb: üöÄ View run efficient-cloud-394 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e0yc7mt9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_053818-e0yc7mt9/logs
Seed completed execution! 89 0.9_4
------------------------------------------------------------------
Running for seed 23 of experiment 0.9_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_054606-j3y2y4uv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-lion-396
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/j3y2y4uv
Epoch 0, Batch 0, train loss:8.419449806213379, Elapsed time for epoch : 0.01279604434967041
Epoch 0, Batch 10, train loss:3.493495225906372, Elapsed time for epoch : 0.1291706164677938
Epoch 0, Batch 20, train loss:3.828256130218506, Elapsed time for epoch : 0.2455539385477702
Epoch 0, Batch 30, train loss:3.880758047103882, Elapsed time for epoch : 0.361555282274882
Epoch 0, Batch 40, train loss:2.939873456954956, Elapsed time for epoch : 0.4778239051500956
Epoch 0, Batch 50, train loss:2.818828821182251, Elapsed time for epoch : 0.5944784204165141
Epoch 0, Batch 60, train loss:2.6535091400146484, Elapsed time for epoch : 0.7103636582692464
Epoch 0, Batch 70, train loss:2.5204989910125732, Elapsed time for epoch : 0.8269308169682821
Epoch 0, Batch 80, train loss:2.5866663455963135, Elapsed time for epoch : 0.9437118013699849
Epoch 0, Batch 90, train loss:2.3998920917510986, Elapsed time for epoch : 1.0602306763331095
Epoch 0, Batch 100, train loss:1.9801827669143677, Elapsed time for epoch : 1.1765690565109252
Epoch 0, Batch 110, train loss:1.8266518115997314, Elapsed time for epoch : 1.2933146754900615
Batch 0, val loss:5.101893901824951
Batch 10, val loss:5.321146488189697
Batch 20, val loss:3.7825424671173096
Batch 30, val loss:5.774630546569824
Epoch 0, Train Loss:3.1996469694635143, Val loss:3.691681186358134
Epoch 1, Batch 0, train loss:2.0064072608947754, Elapsed time for epoch : 0.011657349268595378
Epoch 1, Batch 10, train loss:1.883485198020935, Elapsed time for epoch : 0.12784422238667806
Epoch 1, Batch 20, train loss:1.5403168201446533, Elapsed time for epoch : 0.2441707134246826
Epoch 1, Batch 30, train loss:1.6122928857803345, Elapsed time for epoch : 0.3605468988418579
Epoch 1, Batch 40, train loss:1.638841986656189, Elapsed time for epoch : 0.4774123152097066
Epoch 1, Batch 50, train loss:1.5332133769989014, Elapsed time for epoch : 0.5937116424242656
Epoch 1, Batch 60, train loss:1.4844045639038086, Elapsed time for epoch : 0.7102319439252217
Epoch 1, Batch 70, train loss:1.466153860092163, Elapsed time for epoch : 0.8268228689829509
Epoch 1, Batch 80, train loss:1.3535823822021484, Elapsed time for epoch : 0.9431315024693807
Epoch 1, Batch 90, train loss:1.2530028820037842, Elapsed time for epoch : 1.0596753716468812
Epoch 1, Batch 100, train loss:0.9387345910072327, Elapsed time for epoch : 1.1764497399330138
Epoch 1, Batch 110, train loss:1.096382975578308, Elapsed time for epoch : 1.2930991172790527
Batch 0, val loss:4.5310564041137695
Batch 10, val loss:2.2095932960510254
Batch 20, val loss:4.145381927490234
Batch 30, val loss:4.325518608093262
Epoch 1, Train Loss:1.4314123703085857, Val loss:3.911394132508172
Epoch 2, Batch 0, train loss:1.1224453449249268, Elapsed time for epoch : 0.01175376574198405
Epoch 2, Batch 10, train loss:1.0865859985351562, Elapsed time for epoch : 0.12802735169728596
Epoch 2, Batch 20, train loss:1.1935430765151978, Elapsed time for epoch : 0.2446386178334554
Epoch 2, Batch 30, train loss:1.1900898218154907, Elapsed time for epoch : 0.3611095110575358
Epoch 2, Batch 40, train loss:1.1322451829910278, Elapsed time for epoch : 0.477402917544047
Epoch 2, Batch 50, train loss:1.2344053983688354, Elapsed time for epoch : 0.5939067800839742
Epoch 2, Batch 60, train loss:1.129814624786377, Elapsed time for epoch : 0.710431973139445
Epoch 2, Batch 70, train loss:0.6422963738441467, Elapsed time for epoch : 0.8269795417785645
Epoch 2, Batch 80, train loss:1.081626296043396, Elapsed time for epoch : 0.9434840480486552
Epoch 2, Batch 90, train loss:1.0678811073303223, Elapsed time for epoch : 1.0602772037188213
Epoch 2, Batch 100, train loss:1.0798594951629639, Elapsed time for epoch : 1.176708161830902
Epoch 2, Batch 110, train loss:0.8969236612319946, Elapsed time for epoch : 1.2931925415992738
Batch 0, val loss:2.1620330810546875
Batch 10, val loss:2.2868030071258545
Batch 20, val loss:2.4357693195343018
Batch 30, val loss:3.5917718410491943
Epoch 2, Train Loss:1.0166485783846482, Val loss:4.019706098569764
Epoch 3, Batch 0, train loss:0.958504855632782, Elapsed time for epoch : 0.011725127696990967
Epoch 3, Batch 10, train loss:1.024553894996643, Elapsed time for epoch : 0.1280981183052063
Epoch 3, Batch 20, train loss:0.9811776876449585, Elapsed time for epoch : 0.244538418451945
Epoch 3, Batch 30, train loss:0.9428118467330933, Elapsed time for epoch : 0.3608526627222697
Epoch 3, Batch 40, train loss:0.8780783414840698, Elapsed time for epoch : 0.4772713700930277
Epoch 3, Batch 50, train loss:0.5216779112815857, Elapsed time for epoch : 0.5938125332196553
Epoch 3, Batch 60, train loss:0.8914651870727539, Elapsed time for epoch : 0.7104604363441467
Epoch 3, Batch 70, train loss:0.9181913137435913, Elapsed time for epoch : 0.8269380331039429
Epoch 3, Batch 80, train loss:0.8114147186279297, Elapsed time for epoch : 0.943668254216512
Epoch 3, Batch 90, train loss:0.7811036109924316, Elapsed time for epoch : 1.0602555433909098
Epoch 3, Batch 100, train loss:0.3788732886314392, Elapsed time for epoch : 1.1765490134557088
Epoch 3, Batch 110, train loss:0.8263855576515198, Elapsed time for epoch : 1.2949092984199524
Batch 0, val loss:5.598031044006348
Batch 10, val loss:9.341019630432129
Batch 20, val loss:13.789640426635742
Batch 30, val loss:3.6481270790100098
Epoch 3, Train Loss:0.8162530702093374, Val loss:7.393756601545546
Epoch 4, Batch 0, train loss:1.103658676147461, Elapsed time for epoch : 0.011809996763865153
Epoch 4, Batch 10, train loss:0.3839494287967682, Elapsed time for epoch : 0.13025713364283245
Epoch 4, Batch 20, train loss:0.34669479727745056, Elapsed time for epoch : 0.24771938721338907
Epoch 4, Batch 30, train loss:0.7389479875564575, Elapsed time for epoch : 0.3665889342625936
Epoch 4, Batch 40, train loss:0.7051209211349487, Elapsed time for epoch : 0.48456385135650637
Epoch 4, Batch 50, train loss:0.6566702723503113, Elapsed time for epoch : 0.6030308485031128
Epoch 4, Batch 60, train loss:0.6828315854072571, Elapsed time for epoch : 0.7207258780797322
Epoch 4, Batch 70, train loss:0.6731645464897156, Elapsed time for epoch : 0.838800044854482
Epoch 4, Batch 80, train loss:0.319559782743454, Elapsed time for epoch : 0.9561362147331238
Epoch 4, Batch 90, train loss:0.8586187362670898, Elapsed time for epoch : 1.0739213228225708
Epoch 4, Batch 100, train loss:0.6514291763305664, Elapsed time for epoch : 1.1918673555056254
Epoch 4, Batch 110, train loss:0.6082725524902344, Elapsed time for epoch : 1.3092281858126322
Batch 0, val loss:4.1254377365112305
Batch 10, val loss:2.405317783355713
Batch 20, val loss:3.3379099369049072
Batch 30, val loss:2.2862589359283447
Epoch 4, Train Loss:0.6137877040583154, Val loss:4.357947644260195
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61379
wandb:         Val Loss 4.35795
wandb:      train_batch 110
wandb: train_batch_loss 0.60827
wandb:        val_batch 30
wandb:   val_batch_loss 2.28626
wandb: 
wandb: üöÄ View run balmy-lion-396 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/j3y2y4uv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_054606-j3y2y4uv/logs
Seed completed execution! 23 0.9_4
------------------------------------------------------------------
Running for seed 113 of experiment 0.9_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_055355-vkcot9kw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-star-398
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/vkcot9kw
Epoch 0, Batch 0, train loss:8.419449806213379, Elapsed time for epoch : 0.013364112377166748
Epoch 0, Batch 10, train loss:3.493495225906372, Elapsed time for epoch : 0.12926464478174846
Epoch 0, Batch 20, train loss:3.828256130218506, Elapsed time for epoch : 0.24539068539937336
Epoch 0, Batch 30, train loss:3.880758047103882, Elapsed time for epoch : 0.3614921768506368
Epoch 0, Batch 40, train loss:2.939873456954956, Elapsed time for epoch : 0.47840654055277504
Epoch 0, Batch 50, train loss:2.818828821182251, Elapsed time for epoch : 0.5945589025815328
Epoch 0, Batch 60, train loss:2.6535091400146484, Elapsed time for epoch : 0.7108928561210632
Epoch 0, Batch 70, train loss:2.5204989910125732, Elapsed time for epoch : 0.8270068724950155
Epoch 0, Batch 80, train loss:2.5866663455963135, Elapsed time for epoch : 0.9432640592257182
Epoch 0, Batch 90, train loss:2.3998920917510986, Elapsed time for epoch : 1.0595063130060831
Epoch 0, Batch 100, train loss:1.9801827669143677, Elapsed time for epoch : 1.1761699557304381
Epoch 0, Batch 110, train loss:1.8266518115997314, Elapsed time for epoch : 1.2924695134162902
Batch 0, val loss:5.101893901824951
Batch 10, val loss:5.321146488189697
Batch 20, val loss:3.7825424671173096
Batch 30, val loss:5.774630546569824
Epoch 0, Train Loss:3.1996469694635143, Val loss:3.691681186358134
Epoch 1, Batch 0, train loss:2.0064072608947754, Elapsed time for epoch : 0.01168283224105835
Epoch 1, Batch 10, train loss:1.883485198020935, Elapsed time for epoch : 0.12825543483098348
Epoch 1, Batch 20, train loss:1.5403168201446533, Elapsed time for epoch : 0.2450616955757141
Epoch 1, Batch 30, train loss:1.6122928857803345, Elapsed time for epoch : 0.36121004819869995
Epoch 1, Batch 40, train loss:1.638841986656189, Elapsed time for epoch : 0.4777507503827413
Epoch 1, Batch 50, train loss:1.5332133769989014, Elapsed time for epoch : 0.5941545128822326
Epoch 1, Batch 60, train loss:1.4844045639038086, Elapsed time for epoch : 0.7108997662862142
Epoch 1, Batch 70, train loss:1.466153860092163, Elapsed time for epoch : 0.8272411584854126
Epoch 1, Batch 80, train loss:1.3535823822021484, Elapsed time for epoch : 0.9436874787012736
Epoch 1, Batch 90, train loss:1.2530028820037842, Elapsed time for epoch : 1.060382854938507
Epoch 1, Batch 100, train loss:0.9387345910072327, Elapsed time for epoch : 1.1767966826756795
Epoch 1, Batch 110, train loss:1.096382975578308, Elapsed time for epoch : 1.2937594850858052
Batch 0, val loss:4.5310564041137695
Batch 10, val loss:2.2095932960510254
Batch 20, val loss:4.145381927490234
Batch 30, val loss:4.325518608093262
Epoch 1, Train Loss:1.4314123703085857, Val loss:3.911394132508172
Epoch 2, Batch 0, train loss:1.1224453449249268, Elapsed time for epoch : 0.011678191026051839
Epoch 2, Batch 10, train loss:1.0865859985351562, Elapsed time for epoch : 0.1279348889986674
Epoch 2, Batch 20, train loss:1.1935430765151978, Elapsed time for epoch : 0.24451472759246826
Epoch 2, Batch 30, train loss:1.1900898218154907, Elapsed time for epoch : 0.3611088792483012
Epoch 2, Batch 40, train loss:1.1322451829910278, Elapsed time for epoch : 0.4781556169191996
Epoch 2, Batch 50, train loss:1.2344053983688354, Elapsed time for epoch : 0.5946762204170227
Epoch 2, Batch 60, train loss:1.129814624786377, Elapsed time for epoch : 0.711016043027242
Epoch 2, Batch 70, train loss:0.6422963738441467, Elapsed time for epoch : 0.8276162266731262
Epoch 2, Batch 80, train loss:1.081626296043396, Elapsed time for epoch : 0.9442456881205241
Epoch 2, Batch 90, train loss:1.0678811073303223, Elapsed time for epoch : 1.060693609714508
Epoch 2, Batch 100, train loss:1.0798594951629639, Elapsed time for epoch : 1.177481436729431
Epoch 2, Batch 110, train loss:0.8969236612319946, Elapsed time for epoch : 1.2942004243532816
Batch 0, val loss:2.1620330810546875
Batch 10, val loss:2.2868030071258545
Batch 20, val loss:2.4357693195343018
Batch 30, val loss:3.5917718410491943
Epoch 2, Train Loss:1.0166485783846482, Val loss:4.019706098569764
Epoch 3, Batch 0, train loss:0.958504855632782, Elapsed time for epoch : 0.011694657802581786
Epoch 3, Batch 10, train loss:1.024553894996643, Elapsed time for epoch : 0.12847208579381306
Epoch 3, Batch 20, train loss:0.9811776876449585, Elapsed time for epoch : 0.24484012126922608
Epoch 3, Batch 30, train loss:0.9428118467330933, Elapsed time for epoch : 0.36112586259841917
Epoch 3, Batch 40, train loss:0.8780783414840698, Elapsed time for epoch : 0.47812790075937905
Epoch 3, Batch 50, train loss:0.5216779112815857, Elapsed time for epoch : 0.5946178992589315
Epoch 3, Batch 60, train loss:0.8914651870727539, Elapsed time for epoch : 0.7110977252324422
Epoch 3, Batch 70, train loss:0.9181913137435913, Elapsed time for epoch : 0.827592686812083
Epoch 3, Batch 80, train loss:0.8114147186279297, Elapsed time for epoch : 0.9440791447957356
Epoch 3, Batch 90, train loss:0.7811036109924316, Elapsed time for epoch : 1.0608079552650451
Epoch 3, Batch 100, train loss:0.3788732886314392, Elapsed time for epoch : 1.1772937496503195
Epoch 3, Batch 110, train loss:0.8263855576515198, Elapsed time for epoch : 1.2944352547327678
Batch 0, val loss:5.598031044006348
Batch 10, val loss:9.341019630432129
Batch 20, val loss:13.789640426635742
Batch 30, val loss:3.6481270790100098
Epoch 3, Train Loss:0.8162530702093374, Val loss:7.393756601545546
Epoch 4, Batch 0, train loss:1.103658676147461, Elapsed time for epoch : 0.011727237701416015
Epoch 4, Batch 10, train loss:0.3839494287967682, Elapsed time for epoch : 0.12812538544336954
Epoch 4, Batch 20, train loss:0.34669479727745056, Elapsed time for epoch : 0.24472943147023518
Epoch 4, Batch 30, train loss:0.7389479875564575, Elapsed time for epoch : 0.3609905163447062
Epoch 4, Batch 40, train loss:0.7051209211349487, Elapsed time for epoch : 0.47796873648961385
Epoch 4, Batch 50, train loss:0.6566702723503113, Elapsed time for epoch : 0.5943862994511923
Epoch 4, Batch 60, train loss:0.6828315854072571, Elapsed time for epoch : 0.7107826550801595
Epoch 4, Batch 70, train loss:0.6731645464897156, Elapsed time for epoch : 0.8272086183230082
Epoch 4, Batch 80, train loss:0.319559782743454, Elapsed time for epoch : 0.9435481667518616
Epoch 4, Batch 90, train loss:0.8586187362670898, Elapsed time for epoch : 1.0601454337437948
Epoch 4, Batch 100, train loss:0.6514291763305664, Elapsed time for epoch : 1.176760689417521
Epoch 4, Batch 110, train loss:0.6082725524902344, Elapsed time for epoch : 1.2936507264773052
Batch 0, val loss:4.1254377365112305
Batch 10, val loss:2.405317783355713
Batch 20, val loss:3.3379099369049072
Batch 30, val loss:2.2862589359283447
Epoch 4, Train Loss:0.6137877040583154, Val loss:4.357947644260195
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61379
wandb:         Val Loss 4.35795
wandb:      train_batch 110
wandb: train_batch_loss 0.60827
wandb:        val_batch 30
wandb:   val_batch_loss 2.28626
wandb: 
wandb: üöÄ View run fallen-star-398 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/vkcot9kw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_055355-vkcot9kw/logs
Seed completed execution! 113 0.9_4
------------------------------------------------------------------
Experiment complete 0.9_4
==========================================================================
Running experiment for setting 0.9_5
==========================================================================
Running for seed 1 of experiment 0.9_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_060143-ic70mms1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-salad-400
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ic70mms1
Epoch 0, Batch 0, train loss:8.686380386352539, Elapsed time for epoch : 0.012183018525441488
Epoch 0, Batch 10, train loss:4.013209342956543, Elapsed time for epoch : 0.1285369038581848
Epoch 0, Batch 20, train loss:4.244500160217285, Elapsed time for epoch : 0.24490244388580323
Epoch 0, Batch 30, train loss:3.1841530799865723, Elapsed time for epoch : 0.3609723250071208
Epoch 0, Batch 40, train loss:3.2033283710479736, Elapsed time for epoch : 0.4774841547012329
Epoch 0, Batch 50, train loss:2.9221115112304688, Elapsed time for epoch : 0.5940617084503174
Epoch 0, Batch 60, train loss:2.808889627456665, Elapsed time for epoch : 0.7103456934293111
Epoch 0, Batch 70, train loss:2.570650339126587, Elapsed time for epoch : 0.8264828085899353
Epoch 0, Batch 80, train loss:2.838886022567749, Elapsed time for epoch : 0.9430169423421224
Epoch 0, Batch 90, train loss:2.509636402130127, Elapsed time for epoch : 1.0594634652137755
Epoch 0, Batch 100, train loss:2.164167881011963, Elapsed time for epoch : 1.176082714398702
Epoch 0, Batch 110, train loss:1.97530198097229, Elapsed time for epoch : 1.2926998575528463
Batch 0, val loss:5.011483192443848
Batch 10, val loss:4.004398345947266
Batch 20, val loss:3.645171642303467
Batch 30, val loss:6.98581600189209
Epoch 0, Train Loss:3.225737426591956, Val loss:4.008006897237566
Epoch 1, Batch 0, train loss:1.953001618385315, Elapsed time for epoch : 0.011681282520294189
Epoch 1, Batch 10, train loss:1.812613606452942, Elapsed time for epoch : 0.1281679590543111
Epoch 1, Batch 20, train loss:1.43244206905365, Elapsed time for epoch : 0.24476547241210939
Epoch 1, Batch 30, train loss:1.7130661010742188, Elapsed time for epoch : 0.3615678946177165
Epoch 1, Batch 40, train loss:1.5028754472732544, Elapsed time for epoch : 0.47783316373825074
Epoch 1, Batch 50, train loss:1.5146178007125854, Elapsed time for epoch : 0.5946890115737915
Epoch 1, Batch 60, train loss:1.6174894571304321, Elapsed time for epoch : 0.7114232261975606
Epoch 1, Batch 70, train loss:1.5220558643341064, Elapsed time for epoch : 0.8280533790588379
Epoch 1, Batch 80, train loss:1.413203477859497, Elapsed time for epoch : 0.9450963139533997
Epoch 1, Batch 90, train loss:1.1499443054199219, Elapsed time for epoch : 1.0612621426582336
Epoch 1, Batch 100, train loss:0.7688693404197693, Elapsed time for epoch : 1.1779137969017028
Epoch 1, Batch 110, train loss:0.9212377667427063, Elapsed time for epoch : 1.2942564209302267
Batch 0, val loss:2.045816659927368
Batch 10, val loss:3.2857162952423096
Batch 20, val loss:3.9255645275115967
Batch 30, val loss:2.634429693222046
Epoch 1, Train Loss:1.4284353178480398, Val loss:3.612700548436907
Epoch 2, Batch 0, train loss:1.057824730873108, Elapsed time for epoch : 0.011698285738627115
Epoch 2, Batch 10, train loss:0.8888640999794006, Elapsed time for epoch : 0.12847675879796347
Epoch 2, Batch 20, train loss:0.9739216566085815, Elapsed time for epoch : 0.24567917585372925
Epoch 2, Batch 30, train loss:0.8586397767066956, Elapsed time for epoch : 0.36253732442855835
Epoch 2, Batch 40, train loss:0.8945119976997375, Elapsed time for epoch : 0.47916650772094727
Epoch 2, Batch 50, train loss:0.8961825370788574, Elapsed time for epoch : 0.5959792772928874
Epoch 2, Batch 60, train loss:0.8756738901138306, Elapsed time for epoch : 0.7122976263364156
Epoch 2, Batch 70, train loss:0.35182809829711914, Elapsed time for epoch : 0.8288404822349549
Epoch 2, Batch 80, train loss:0.7856441736221313, Elapsed time for epoch : 0.9454110344250997
Epoch 2, Batch 90, train loss:0.7497419714927673, Elapsed time for epoch : 1.0616005063056946
Epoch 2, Batch 100, train loss:0.8198772072792053, Elapsed time for epoch : 1.178271190325419
Epoch 2, Batch 110, train loss:0.6554508209228516, Elapsed time for epoch : 1.2951929688453674
Batch 0, val loss:1.2620869874954224
Batch 10, val loss:2.60335111618042
Batch 20, val loss:1.6270097494125366
Batch 30, val loss:3.113563299179077
Epoch 2, Train Loss:0.7753076206082883, Val loss:4.706082164413399
Epoch 3, Batch 0, train loss:0.6049898862838745, Elapsed time for epoch : 0.011696537335713705
Epoch 3, Batch 10, train loss:0.6785024404525757, Elapsed time for epoch : 0.12824760278066
Epoch 3, Batch 20, train loss:0.6737594604492188, Elapsed time for epoch : 0.2444539229075114
Epoch 3, Batch 30, train loss:0.5991178154945374, Elapsed time for epoch : 0.3609944423039754
Epoch 3, Batch 40, train loss:0.6183305382728577, Elapsed time for epoch : 0.4773072322209676
Epoch 3, Batch 50, train loss:0.1770518720149994, Elapsed time for epoch : 0.5938211758931478
Epoch 3, Batch 60, train loss:0.5480236411094666, Elapsed time for epoch : 0.7099685708681742
Epoch 3, Batch 70, train loss:0.5455349087715149, Elapsed time for epoch : 0.8263203859329223
Epoch 3, Batch 80, train loss:0.5808342099189758, Elapsed time for epoch : 0.9427695592244466
Epoch 3, Batch 90, train loss:0.4838888943195343, Elapsed time for epoch : 1.059191604455312
Epoch 3, Batch 100, train loss:0.07487261295318604, Elapsed time for epoch : 1.1760261416435243
Epoch 3, Batch 110, train loss:0.4752545952796936, Elapsed time for epoch : 1.2924636165301004
Batch 0, val loss:3.3621983528137207
Batch 10, val loss:16.114625930786133
Batch 20, val loss:20.97276496887207
Batch 30, val loss:2.2044825553894043
Epoch 3, Train Loss:0.4889589593462322, Val loss:5.962275430560112
Epoch 4, Batch 0, train loss:0.47602567076683044, Elapsed time for epoch : 0.011735896269480387
Epoch 4, Batch 10, train loss:0.11708682775497437, Elapsed time for epoch : 0.12847239176432293
Epoch 4, Batch 20, train loss:0.08346037566661835, Elapsed time for epoch : 0.24532426993052164
Epoch 4, Batch 30, train loss:0.35239413380622864, Elapsed time for epoch : 0.3618730386098226
Epoch 4, Batch 40, train loss:0.31869369745254517, Elapsed time for epoch : 0.47850403785705564
Epoch 4, Batch 50, train loss:0.4584549367427826, Elapsed time for epoch : 0.5951518138249715
Epoch 4, Batch 60, train loss:0.305950403213501, Elapsed time for epoch : 0.7115572452545166
Epoch 4, Batch 70, train loss:0.32742026448249817, Elapsed time for epoch : 0.8281347433725993
Epoch 4, Batch 80, train loss:0.09265219420194626, Elapsed time for epoch : 0.9444111784299215
Epoch 4, Batch 90, train loss:0.2456897348165512, Elapsed time for epoch : 1.061165189743042
Epoch 4, Batch 100, train loss:0.3868698477745056, Elapsed time for epoch : 1.1775135397911072
Epoch 4, Batch 110, train loss:0.34486034512519836, Elapsed time for epoch : 1.2943461497624715
Batch 0, val loss:8.85212230682373
Batch 10, val loss:2.3190176486968994
Batch 20, val loss:6.676617622375488
Batch 30, val loss:1.6706277132034302
Epoch 4, Train Loss:0.29313002128316007, Val loss:4.7588384201129275
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29313
wandb:         Val Loss 4.75884
wandb:      train_batch 110
wandb: train_batch_loss 0.34486
wandb:        val_batch 30
wandb:   val_batch_loss 1.67063
wandb: 
wandb: üöÄ View run devoted-salad-400 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ic70mms1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_060143-ic70mms1/logs
Seed completed execution! 1 0.9_5
------------------------------------------------------------------
Running for seed 42 of experiment 0.9_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_060931-rs9xrdsi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-firebrand-402
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rs9xrdsi
Epoch 0, Batch 0, train loss:8.686380386352539, Elapsed time for epoch : 0.013054490089416504
Epoch 0, Batch 10, train loss:4.013209342956543, Elapsed time for epoch : 0.12917099793752035
Epoch 0, Batch 20, train loss:4.244500160217285, Elapsed time for epoch : 0.24526429176330566
Epoch 0, Batch 30, train loss:3.1841530799865723, Elapsed time for epoch : 0.3614247957865397
Epoch 0, Batch 40, train loss:3.2033283710479736, Elapsed time for epoch : 0.4776627858479818
Epoch 0, Batch 50, train loss:2.9221115112304688, Elapsed time for epoch : 0.5940915107727051
Epoch 0, Batch 60, train loss:2.808889627456665, Elapsed time for epoch : 0.7101675351460774
Epoch 0, Batch 70, train loss:2.570650339126587, Elapsed time for epoch : 0.8265265345573425
Epoch 0, Batch 80, train loss:2.838886022567749, Elapsed time for epoch : 0.9429492155710856
Epoch 0, Batch 90, train loss:2.509636402130127, Elapsed time for epoch : 1.0591295838356019
Epoch 0, Batch 100, train loss:2.164167881011963, Elapsed time for epoch : 1.175497023264567
Epoch 0, Batch 110, train loss:1.97530198097229, Elapsed time for epoch : 1.2919000307718913
Batch 0, val loss:5.011483192443848
Batch 10, val loss:4.004398345947266
Batch 20, val loss:3.645171642303467
Batch 30, val loss:6.98581600189209
Epoch 0, Train Loss:3.225737426591956, Val loss:4.008006897237566
Epoch 1, Batch 0, train loss:1.953001618385315, Elapsed time for epoch : 0.011689901351928711
Epoch 1, Batch 10, train loss:1.812613606452942, Elapsed time for epoch : 0.12817298571268718
Epoch 1, Batch 20, train loss:1.43244206905365, Elapsed time for epoch : 0.2446475108464559
Epoch 1, Batch 30, train loss:1.7130661010742188, Elapsed time for epoch : 0.36128506263097127
Epoch 1, Batch 40, train loss:1.5028754472732544, Elapsed time for epoch : 0.47826108932495115
Epoch 1, Batch 50, train loss:1.5146178007125854, Elapsed time for epoch : 0.594838007291158
Epoch 1, Batch 60, train loss:1.6174894571304321, Elapsed time for epoch : 0.7113772034645081
Epoch 1, Batch 70, train loss:1.5220558643341064, Elapsed time for epoch : 0.8281381726264954
Epoch 1, Batch 80, train loss:1.413203477859497, Elapsed time for epoch : 0.9446972449620564
Epoch 1, Batch 90, train loss:1.1499443054199219, Elapsed time for epoch : 1.0615918477376303
Epoch 1, Batch 100, train loss:0.7688693404197693, Elapsed time for epoch : 1.1781941294670104
Epoch 1, Batch 110, train loss:0.9212377667427063, Elapsed time for epoch : 1.294714617729187
Batch 0, val loss:2.045816659927368
Batch 10, val loss:3.2857162952423096
Batch 20, val loss:3.9255645275115967
Batch 30, val loss:2.634429693222046
Epoch 1, Train Loss:1.4284353178480398, Val loss:3.612700548436907
Epoch 2, Batch 0, train loss:1.057824730873108, Elapsed time for epoch : 0.011661787827809652
Epoch 2, Batch 10, train loss:0.8888640999794006, Elapsed time for epoch : 0.12812188069025676
Epoch 2, Batch 20, train loss:0.9739216566085815, Elapsed time for epoch : 0.24448476632436117
Epoch 2, Batch 30, train loss:0.8586397767066956, Elapsed time for epoch : 0.3609048088391622
Epoch 2, Batch 40, train loss:0.8945119976997375, Elapsed time for epoch : 0.4776965339978536
Epoch 2, Batch 50, train loss:0.8961825370788574, Elapsed time for epoch : 0.5941676020622253
Epoch 2, Batch 60, train loss:0.8756738901138306, Elapsed time for epoch : 0.7111456831296284
Epoch 2, Batch 70, train loss:0.35182809829711914, Elapsed time for epoch : 0.8275377313296001
Epoch 2, Batch 80, train loss:0.7856441736221313, Elapsed time for epoch : 0.9442617615063985
Epoch 2, Batch 90, train loss:0.7497419714927673, Elapsed time for epoch : 1.0608450571695964
Epoch 2, Batch 100, train loss:0.8198772072792053, Elapsed time for epoch : 1.1774708787600199
Epoch 2, Batch 110, train loss:0.6554508209228516, Elapsed time for epoch : 1.2941153605779012
Batch 0, val loss:1.2620869874954224
Batch 10, val loss:2.60335111618042
Batch 20, val loss:1.6270097494125366
Batch 30, val loss:3.113563299179077
Epoch 2, Train Loss:0.7753076206082883, Val loss:4.706082164413399
Epoch 3, Batch 0, train loss:0.6049898862838745, Elapsed time for epoch : 0.011695488293965658
Epoch 3, Batch 10, train loss:0.6785024404525757, Elapsed time for epoch : 0.12826709747314452
Epoch 3, Batch 20, train loss:0.6737594604492188, Elapsed time for epoch : 0.24476684331893922
Epoch 3, Batch 30, train loss:0.5991178154945374, Elapsed time for epoch : 0.36132655541102093
Epoch 3, Batch 40, train loss:0.6183305382728577, Elapsed time for epoch : 0.4783598860104879
Epoch 3, Batch 50, train loss:0.1770518720149994, Elapsed time for epoch : 0.5950621803601582
Epoch 3, Batch 60, train loss:0.5480236411094666, Elapsed time for epoch : 0.7115437587102255
Epoch 3, Batch 70, train loss:0.5455349087715149, Elapsed time for epoch : 0.8282467881838481
Epoch 3, Batch 80, train loss:0.5808342099189758, Elapsed time for epoch : 0.9450037876764933
Epoch 3, Batch 90, train loss:0.4838888943195343, Elapsed time for epoch : 1.0618839462598164
Epoch 3, Batch 100, train loss:0.07487261295318604, Elapsed time for epoch : 1.1786244591077168
Epoch 3, Batch 110, train loss:0.4752545952796936, Elapsed time for epoch : 1.2951305548350016
Batch 0, val loss:3.3621983528137207
Batch 10, val loss:16.114625930786133
Batch 20, val loss:20.97276496887207
Batch 30, val loss:2.2044825553894043
Epoch 3, Train Loss:0.4889589593462322, Val loss:5.962275430560112
Epoch 4, Batch 0, train loss:0.47602567076683044, Elapsed time for epoch : 0.011693052450815837
Epoch 4, Batch 10, train loss:0.11708682775497437, Elapsed time for epoch : 0.12855108976364135
Epoch 4, Batch 20, train loss:0.08346037566661835, Elapsed time for epoch : 0.2454169193903605
Epoch 4, Batch 30, train loss:0.35239413380622864, Elapsed time for epoch : 0.362171479066213
Epoch 4, Batch 40, train loss:0.31869369745254517, Elapsed time for epoch : 0.4785529891649882
Epoch 4, Batch 50, train loss:0.4584549367427826, Elapsed time for epoch : 0.5951725641886393
Epoch 4, Batch 60, train loss:0.305950403213501, Elapsed time for epoch : 0.711614175637563
Epoch 4, Batch 70, train loss:0.32742026448249817, Elapsed time for epoch : 0.8282527049382528
Epoch 4, Batch 80, train loss:0.09265219420194626, Elapsed time for epoch : 0.9453290025393168
Epoch 4, Batch 90, train loss:0.2456897348165512, Elapsed time for epoch : 1.0616528232892355
Epoch 4, Batch 100, train loss:0.3868698477745056, Elapsed time for epoch : 1.1781929016113282
Epoch 4, Batch 110, train loss:0.34486034512519836, Elapsed time for epoch : 1.2948087890942892
Batch 0, val loss:8.85212230682373
Batch 10, val loss:2.3190176486968994
Batch 20, val loss:6.676617622375488
Batch 30, val loss:1.6706277132034302
Epoch 4, Train Loss:0.29313002128316007, Val loss:4.7588384201129275
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29313
wandb:         Val Loss 4.75884
wandb:      train_batch 110
wandb: train_batch_loss 0.34486
wandb:        val_batch 30
wandb:   val_batch_loss 1.67063
wandb: 
wandb: üöÄ View run lyric-firebrand-402 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rs9xrdsi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_060931-rs9xrdsi/logs
Seed completed execution! 42 0.9_5
------------------------------------------------------------------
Running for seed 89 of experiment 0.9_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_061719-bghbec57
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-feather-404
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bghbec57
Epoch 0, Batch 0, train loss:8.686380386352539, Elapsed time for epoch : 0.012030482292175293
Epoch 0, Batch 10, train loss:4.013209342956543, Elapsed time for epoch : 0.12880686521530152
Epoch 0, Batch 20, train loss:4.244500160217285, Elapsed time for epoch : 0.24494055906931558
Epoch 0, Batch 30, train loss:3.1841530799865723, Elapsed time for epoch : 0.36108448505401614
Epoch 0, Batch 40, train loss:3.2033283710479736, Elapsed time for epoch : 0.4773304979006449
Epoch 0, Batch 50, train loss:2.9221115112304688, Elapsed time for epoch : 0.5935570120811462
Epoch 0, Batch 60, train loss:2.808889627456665, Elapsed time for epoch : 0.7097801804542542
Epoch 0, Batch 70, train loss:2.570650339126587, Elapsed time for epoch : 0.8262256026268006
Epoch 0, Batch 80, train loss:2.838886022567749, Elapsed time for epoch : 0.9429237484931946
Epoch 0, Batch 90, train loss:2.509636402130127, Elapsed time for epoch : 1.0601106524467467
Epoch 0, Batch 100, train loss:2.164167881011963, Elapsed time for epoch : 1.1770194927851358
Epoch 0, Batch 110, train loss:1.97530198097229, Elapsed time for epoch : 1.2937487522761026
Batch 0, val loss:5.011483192443848
Batch 10, val loss:4.004398345947266
Batch 20, val loss:3.645171642303467
Batch 30, val loss:6.98581600189209
Epoch 0, Train Loss:3.225737426591956, Val loss:4.008006897237566
Epoch 1, Batch 0, train loss:1.953001618385315, Elapsed time for epoch : 0.011687274773915608
Epoch 1, Batch 10, train loss:1.812613606452942, Elapsed time for epoch : 0.12806939681371052
Epoch 1, Batch 20, train loss:1.43244206905365, Elapsed time for epoch : 0.2448289672533671
Epoch 1, Batch 30, train loss:1.7130661010742188, Elapsed time for epoch : 0.3614122907320658
Epoch 1, Batch 40, train loss:1.5028754472732544, Elapsed time for epoch : 0.4779055992762248
Epoch 1, Batch 50, train loss:1.5146178007125854, Elapsed time for epoch : 0.5947539846102396
Epoch 1, Batch 60, train loss:1.6174894571304321, Elapsed time for epoch : 0.7113268494606018
Epoch 1, Batch 70, train loss:1.5220558643341064, Elapsed time for epoch : 0.8277967572212219
Epoch 1, Batch 80, train loss:1.413203477859497, Elapsed time for epoch : 0.944721249739329
Epoch 1, Batch 90, train loss:1.1499443054199219, Elapsed time for epoch : 1.0611570596694946
Epoch 1, Batch 100, train loss:0.7688693404197693, Elapsed time for epoch : 1.1783001939455668
Epoch 1, Batch 110, train loss:0.9212377667427063, Elapsed time for epoch : 1.2952631831169128
Batch 0, val loss:2.045816659927368
Batch 10, val loss:3.2857162952423096
Batch 20, val loss:3.9255645275115967
Batch 30, val loss:2.634429693222046
Epoch 1, Train Loss:1.4284353178480398, Val loss:3.612700548436907
Epoch 2, Batch 0, train loss:1.057824730873108, Elapsed time for epoch : 0.011645996570587158
Epoch 2, Batch 10, train loss:0.8888640999794006, Elapsed time for epoch : 0.12801086902618408
Epoch 2, Batch 20, train loss:0.9739216566085815, Elapsed time for epoch : 0.24456140597661336
Epoch 2, Batch 30, train loss:0.8586397767066956, Elapsed time for epoch : 0.3613894502321879
Epoch 2, Batch 40, train loss:0.8945119976997375, Elapsed time for epoch : 0.47762695550918577
Epoch 2, Batch 50, train loss:0.8961825370788574, Elapsed time for epoch : 0.5948905746142069
Epoch 2, Batch 60, train loss:0.8756738901138306, Elapsed time for epoch : 0.7112201015154521
Epoch 2, Batch 70, train loss:0.35182809829711914, Elapsed time for epoch : 0.8276104768117268
Epoch 2, Batch 80, train loss:0.7856441736221313, Elapsed time for epoch : 0.9442680438359579
Epoch 2, Batch 90, train loss:0.7497419714927673, Elapsed time for epoch : 1.0608243028322855
Epoch 2, Batch 100, train loss:0.8198772072792053, Elapsed time for epoch : 1.177749228477478
Epoch 2, Batch 110, train loss:0.6554508209228516, Elapsed time for epoch : 1.2945312023162843
Batch 0, val loss:1.2620869874954224
Batch 10, val loss:2.60335111618042
Batch 20, val loss:1.6270097494125366
Batch 30, val loss:3.113563299179077
Epoch 2, Train Loss:0.7753076206082883, Val loss:4.706082164413399
Epoch 3, Batch 0, train loss:0.6049898862838745, Elapsed time for epoch : 0.011743466059366861
Epoch 3, Batch 10, train loss:0.6785024404525757, Elapsed time for epoch : 0.128622039159139
Epoch 3, Batch 20, train loss:0.6737594604492188, Elapsed time for epoch : 0.24528197050094605
Epoch 3, Batch 30, train loss:0.5991178154945374, Elapsed time for epoch : 0.36174102226893107
Epoch 3, Batch 40, train loss:0.6183305382728577, Elapsed time for epoch : 0.4784613807996114
Epoch 3, Batch 50, train loss:0.1770518720149994, Elapsed time for epoch : 0.5948996067047119
Epoch 3, Batch 60, train loss:0.5480236411094666, Elapsed time for epoch : 0.7116765817006429
Epoch 3, Batch 70, train loss:0.5455349087715149, Elapsed time for epoch : 0.8285297592480977
Epoch 3, Batch 80, train loss:0.5808342099189758, Elapsed time for epoch : 0.9448291142781575
Epoch 3, Batch 90, train loss:0.4838888943195343, Elapsed time for epoch : 1.0614346385002136
Epoch 3, Batch 100, train loss:0.07487261295318604, Elapsed time for epoch : 1.1780123432477316
Epoch 3, Batch 110, train loss:0.4752545952796936, Elapsed time for epoch : 1.2948200901349385
Batch 0, val loss:3.3621983528137207
Batch 10, val loss:16.114625930786133
Batch 20, val loss:20.97276496887207
Batch 30, val loss:2.2044825553894043
Epoch 3, Train Loss:0.4889589593462322, Val loss:5.962275430560112
Epoch 4, Batch 0, train loss:0.47602567076683044, Elapsed time for epoch : 0.011663818359375
Epoch 4, Batch 10, train loss:0.11708682775497437, Elapsed time for epoch : 0.12809820572535197
Epoch 4, Batch 20, train loss:0.08346037566661835, Elapsed time for epoch : 0.24498552481333416
Epoch 4, Batch 30, train loss:0.35239413380622864, Elapsed time for epoch : 0.3613746245702108
Epoch 4, Batch 40, train loss:0.31869369745254517, Elapsed time for epoch : 0.4780341664950053
Epoch 4, Batch 50, train loss:0.4584549367427826, Elapsed time for epoch : 0.5944129665692647
Epoch 4, Batch 60, train loss:0.305950403213501, Elapsed time for epoch : 0.7110873818397522
Epoch 4, Batch 70, train loss:0.32742026448249817, Elapsed time for epoch : 0.8275365869204203
Epoch 4, Batch 80, train loss:0.09265219420194626, Elapsed time for epoch : 0.9438051184018453
Epoch 4, Batch 90, train loss:0.2456897348165512, Elapsed time for epoch : 1.0603705326716104
Epoch 4, Batch 100, train loss:0.3868698477745056, Elapsed time for epoch : 1.1766433358192443
Epoch 4, Batch 110, train loss:0.34486034512519836, Elapsed time for epoch : 1.293457845846812
Batch 0, val loss:8.85212230682373
Batch 10, val loss:2.3190176486968994
Batch 20, val loss:6.676617622375488
Batch 30, val loss:1.6706277132034302
Epoch 4, Train Loss:0.29313002128316007, Val loss:4.7588384201129275
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29313
wandb:         Val Loss 4.75884
wandb:      train_batch 110
wandb: train_batch_loss 0.34486
wandb:        val_batch 30
wandb:   val_batch_loss 1.67063
wandb: 
wandb: üöÄ View run noble-feather-404 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bghbec57
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_061719-bghbec57/logs
Seed completed execution! 89 0.9_5
------------------------------------------------------------------
Running for seed 23 of experiment 0.9_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_062508-gk3hhvjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-totem-406
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gk3hhvjg
Epoch 0, Batch 0, train loss:8.686380386352539, Elapsed time for epoch : 0.013446382681528727
Epoch 0, Batch 10, train loss:4.013209342956543, Elapsed time for epoch : 0.1298395832379659
Epoch 0, Batch 20, train loss:4.244500160217285, Elapsed time for epoch : 0.24672263860702515
Epoch 0, Batch 30, train loss:3.1841530799865723, Elapsed time for epoch : 0.3632526477177938
Epoch 0, Batch 40, train loss:3.2033283710479736, Elapsed time for epoch : 0.480177628993988
Epoch 0, Batch 50, train loss:2.9221115112304688, Elapsed time for epoch : 0.5963931322097779
Epoch 0, Batch 60, train loss:2.808889627456665, Elapsed time for epoch : 0.7131161848704021
Epoch 0, Batch 70, train loss:2.570650339126587, Elapsed time for epoch : 0.8295631885528565
Epoch 0, Batch 80, train loss:2.838886022567749, Elapsed time for epoch : 0.9460850159327189
Epoch 0, Batch 90, train loss:2.509636402130127, Elapsed time for epoch : 1.0623339732487997
Epoch 0, Batch 100, train loss:2.164167881011963, Elapsed time for epoch : 1.1787280758221945
Epoch 0, Batch 110, train loss:1.97530198097229, Elapsed time for epoch : 1.2951828916867574
Batch 0, val loss:5.011483192443848
Batch 10, val loss:4.004398345947266
Batch 20, val loss:3.645171642303467
Batch 30, val loss:6.98581600189209
Epoch 0, Train Loss:3.225737426591956, Val loss:4.008006897237566
Epoch 1, Batch 0, train loss:1.953001618385315, Elapsed time for epoch : 0.01163485050201416
Epoch 1, Batch 10, train loss:1.812613606452942, Elapsed time for epoch : 0.12785412867863974
Epoch 1, Batch 20, train loss:1.43244206905365, Elapsed time for epoch : 0.24440595706303914
Epoch 1, Batch 30, train loss:1.7130661010742188, Elapsed time for epoch : 0.3608609159787496
Epoch 1, Batch 40, train loss:1.5028754472732544, Elapsed time for epoch : 0.4775351881980896
Epoch 1, Batch 50, train loss:1.5146178007125854, Elapsed time for epoch : 0.594145393371582
Epoch 1, Batch 60, train loss:1.6174894571304321, Elapsed time for epoch : 0.7104215224583944
Epoch 1, Batch 70, train loss:1.5220558643341064, Elapsed time for epoch : 0.8272803862889607
Epoch 1, Batch 80, train loss:1.413203477859497, Elapsed time for epoch : 0.9436239163080852
Epoch 1, Batch 90, train loss:1.1499443054199219, Elapsed time for epoch : 1.0599221905072531
Epoch 1, Batch 100, train loss:0.7688693404197693, Elapsed time for epoch : 1.1762952526410422
Epoch 1, Batch 110, train loss:0.9212377667427063, Elapsed time for epoch : 1.29288299481074
Batch 0, val loss:2.045816659927368
Batch 10, val loss:3.2857162952423096
Batch 20, val loss:3.9255645275115967
Batch 30, val loss:2.634429693222046
Epoch 1, Train Loss:1.4284353178480398, Val loss:3.612700548436907
Epoch 2, Batch 0, train loss:1.057824730873108, Elapsed time for epoch : 0.011665427684783935
Epoch 2, Batch 10, train loss:0.8888640999794006, Elapsed time for epoch : 0.12830291191736856
Epoch 2, Batch 20, train loss:0.9739216566085815, Elapsed time for epoch : 0.24485402504603068
Epoch 2, Batch 30, train loss:0.8586397767066956, Elapsed time for epoch : 0.3613413612047831
Epoch 2, Batch 40, train loss:0.8945119976997375, Elapsed time for epoch : 0.4785791873931885
Epoch 2, Batch 50, train loss:0.8961825370788574, Elapsed time for epoch : 0.5952560544013977
Epoch 2, Batch 60, train loss:0.8756738901138306, Elapsed time for epoch : 0.7119980136553447
Epoch 2, Batch 70, train loss:0.35182809829711914, Elapsed time for epoch : 0.8287775953610738
Epoch 2, Batch 80, train loss:0.7856441736221313, Elapsed time for epoch : 0.945054825146993
Epoch 2, Batch 90, train loss:0.7497419714927673, Elapsed time for epoch : 1.0617239157358804
Epoch 2, Batch 100, train loss:0.8198772072792053, Elapsed time for epoch : 1.178315238157908
Epoch 2, Batch 110, train loss:0.6554508209228516, Elapsed time for epoch : 1.294676423072815
Batch 0, val loss:1.2620869874954224
Batch 10, val loss:2.60335111618042
Batch 20, val loss:1.6270097494125366
Batch 30, val loss:3.113563299179077
Epoch 2, Train Loss:0.7753076206082883, Val loss:4.706082164413399
Epoch 3, Batch 0, train loss:0.6049898862838745, Elapsed time for epoch : 0.011799307664235432
Epoch 3, Batch 10, train loss:0.6785024404525757, Elapsed time for epoch : 0.12836681207021078
Epoch 3, Batch 20, train loss:0.6737594604492188, Elapsed time for epoch : 0.24523261388142903
Epoch 3, Batch 30, train loss:0.5991178154945374, Elapsed time for epoch : 0.3618163545926412
Epoch 3, Batch 40, train loss:0.6183305382728577, Elapsed time for epoch : 0.47894301017125446
Epoch 3, Batch 50, train loss:0.1770518720149994, Elapsed time for epoch : 0.5951364715894063
Epoch 3, Batch 60, train loss:0.5480236411094666, Elapsed time for epoch : 0.7119234402974447
Epoch 3, Batch 70, train loss:0.5455349087715149, Elapsed time for epoch : 0.8291499018669128
Epoch 3, Batch 80, train loss:0.5808342099189758, Elapsed time for epoch : 0.9453061699867249
Epoch 3, Batch 90, train loss:0.4838888943195343, Elapsed time for epoch : 1.0618298212687174
Epoch 3, Batch 100, train loss:0.07487261295318604, Elapsed time for epoch : 1.1781415541966755
Epoch 3, Batch 110, train loss:0.4752545952796936, Elapsed time for epoch : 1.2946971575419108
Batch 0, val loss:3.3621983528137207
Batch 10, val loss:16.114625930786133
Batch 20, val loss:20.97276496887207
Batch 30, val loss:2.2044825553894043
Epoch 3, Train Loss:0.4889589593462322, Val loss:5.962275430560112
Epoch 4, Batch 0, train loss:0.47602567076683044, Elapsed time for epoch : 0.011885110537211101
Epoch 4, Batch 10, train loss:0.11708682775497437, Elapsed time for epoch : 0.12839089234670004
Epoch 4, Batch 20, train loss:0.08346037566661835, Elapsed time for epoch : 0.24512571493784588
Epoch 4, Batch 30, train loss:0.35239413380622864, Elapsed time for epoch : 0.3618401606877645
Epoch 4, Batch 40, train loss:0.31869369745254517, Elapsed time for epoch : 0.4782927354176839
Epoch 4, Batch 50, train loss:0.4584549367427826, Elapsed time for epoch : 0.5948904275894165
Epoch 4, Batch 60, train loss:0.305950403213501, Elapsed time for epoch : 0.7111983974774678
Epoch 4, Batch 70, train loss:0.32742026448249817, Elapsed time for epoch : 0.8276984214782714
Epoch 4, Batch 80, train loss:0.09265219420194626, Elapsed time for epoch : 0.9442030270894368
Epoch 4, Batch 90, train loss:0.2456897348165512, Elapsed time for epoch : 1.0607163747151693
Epoch 4, Batch 100, train loss:0.3868698477745056, Elapsed time for epoch : 1.177320663134257
Epoch 4, Batch 110, train loss:0.34486034512519836, Elapsed time for epoch : 1.2936873356501262
Batch 0, val loss:8.85212230682373
Batch 10, val loss:2.3190176486968994
Batch 20, val loss:6.676617622375488
Batch 30, val loss:1.6706277132034302
Epoch 4, Train Loss:0.29313002128316007, Val loss:4.7588384201129275
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29313
wandb:         Val Loss 4.75884
wandb:      train_batch 110
wandb: train_batch_loss 0.34486
wandb:        val_batch 30
wandb:   val_batch_loss 1.67063
wandb: 
wandb: üöÄ View run glowing-totem-406 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gk3hhvjg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_062508-gk3hhvjg/logs
Seed completed execution! 23 0.9_5
------------------------------------------------------------------
Running for seed 113 of experiment 0.9_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_063256-4w5faa5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run prime-planet-408
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4w5faa5v
Epoch 0, Batch 0, train loss:8.686380386352539, Elapsed time for epoch : 0.01359634002049764
Epoch 0, Batch 10, train loss:4.013209342956543, Elapsed time for epoch : 0.1314334789911906
Epoch 0, Batch 20, train loss:4.244500160217285, Elapsed time for epoch : 0.2500438849131266
Epoch 0, Batch 30, train loss:3.1841530799865723, Elapsed time for epoch : 0.36737260421117146
Epoch 0, Batch 40, train loss:3.2033283710479736, Elapsed time for epoch : 0.4851408839225769
Epoch 0, Batch 50, train loss:2.9221115112304688, Elapsed time for epoch : 0.6031376401583354
Epoch 0, Batch 60, train loss:2.808889627456665, Elapsed time for epoch : 0.7205613056818644
Epoch 0, Batch 70, train loss:2.570650339126587, Elapsed time for epoch : 0.8377316315968831
Epoch 0, Batch 80, train loss:2.838886022567749, Elapsed time for epoch : 0.957248055934906
Epoch 0, Batch 90, train loss:2.509636402130127, Elapsed time for epoch : 1.074639117717743
Epoch 0, Batch 100, train loss:2.164167881011963, Elapsed time for epoch : 1.1919623613357544
Epoch 0, Batch 110, train loss:1.97530198097229, Elapsed time for epoch : 1.310184661547343
Batch 0, val loss:5.011483192443848
Batch 10, val loss:4.004398345947266
Batch 20, val loss:3.645171642303467
Batch 30, val loss:6.98581600189209
Epoch 0, Train Loss:3.225737426591956, Val loss:4.008006897237566
Epoch 1, Batch 0, train loss:1.953001618385315, Elapsed time for epoch : 0.011640751361846923
Epoch 1, Batch 10, train loss:1.812613606452942, Elapsed time for epoch : 0.12803069750467935
Epoch 1, Batch 20, train loss:1.43244206905365, Elapsed time for epoch : 0.2443215529123942
Epoch 1, Batch 30, train loss:1.7130661010742188, Elapsed time for epoch : 0.3607223709424337
Epoch 1, Batch 40, train loss:1.5028754472732544, Elapsed time for epoch : 0.4777953068415324
Epoch 1, Batch 50, train loss:1.5146178007125854, Elapsed time for epoch : 0.5945704062779744
Epoch 1, Batch 60, train loss:1.6174894571304321, Elapsed time for epoch : 0.7106613357861836
Epoch 1, Batch 70, train loss:1.5220558643341064, Elapsed time for epoch : 0.8270383596420288
Epoch 1, Batch 80, train loss:1.413203477859497, Elapsed time for epoch : 0.943552295366923
Epoch 1, Batch 90, train loss:1.1499443054199219, Elapsed time for epoch : 1.059736704826355
Epoch 1, Batch 100, train loss:0.7688693404197693, Elapsed time for epoch : 1.1761372089385986
Epoch 1, Batch 110, train loss:0.9212377667427063, Elapsed time for epoch : 1.2927881797154746
Batch 0, val loss:2.045816659927368
Batch 10, val loss:3.2857162952423096
Batch 20, val loss:3.9255645275115967
Batch 30, val loss:2.634429693222046
Epoch 1, Train Loss:1.4284353178480398, Val loss:3.612700548436907
Epoch 2, Batch 0, train loss:1.057824730873108, Elapsed time for epoch : 0.011633952458699545
Epoch 2, Batch 10, train loss:0.8888640999794006, Elapsed time for epoch : 0.1280320962270101
Epoch 2, Batch 20, train loss:0.9739216566085815, Elapsed time for epoch : 0.2443600336710612
Epoch 2, Batch 30, train loss:0.8586397767066956, Elapsed time for epoch : 0.3609778006871541
Epoch 2, Batch 40, train loss:0.8945119976997375, Elapsed time for epoch : 0.4781085054079692
Epoch 2, Batch 50, train loss:0.8961825370788574, Elapsed time for epoch : 0.5946525812149048
Epoch 2, Batch 60, train loss:0.8756738901138306, Elapsed time for epoch : 0.7113734205563863
Epoch 2, Batch 70, train loss:0.35182809829711914, Elapsed time for epoch : 0.8276925007502238
Epoch 2, Batch 80, train loss:0.7856441736221313, Elapsed time for epoch : 0.944138769308726
Epoch 2, Batch 90, train loss:0.7497419714927673, Elapsed time for epoch : 1.0601729035377503
Epoch 2, Batch 100, train loss:0.8198772072792053, Elapsed time for epoch : 1.1770043929417928
Epoch 2, Batch 110, train loss:0.6554508209228516, Elapsed time for epoch : 1.2935284852981568
Batch 0, val loss:1.2620869874954224
Batch 10, val loss:2.60335111618042
Batch 20, val loss:1.6270097494125366
Batch 30, val loss:3.113563299179077
Epoch 2, Train Loss:0.7753076206082883, Val loss:4.706082164413399
Epoch 3, Batch 0, train loss:0.6049898862838745, Elapsed time for epoch : 0.011706324418385823
Epoch 3, Batch 10, train loss:0.6785024404525757, Elapsed time for epoch : 0.12811443010965984
Epoch 3, Batch 20, train loss:0.6737594604492188, Elapsed time for epoch : 0.24458142518997192
Epoch 3, Batch 30, train loss:0.5991178154945374, Elapsed time for epoch : 0.36072229544321693
Epoch 3, Batch 40, train loss:0.6183305382728577, Elapsed time for epoch : 0.47707701524098717
Epoch 3, Batch 50, train loss:0.1770518720149994, Elapsed time for epoch : 0.5936424056688945
Epoch 3, Batch 60, train loss:0.5480236411094666, Elapsed time for epoch : 0.7100590705871582
Epoch 3, Batch 70, train loss:0.5455349087715149, Elapsed time for epoch : 0.8263412316640218
Epoch 3, Batch 80, train loss:0.5808342099189758, Elapsed time for epoch : 0.9424971381823222
Epoch 3, Batch 90, train loss:0.4838888943195343, Elapsed time for epoch : 1.0591762900352477
Epoch 3, Batch 100, train loss:0.07487261295318604, Elapsed time for epoch : 1.1753848433494567
Epoch 3, Batch 110, train loss:0.4752545952796936, Elapsed time for epoch : 1.2920835057894389
Batch 0, val loss:3.3621983528137207
Batch 10, val loss:16.114625930786133
Batch 20, val loss:20.97276496887207
Batch 30, val loss:2.2044825553894043
Epoch 3, Train Loss:0.4889589593462322, Val loss:5.962275430560112
Epoch 4, Batch 0, train loss:0.47602567076683044, Elapsed time for epoch : 0.01164620320002238
Epoch 4, Batch 10, train loss:0.11708682775497437, Elapsed time for epoch : 0.12796563704808553
Epoch 4, Batch 20, train loss:0.08346037566661835, Elapsed time for epoch : 0.24465214014053344
Epoch 4, Batch 30, train loss:0.35239413380622864, Elapsed time for epoch : 0.360828697681427
Epoch 4, Batch 40, train loss:0.31869369745254517, Elapsed time for epoch : 0.47742911577224734
Epoch 4, Batch 50, train loss:0.4584549367427826, Elapsed time for epoch : 0.5940256158510844
Epoch 4, Batch 60, train loss:0.305950403213501, Elapsed time for epoch : 0.7100671291351318
Epoch 4, Batch 70, train loss:0.32742026448249817, Elapsed time for epoch : 0.8270302653312683
Epoch 4, Batch 80, train loss:0.09265219420194626, Elapsed time for epoch : 0.9434428970019023
Epoch 4, Batch 90, train loss:0.2456897348165512, Elapsed time for epoch : 1.0598607341448465
Epoch 4, Batch 100, train loss:0.3868698477745056, Elapsed time for epoch : 1.1761017600695292
Epoch 4, Batch 110, train loss:0.34486034512519836, Elapsed time for epoch : 1.2923820813496907
Batch 0, val loss:8.85212230682373
Batch 10, val loss:2.3190176486968994
Batch 20, val loss:6.676617622375488
Batch 30, val loss:1.6706277132034302
Epoch 4, Train Loss:0.29313002128316007, Val loss:4.7588384201129275
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÇ‚ñÅ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29313
wandb:         Val Loss 4.75884
wandb:      train_batch 110
wandb: train_batch_loss 0.34486
wandb:        val_batch 30
wandb:   val_batch_loss 1.67063
wandb: 
wandb: üöÄ View run prime-planet-408 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4w5faa5v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_063256-4w5faa5v/logs
Seed completed execution! 113 0.9_5
------------------------------------------------------------------
Experiment complete 0.9_5
