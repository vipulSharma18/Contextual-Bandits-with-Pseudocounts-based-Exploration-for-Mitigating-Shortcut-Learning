Pseudo-code for the architecture is provided below:
def encode(x,z_dim):
"""
ConvNet encoder
args:
B-batch_size, C-channels
H,W-spatial_dims
x : shape : [B, C, H, W]
C = 3 * num_frames; 3 - R/G/B
z_dim: latent dimension
"""
x = x / 255.
# c: channels, f: filters
# k: kernel, s: stride
z = Conv2d(c=x.shape[1], f=32, k=3, s=2)])(
x)
z = ReLU(z)
for _ in range(num_layers - 1):
z = Conv2d((c=32, f=32, k=3, s=1))(z)
z = ReLU(z)
z = flatten(z)
# in: input dim, out: output_dim, h:
hiddens
z = mlp(in=z.size(),out=z_dim,h=1024)
z = LayerNorm(z)
z = tanh(z)

MOCO: 


CURL: 
# f_q, f_k: encoder networks for anchor
# (query) and target (keys) respectively.
# loader: minibatch sampler from ReplayBuffer
# B-batch_size, C-channels, H,W-spatial_dims
# x : shape : [B, C, H, W]
# C = c * num_frames; c=3 (R/G/B) or 1 (gray)
# m: momentum, e.g. 0.95
# z_dim: latent dimension
f_k.params = f_q.params
W = rand(z_dim, z_dim) # bilinear product.
for x in loader: # load minibatch from buffer
x_q = aug(x) # random augmentation
x_k = aug(x) # different random augmentation
z_q = f_q.forward(x_q)
z_k = f_k.forward(x_k)
z_k = z_k.detach() # stop gradient
proj_k = matmul(W, z_k.T) # bilinear product
logits = matmul(z_q, proj_k) # B x B
# subtract max from logits for stability
logits = logits - max(logits, axis=1)
labels = arange(logits.shape[0])
loss = CrossEntropyLoss(logits, labels)
loss.backward()
update(f_q.params) # Adam
update(W) # Adam
f_k.params = m*f_k.params+(1-m)*f_q.params