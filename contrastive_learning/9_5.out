## SLURM PROLOG ###############################################################
##    Job ID : 1988398
##  Job Name : 9_5
##  Nodelist : gpu1401
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning
##   Job Started : Fri May 10 11:43:20 PM EDT 2024
###############################################################################
Running for input 0.9_5
==========================================================================
Running experiment for setting 0.6_1
==========================================================================
Running for seed 1 of experiment 0.6_1
wandb: Currently logged in as: vipul. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_234329-8zju5oc6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-gorge-303
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8zju5oc6
/users/vsharm44/.conda/envs/dl_project/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.017500698566436768
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.13180877367655436
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24520955880482992
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.35974735418955484
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47368744214375813
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5880783677101136
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.702229130268097
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.817090920607249
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9316854039827983
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0462793946266173
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1614142219225565
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2761680523554484
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011550052960713705
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.1269398331642151
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.2423314889272054
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.3580958882967631
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.47435959180196124
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5899436950683594
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7055489659309387
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8214751561482747
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9375311175982157
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.0532546679178874
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1690235137939453
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2858044942220053
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.01155852476755778
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.12745975653330485
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.24377150932947794
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.36004707018534343
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.47572837670644125
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.591865622997284
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.7079748352368672
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.8237067540486653
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.939767348766327
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0556510885556538
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.171736772855123
