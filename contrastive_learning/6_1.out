## SLURM PROLOG ###############################################################
##    Job ID : 1988330
##  Job Name : 6_1
##  Nodelist : gpu2001
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning
##   Job Started : Fri May 10 11:27:55 PM EDT 2024
###############################################################################
Running for input 0.6_1
==========================================================================
Running experiment for setting 0.6_1
==========================================================================
Running for seed 1 of experiment 0.6_1
wandb: Currently logged in as: vipul. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_232804-z3vir5iy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-waterfall-278
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/z3vir5iy
/users/vsharm44/.conda/envs/dl_project/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.017278615633646646
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.1332974116007487
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24824529488881428
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3637486775716146
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.4803890585899353
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5962960282961528
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7122580567995708
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8286187132199605
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9447433074315389
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0608743985493978
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.177204934755961
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2938263932863872
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.021 MB of 0.031 MB uploaded (0.004 MB deduped)wandb: / 0.021 MB of 0.031 MB uploaded (0.004 MB deduped)wandb: - 0.031 MB of 0.031 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 12.2%             
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 3.06338
wandb:         Val Loss 3.66226
wandb:      train_batch 110
wandb: train_batch_loss 1.50213
wandb:        val_batch 30
wandb:   val_batch_loss 8.31066
wandb: 
wandb: üöÄ View run fresh-waterfall-278 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/z3vir5iy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_232804-z3vir5iy/logs
Seed completed execution! 1 0.6_1
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_232946-rl0gvn75
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-pond-280
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rl0gvn75
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.01287450393040975
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12901215155919393
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24532402356465657
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.36159874200820924
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47842259804407755
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5949266433715821
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7120245933532715
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8284740567207336
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9458516955375671
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.06244854927063
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1790266354878745
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2967161615689595
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 3.06338
wandb:         Val Loss 3.66226
wandb:      train_batch 110
wandb: train_batch_loss 1.50213
wandb:        val_batch 30
wandb:   val_batch_loss 8.31066
wandb: 
wandb: üöÄ View run vivid-pond-280 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rl0gvn75
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_232946-rl0gvn75/logs
Seed completed execution! 42 0.6_1
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_233128-9vhih01w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-haze-282
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9vhih01w
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.012764163812001546
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.1296139081319173
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24633537928263347
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3629590113957723
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47962726354599
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5960866928100585
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7130590915679932
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8299168308575948
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9466471552848816
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0638326764106751
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.180597444375356
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2972705761591594
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 3.06338
wandb:         Val Loss 3.66226
wandb:      train_batch 110
wandb: train_batch_loss 1.50213
wandb:        val_batch 30
wandb:   val_batch_loss 8.31066
wandb: 
wandb: üöÄ View run crimson-haze-282 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9vhih01w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_233128-9vhih01w/logs
Seed completed execution! 89 0.6_1
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_233309-x9alyq4v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-shadow-284
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/x9alyq4v
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.012144339084625245
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12858970165252687
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24503578344980875
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3617934862772624
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47897337277730306
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5957182288169861
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7126407821973165
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8296489119529724
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9470483740170796
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0637757182121277
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1807276725769043
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2990322271982828
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 3.06338
wandb:         Val Loss 3.66226
wandb:      train_batch 110
wandb: train_batch_loss 1.50213
wandb:        val_batch 30
wandb:   val_batch_loss 8.31066
wandb: 
wandb: üöÄ View run confused-shadow-284 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/x9alyq4v
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_233309-x9alyq4v/logs
Seed completed execution! 23 0.6_1
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_233450-b5qaov8k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-galaxy-286
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/b5qaov8k
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.012335411707560221
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12877225081125895
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24567968448003133
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3620815912882487
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47938061952590943
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.596402907371521
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.713555924097697
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8302538673082988
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9470686316490173
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0640145897865296
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1810605963071188
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.298453934987386
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
wandb: - 0.015 MB of 0.024 MB uploadedwandb: \ 0.015 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÅ‚ñà
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 3.06338
wandb:         Val Loss 3.66226
wandb:      train_batch 110
wandb: train_batch_loss 1.50213
wandb:        val_batch 30
wandb:   val_batch_loss 8.31066
wandb: 
wandb: üöÄ View run gentle-galaxy-286 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/b5qaov8k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_233450-b5qaov8k/logs
Seed completed execution! 113 0.6_1
------------------------------------------------------------------
Experiment complete 0.6_1
==========================================================================
Running experiment for setting 0.6_2
==========================================================================
Running for seed 1 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_233631-tnss57q6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-lake-288
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/tnss57q6
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.013161798318227133
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.13006307284037272
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.24653517405192057
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.3635742386182149
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.4803681135177612
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5973597407341004
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7142879207928975
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8313538789749145
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9487482825915019
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0657979806264242
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1834528684616088
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.3010805209477743
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
wandb: - 0.015 MB of 0.024 MB uploadedwandb: \ 0.015 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñà‚ñÇ‚ñÜ
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 2.92498
wandb:         Val Loss 3.99553
wandb:      train_batch 110
wandb: train_batch_loss 1.40783
wandb:        val_batch 30
wandb:   val_batch_loss 7.08177
wandb: 
wandb: üöÄ View run treasured-lake-288 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/tnss57q6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_233631-tnss57q6/logs
Seed completed execution! 1 0.6_2
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240510_233814-4xz772b2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-brook-290
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4xz772b2
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.012414141496022543
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.12936893304189045
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.2460294246673584
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.362693727016449
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.47993495464324953
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5977251410484314
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7148049275080363
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8316773692766826
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9489552736282348
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0661614259084067
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1833978056907655
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.3005906025568643
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
wandb: - 0.015 MB of 0.024 MB uploadedwandb: \ 0.015 MB of 0.024 MB uploadedwandb: | 0.024 MB of 0.024 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ
wandb:       Train Loss ‚ñÅ
wandb:         Val Loss ‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñà‚ñÇ‚ñÜ
wandb: 
wandb: Run summary:
wandb:            Epoch 0
wandb:       Train Loss 2.92498
wandb:         Val Loss 3.99553
wandb:      train_batch 110
wandb: train_batch_loss 1.40783
wandb:        val_batch 30
wandb:   val_batch_loss 7.08177
wandb: 
wandb: üöÄ View run mild-brook-290 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4xz772b2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240510_233814-4xz772b2/logs
Seed completed execution! 42 0.6_2
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_2
slurmstepd: error: *** JOB 1988330 ON gpu2001 CANCELLED AT 2024-05-10T23:39:58 ***
