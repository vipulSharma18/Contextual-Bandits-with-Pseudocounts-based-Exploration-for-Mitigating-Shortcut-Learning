## SLURM PROLOG ###############################################################
##    Job ID : 1988453
##  Job Name : first_half
##  Nodelist : gpu1401
##      CPUs : 1
##  Mem/Node : 10240 MB
## Directory : /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning
##   Job Started : Sat May 11 12:09:10 AM EDT 2024
###############################################################################
==========================================================================
Running experiment for setting 0.6_1
==========================================================================
Running for seed 1 of experiment 0.6_1
wandb: Currently logged in as: vipul. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_000918-13g93lhb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-pine-309
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/13g93lhb
/users/vsharm44/.conda/envs/dl_project/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.018272693951924643
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.1328747232755025
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24695885578791302
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3609718402226766
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.47552605072657267
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5903273940086364
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7045921127001444
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8196478486061096
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9340683579444885
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.04936021566391
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1644840200742086
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2793985525767009
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011600395043691
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.12657920519510904
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.24182501236597698
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.3571364720662435
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.47289034525553386
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5883616089820862
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7035420656204223
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8195076187451681
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9347819368044535
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.0501371582349142
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1658003648122153
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2812290787696838
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.011562653382619222
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.12712600231170654
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.24284157752990723
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.35864624977111814
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.47510584592819216
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.5908422787984212
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.7064911603927613
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.8226659814516704
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.9382935047149659
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0537925799687704
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.1693412105242411
Epoch 2, Batch 110, train loss:1.1252291202545166, Elapsed time for epoch : 1.2850650191307067
Batch 0, val loss:25.933643341064453
Batch 10, val loss:9.269792556762695
Batch 20, val loss:4.108797073364258
Batch 30, val loss:3.144083023071289
Epoch 2, Train Loss:1.1350151119024858, Val loss:12.079154133796692
Epoch 3, Batch 0, train loss:1.0006130933761597, Elapsed time for epoch : 0.01158130168914795
Epoch 3, Batch 10, train loss:1.040587306022644, Elapsed time for epoch : 0.1271284302075704
Epoch 3, Batch 20, train loss:1.0783007144927979, Elapsed time for epoch : 0.24274879693984985
Epoch 3, Batch 30, train loss:1.1385987997055054, Elapsed time for epoch : 0.3583958069483439
Epoch 3, Batch 40, train loss:1.008847713470459, Elapsed time for epoch : 0.47443803151448566
Epoch 3, Batch 50, train loss:0.8416311144828796, Elapsed time for epoch : 0.5902409036954244
Epoch 3, Batch 60, train loss:1.052704095840454, Elapsed time for epoch : 0.7059255282084147
Epoch 3, Batch 70, train loss:1.0969452857971191, Elapsed time for epoch : 0.8215637962023418
Epoch 3, Batch 80, train loss:1.0573866367340088, Elapsed time for epoch : 0.937279462814331
Epoch 3, Batch 90, train loss:1.022617220878601, Elapsed time for epoch : 1.053697947661082
Epoch 3, Batch 100, train loss:0.8345481157302856, Elapsed time for epoch : 1.169201668103536
Epoch 3, Batch 110, train loss:0.9623523950576782, Elapsed time for epoch : 1.2847864190737406
Batch 0, val loss:3.271911382675171
Batch 10, val loss:31.603137969970703
Batch 20, val loss:8.570819854736328
Batch 30, val loss:2.5292575359344482
Epoch 3, Train Loss:1.0117131481999937, Val loss:8.457930773496628
Epoch 4, Batch 0, train loss:0.9438236355781555, Elapsed time for epoch : 0.011563297112782795
Epoch 4, Batch 10, train loss:0.8099328279495239, Elapsed time for epoch : 0.12735906839370728
Epoch 4, Batch 20, train loss:0.8158267140388489, Elapsed time for epoch : 0.2433246652285258
Epoch 4, Batch 30, train loss:0.9238308668136597, Elapsed time for epoch : 0.35897072156270343
Epoch 4, Batch 40, train loss:0.9983869194984436, Elapsed time for epoch : 0.47450448671976725
Epoch 4, Batch 50, train loss:0.9389386773109436, Elapsed time for epoch : 0.5906185110410055
Epoch 4, Batch 60, train loss:1.0369113683700562, Elapsed time for epoch : 0.706497327486674
Epoch 4, Batch 70, train loss:0.926778256893158, Elapsed time for epoch : 0.8222357114156087
Epoch 4, Batch 80, train loss:0.8331975340843201, Elapsed time for epoch : 0.9386804064114889
Epoch 4, Batch 90, train loss:0.968751072883606, Elapsed time for epoch : 1.0544873118400573
Epoch 4, Batch 100, train loss:0.9753999710083008, Elapsed time for epoch : 1.1701951066652934
Epoch 4, Batch 110, train loss:0.9521971940994263, Elapsed time for epoch : 1.2864519198735556
Batch 0, val loss:8.002314567565918
Batch 10, val loss:2.2984747886657715
Batch 20, val loss:19.263139724731445
Batch 30, val loss:1.9474610090255737
Epoch 4, Train Loss:0.9391766739928205, Val loss:10.546193020211327
wandb: - 0.099 MB of 0.099 MB uploadedwandb: \ 0.099 MB of 0.099 MB uploadedwandb: | 0.099 MB of 0.099 MB uploadedwandb: / 0.161 MB of 0.161 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 2.3%             
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93918
wandb:         Val Loss 10.54619
wandb:      train_batch 110
wandb: train_batch_loss 0.9522
wandb:        val_batch 30
wandb:   val_batch_loss 1.94746
wandb: 
wandb: üöÄ View run olive-pine-309 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/13g93lhb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_000918-13g93lhb/logs
Seed completed execution! 1 0.6_1
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_001703-6yo2mmit
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-grass-311
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6yo2mmit
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.01339410146077474
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12849570910135905
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.2442306399345398
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.35978126525878906
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.4755298455556234
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5910649100939432
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7069368918736776
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8224396745363871
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9384549895922343
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0545404116312662
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.170165447394053
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.285841178894043
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011637334028879802
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.1273170550664266
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.2429729739824931
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.3585716446240743
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.4746150732040405
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5903962969779968
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7065153280893962
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8229764143625895
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9384670694669087
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.054379649957021
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1700376431147257
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2861276189486186
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.011578611532847087
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.12784370581309001
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.24364495277404785
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.35954148372014366
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.47631011803944906
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.5922445297241211
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.7079891522725423
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.824056073029836
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.9401605765024821
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0562376419703166
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.1723196546236674
Epoch 2, Batch 110, train loss:1.1252291202545166, Elapsed time for epoch : 1.2887883067131043
Batch 0, val loss:25.933643341064453
Batch 10, val loss:9.269792556762695
Batch 20, val loss:4.108797073364258
Batch 30, val loss:3.144083023071289
Epoch 2, Train Loss:1.1350151119024858, Val loss:12.079154133796692
Epoch 3, Batch 0, train loss:1.0006130933761597, Elapsed time for epoch : 0.01164023478825887
Epoch 3, Batch 10, train loss:1.040587306022644, Elapsed time for epoch : 0.12774380445480346
Epoch 3, Batch 20, train loss:1.0783007144927979, Elapsed time for epoch : 0.24350801706314087
Epoch 3, Batch 30, train loss:1.1385987997055054, Elapsed time for epoch : 0.35932284593582153
Epoch 3, Batch 40, train loss:1.008847713470459, Elapsed time for epoch : 0.4753976901372274
Epoch 3, Batch 50, train loss:0.8416311144828796, Elapsed time for epoch : 0.5913466930389404
Epoch 3, Batch 60, train loss:1.052704095840454, Elapsed time for epoch : 0.7069951931635539
Epoch 3, Batch 70, train loss:1.0969452857971191, Elapsed time for epoch : 0.8229372223218282
Epoch 3, Batch 80, train loss:1.0573866367340088, Elapsed time for epoch : 0.938460115591685
Epoch 3, Batch 90, train loss:1.022617220878601, Elapsed time for epoch : 1.0542322357495626
Epoch 3, Batch 100, train loss:0.8345481157302856, Elapsed time for epoch : 1.170507510503133
Epoch 3, Batch 110, train loss:0.9623523950576782, Elapsed time for epoch : 1.2862378438313802
Batch 0, val loss:3.271911382675171
Batch 10, val loss:31.603137969970703
Batch 20, val loss:8.570819854736328
Batch 30, val loss:2.5292575359344482
Epoch 3, Train Loss:1.0117131481999937, Val loss:8.457930773496628
Epoch 4, Batch 0, train loss:0.9438236355781555, Elapsed time for epoch : 0.011602604389190673
Epoch 4, Batch 10, train loss:0.8099328279495239, Elapsed time for epoch : 0.1277710477511088
Epoch 4, Batch 20, train loss:0.8158267140388489, Elapsed time for epoch : 0.2432857354482015
Epoch 4, Batch 30, train loss:0.9238308668136597, Elapsed time for epoch : 0.3593842109044393
Epoch 4, Batch 40, train loss:0.9983869194984436, Elapsed time for epoch : 0.47517184416453045
Epoch 4, Batch 50, train loss:0.9389386773109436, Elapsed time for epoch : 0.5908915519714355
Epoch 4, Batch 60, train loss:1.0369113683700562, Elapsed time for epoch : 0.7065832575162252
Epoch 4, Batch 70, train loss:0.926778256893158, Elapsed time for epoch : 0.8225564042727153
Epoch 4, Batch 80, train loss:0.8331975340843201, Elapsed time for epoch : 0.9385436058044434
Epoch 4, Batch 90, train loss:0.968751072883606, Elapsed time for epoch : 1.0542996247609457
Epoch 4, Batch 100, train loss:0.9753999710083008, Elapsed time for epoch : 1.170131770769755
Epoch 4, Batch 110, train loss:0.9521971940994263, Elapsed time for epoch : 1.285698135693868
Batch 0, val loss:8.002314567565918
Batch 10, val loss:2.2984747886657715
Batch 20, val loss:19.263139724731445
Batch 30, val loss:1.9474610090255737
Epoch 4, Train Loss:0.9391766739928205, Val loss:10.546193020211327
wandb: - 0.150 MB of 0.164 MB uploaded (0.004 MB deduped)wandb: \ 0.150 MB of 0.164 MB uploaded (0.004 MB deduped)wandb: | 0.164 MB of 0.164 MB uploaded (0.004 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 2.3%             
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93918
wandb:         Val Loss 10.54619
wandb:      train_batch 110
wandb: train_batch_loss 0.9522
wandb:        val_batch 30
wandb:   val_batch_loss 1.94746
wandb: 
wandb: üöÄ View run generous-grass-311 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6yo2mmit
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_001703-6yo2mmit/logs
Seed completed execution! 42 0.6_1
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_002448-ijrpl90l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-field-313
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ijrpl90l
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.013743066787719726
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.129191792011261
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24453593492507936
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3598028739293416
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.4753182609875997
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5907910585403442
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7067258477210998
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8221350153287251
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9380372524261474
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.053781525293986
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1694036801656087
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2854351957639059
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011628341674804688
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.1279130021731059
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.24365587631861368
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.35909146865208946
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.4754604975382487
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5910395701726278
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7067593852678935
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8223471721013387
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9378105878829956
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.053528650601705
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1692235191663107
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2851163069407145
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.011758474508921306
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.1275613784790039
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.243759818871816
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.3596652150154114
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.47540576855341593
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.5910733064015706
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.70716259876887
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.8229797840118408
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.9394586126009623
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0550293564796447
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.170684238274892
Epoch 2, Batch 110, train loss:1.1252291202545166, Elapsed time for epoch : 1.2863354722658793
Batch 0, val loss:25.933643341064453
Batch 10, val loss:9.269792556762695
Batch 20, val loss:4.108797073364258
Batch 30, val loss:3.144083023071289
Epoch 2, Train Loss:1.1350151119024858, Val loss:12.079154133796692
Epoch 3, Batch 0, train loss:1.0006130933761597, Elapsed time for epoch : 0.011671908696492513
Epoch 3, Batch 10, train loss:1.040587306022644, Elapsed time for epoch : 0.1270884871482849
Epoch 3, Batch 20, train loss:1.0783007144927979, Elapsed time for epoch : 0.24265517393747965
Epoch 3, Batch 30, train loss:1.1385987997055054, Elapsed time for epoch : 0.35835636456807457
Epoch 3, Batch 40, train loss:1.008847713470459, Elapsed time for epoch : 0.47432492176691693
Epoch 3, Batch 50, train loss:0.8416311144828796, Elapsed time for epoch : 0.5898672938346863
Epoch 3, Batch 60, train loss:1.052704095840454, Elapsed time for epoch : 0.7057692845662434
Epoch 3, Batch 70, train loss:1.0969452857971191, Elapsed time for epoch : 0.8214841365814209
Epoch 3, Batch 80, train loss:1.0573866367340088, Elapsed time for epoch : 0.9376909613609314
Epoch 3, Batch 90, train loss:1.022617220878601, Elapsed time for epoch : 1.053557562828064
Epoch 3, Batch 100, train loss:0.8345481157302856, Elapsed time for epoch : 1.16953049103419
Epoch 3, Batch 110, train loss:0.9623523950576782, Elapsed time for epoch : 1.285939637819926
Batch 0, val loss:3.271911382675171
Batch 10, val loss:31.603137969970703
Batch 20, val loss:8.570819854736328
Batch 30, val loss:2.5292575359344482
Epoch 3, Train Loss:1.0117131481999937, Val loss:8.457930773496628
Epoch 4, Batch 0, train loss:0.9438236355781555, Elapsed time for epoch : 0.011627535025278727
Epoch 4, Batch 10, train loss:0.8099328279495239, Elapsed time for epoch : 0.127629550298055
Epoch 4, Batch 20, train loss:0.8158267140388489, Elapsed time for epoch : 0.24383869568506877
Epoch 4, Batch 30, train loss:0.9238308668136597, Elapsed time for epoch : 0.35956680377324424
Epoch 4, Batch 40, train loss:0.9983869194984436, Elapsed time for epoch : 0.4756776452064514
Epoch 4, Batch 50, train loss:0.9389386773109436, Elapsed time for epoch : 0.5919915676116944
Epoch 4, Batch 60, train loss:1.0369113683700562, Elapsed time for epoch : 0.7078802347183227
Epoch 4, Batch 70, train loss:0.926778256893158, Elapsed time for epoch : 0.8236766576766967
Epoch 4, Batch 80, train loss:0.8331975340843201, Elapsed time for epoch : 0.9394619544347127
Epoch 4, Batch 90, train loss:0.968751072883606, Elapsed time for epoch : 1.0554035663604737
Epoch 4, Batch 100, train loss:0.9753999710083008, Elapsed time for epoch : 1.1714930295944215
Epoch 4, Batch 110, train loss:0.9521971940994263, Elapsed time for epoch : 1.2875964164733886
Batch 0, val loss:8.002314567565918
Batch 10, val loss:2.2984747886657715
Batch 20, val loss:19.263139724731445
Batch 30, val loss:1.9474610090255737
Epoch 4, Train Loss:0.9391766739928205, Val loss:10.546193020211327
wandb: - 0.116 MB of 0.116 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93918
wandb:         Val Loss 10.54619
wandb:      train_batch 110
wandb: train_batch_loss 0.9522
wandb:        val_batch 30
wandb:   val_batch_loss 1.94746
wandb: 
wandb: üöÄ View run still-field-313 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ijrpl90l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_002448-ijrpl90l/logs
Seed completed execution! 89 0.6_1
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_003232-aw2j3fc0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-planet-315
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/aw2j3fc0
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.013453229268391927
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12886322736740113
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24454257488250733
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3601682662963867
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.4757900396982829
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5912335356076558
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.706686774889628
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.822094186147054
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9377647995948791
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0536020239194235
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1695563038190206
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2854796210924784
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011818134784698486
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.12757965326309204
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.24308687845865887
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.3588922142982483
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.47482142051060994
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5906726320584615
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7066333015759786
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8224431475003561
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9385440826416016
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.054366401831309
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1701183994611104
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2861396511395773
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.01164772907892863
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.12778899669647217
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.2434588313102722
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.3592553655306498
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.4752706209818522
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.5911312421162923
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.7070744117101033
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.8231749494870504
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.9388032595316569
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0545944889386496
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.1702544252077738
Epoch 2, Batch 110, train loss:1.1252291202545166, Elapsed time for epoch : 1.285979199409485
Batch 0, val loss:25.933643341064453
Batch 10, val loss:9.269792556762695
Batch 20, val loss:4.108797073364258
Batch 30, val loss:3.144083023071289
Epoch 2, Train Loss:1.1350151119024858, Val loss:12.079154133796692
Epoch 3, Batch 0, train loss:1.0006130933761597, Elapsed time for epoch : 0.011587405204772949
Epoch 3, Batch 10, train loss:1.040587306022644, Elapsed time for epoch : 0.12742332220077515
Epoch 3, Batch 20, train loss:1.0783007144927979, Elapsed time for epoch : 0.24310413201649983
Epoch 3, Batch 30, train loss:1.1385987997055054, Elapsed time for epoch : 0.3586240172386169
Epoch 3, Batch 40, train loss:1.008847713470459, Elapsed time for epoch : 0.47474581797917687
Epoch 3, Batch 50, train loss:0.8416311144828796, Elapsed time for epoch : 0.5908218224843343
Epoch 3, Batch 60, train loss:1.052704095840454, Elapsed time for epoch : 0.7070245742797852
Epoch 3, Batch 70, train loss:1.0969452857971191, Elapsed time for epoch : 0.822923195362091
Epoch 3, Batch 80, train loss:1.0573866367340088, Elapsed time for epoch : 0.9385546962420146
Epoch 3, Batch 90, train loss:1.022617220878601, Elapsed time for epoch : 1.0542407830556233
Epoch 3, Batch 100, train loss:0.8345481157302856, Elapsed time for epoch : 1.169672969977061
Epoch 3, Batch 110, train loss:0.9623523950576782, Elapsed time for epoch : 1.2858606139818827
Batch 0, val loss:3.271911382675171
Batch 10, val loss:31.603137969970703
Batch 20, val loss:8.570819854736328
Batch 30, val loss:2.5292575359344482
Epoch 3, Train Loss:1.0117131481999937, Val loss:8.457930773496628
Epoch 4, Batch 0, train loss:0.9438236355781555, Elapsed time for epoch : 0.011617318789164225
Epoch 4, Batch 10, train loss:0.8099328279495239, Elapsed time for epoch : 0.1275342623392741
Epoch 4, Batch 20, train loss:0.8158267140388489, Elapsed time for epoch : 0.2435393730799357
Epoch 4, Batch 30, train loss:0.9238308668136597, Elapsed time for epoch : 0.3592228372891744
Epoch 4, Batch 40, train loss:0.9983869194984436, Elapsed time for epoch : 0.47488611141840614
Epoch 4, Batch 50, train loss:0.9389386773109436, Elapsed time for epoch : 0.591181739171346
Epoch 4, Batch 60, train loss:1.0369113683700562, Elapsed time for epoch : 0.7072687745094299
Epoch 4, Batch 70, train loss:0.926778256893158, Elapsed time for epoch : 0.822834599018097
Epoch 4, Batch 80, train loss:0.8331975340843201, Elapsed time for epoch : 0.9391160726547241
Epoch 4, Batch 90, train loss:0.968751072883606, Elapsed time for epoch : 1.0545743187268575
Epoch 4, Batch 100, train loss:0.9753999710083008, Elapsed time for epoch : 1.170491890112559
Epoch 4, Batch 110, train loss:0.9521971940994263, Elapsed time for epoch : 1.2863271156946818
Batch 0, val loss:8.002314567565918
Batch 10, val loss:2.2984747886657715
Batch 20, val loss:19.263139724731445
Batch 30, val loss:1.9474610090255737
Epoch 4, Train Loss:0.9391766739928205, Val loss:10.546193020211327
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93918
wandb:         Val Loss 10.54619
wandb:      train_batch 110
wandb: train_batch_loss 0.9522
wandb:        val_batch 30
wandb:   val_batch_loss 1.94746
wandb: 
wandb: üöÄ View run sleek-planet-315 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/aw2j3fc0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_003232-aw2j3fc0/logs
Seed completed execution! 23 0.6_1
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_004016-xjekmdsp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-river-317
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xjekmdsp
Epoch 0, Batch 0, train loss:7.749749183654785, Elapsed time for epoch : 0.013408883412679037
Epoch 0, Batch 10, train loss:2.8786063194274902, Elapsed time for epoch : 0.12921544710795085
Epoch 0, Batch 20, train loss:2.7323262691497803, Elapsed time for epoch : 0.24527692000071208
Epoch 0, Batch 30, train loss:2.5986077785491943, Elapsed time for epoch : 0.3613252639770508
Epoch 0, Batch 40, train loss:2.152283191680908, Elapsed time for epoch : 0.477452286084493
Epoch 0, Batch 50, train loss:2.00974702835083, Elapsed time for epoch : 0.5935795942942301
Epoch 0, Batch 60, train loss:2.023862838745117, Elapsed time for epoch : 0.7099804441134135
Epoch 0, Batch 70, train loss:1.9585319757461548, Elapsed time for epoch : 0.8260416070620219
Epoch 0, Batch 80, train loss:1.7858167886734009, Elapsed time for epoch : 0.9425767540931702
Epoch 0, Batch 90, train loss:1.7713773250579834, Elapsed time for epoch : 1.0588709950447082
Epoch 0, Batch 100, train loss:1.7235767841339111, Elapsed time for epoch : 1.1747760931650797
Epoch 0, Batch 110, train loss:1.5021312236785889, Elapsed time for epoch : 1.2910967866579692
Batch 0, val loss:5.606189727783203
Batch 10, val loss:4.975593090057373
Batch 20, val loss:3.281553030014038
Batch 30, val loss:8.310664176940918
Epoch 0, Train Loss:3.063380724450816, Val loss:3.6622612675031028
Epoch 1, Batch 0, train loss:1.6213748455047607, Elapsed time for epoch : 0.011645801862080892
Epoch 1, Batch 10, train loss:1.5649408102035522, Elapsed time for epoch : 0.1277845581372579
Epoch 1, Batch 20, train loss:1.4902235269546509, Elapsed time for epoch : 0.24346868991851806
Epoch 1, Batch 30, train loss:1.6148563623428345, Elapsed time for epoch : 0.35911279122034706
Epoch 1, Batch 40, train loss:1.498252034187317, Elapsed time for epoch : 0.4748201449712118
Epoch 1, Batch 50, train loss:1.4297288656234741, Elapsed time for epoch : 0.5907602190971375
Epoch 1, Batch 60, train loss:1.3938734531402588, Elapsed time for epoch : 0.7064837733904521
Epoch 1, Batch 70, train loss:1.3915188312530518, Elapsed time for epoch : 0.8223399559656779
Epoch 1, Batch 80, train loss:1.3790024518966675, Elapsed time for epoch : 0.9381923238436382
Epoch 1, Batch 90, train loss:1.3224562406539917, Elapsed time for epoch : 1.0540900230407715
Epoch 1, Batch 100, train loss:1.116538166999817, Elapsed time for epoch : 1.1702361742655436
Epoch 1, Batch 110, train loss:1.2389240264892578, Elapsed time for epoch : 1.2860066811243693
Batch 0, val loss:2.394777774810791
Batch 10, val loss:19.06544303894043
Batch 20, val loss:2.956608533859253
Batch 30, val loss:2.7645397186279297
Epoch 1, Train Loss:1.4318218894626784, Val loss:5.813211020496157
Epoch 2, Batch 0, train loss:1.1983665227890015, Elapsed time for epoch : 0.011580002307891846
Epoch 2, Batch 10, train loss:1.1901637315750122, Elapsed time for epoch : 0.1273573160171509
Epoch 2, Batch 20, train loss:1.2106598615646362, Elapsed time for epoch : 0.24304164250691732
Epoch 2, Batch 30, train loss:1.187260627746582, Elapsed time for epoch : 0.3587694485982259
Epoch 2, Batch 40, train loss:1.1398866176605225, Elapsed time for epoch : 0.47472450335820515
Epoch 2, Batch 50, train loss:1.168094277381897, Elapsed time for epoch : 0.5905850092569987
Epoch 2, Batch 60, train loss:1.0461105108261108, Elapsed time for epoch : 0.7065077424049377
Epoch 2, Batch 70, train loss:1.0999122858047485, Elapsed time for epoch : 0.8225229342778524
Epoch 2, Batch 80, train loss:1.1805543899536133, Elapsed time for epoch : 0.9385476350784302
Epoch 2, Batch 90, train loss:1.0766286849975586, Elapsed time for epoch : 1.0546966433525085
Epoch 2, Batch 100, train loss:1.0625262260437012, Elapsed time for epoch : 1.1703227917353312
Epoch 2, Batch 110, train loss:1.1252291202545166, Elapsed time for epoch : 1.2861271540323893
Batch 0, val loss:25.933643341064453
Batch 10, val loss:9.269792556762695
Batch 20, val loss:4.108797073364258
Batch 30, val loss:3.144083023071289
Epoch 2, Train Loss:1.1350151119024858, Val loss:12.079154133796692
Epoch 3, Batch 0, train loss:1.0006130933761597, Elapsed time for epoch : 0.01163793404897054
Epoch 3, Batch 10, train loss:1.040587306022644, Elapsed time for epoch : 0.12734386523564656
Epoch 3, Batch 20, train loss:1.0783007144927979, Elapsed time for epoch : 0.24301343361536662
Epoch 3, Batch 30, train loss:1.1385987997055054, Elapsed time for epoch : 0.3585924585660299
Epoch 3, Batch 40, train loss:1.008847713470459, Elapsed time for epoch : 0.4747353394826253
Epoch 3, Batch 50, train loss:0.8416311144828796, Elapsed time for epoch : 0.5904314994812012
Epoch 3, Batch 60, train loss:1.052704095840454, Elapsed time for epoch : 0.706360912322998
Epoch 3, Batch 70, train loss:1.0969452857971191, Elapsed time for epoch : 0.8226169069608052
Epoch 3, Batch 80, train loss:1.0573866367340088, Elapsed time for epoch : 0.9382337013880412
Epoch 3, Batch 90, train loss:1.022617220878601, Elapsed time for epoch : 1.0542381207148235
Epoch 3, Batch 100, train loss:0.8345481157302856, Elapsed time for epoch : 1.1697485367457072
Epoch 3, Batch 110, train loss:0.9623523950576782, Elapsed time for epoch : 1.2856251756350199
Batch 0, val loss:3.271911382675171
Batch 10, val loss:31.603137969970703
Batch 20, val loss:8.570819854736328
Batch 30, val loss:2.5292575359344482
Epoch 3, Train Loss:1.0117131481999937, Val loss:8.457930773496628
Epoch 4, Batch 0, train loss:0.9438236355781555, Elapsed time for epoch : 0.01158436139424642
Epoch 4, Batch 10, train loss:0.8099328279495239, Elapsed time for epoch : 0.1279813289642334
Epoch 4, Batch 20, train loss:0.8158267140388489, Elapsed time for epoch : 0.24403227965037028
Epoch 4, Batch 30, train loss:0.9238308668136597, Elapsed time for epoch : 0.3600866675376892
Epoch 4, Batch 40, train loss:0.9983869194984436, Elapsed time for epoch : 0.47563360929489135
Epoch 4, Batch 50, train loss:0.9389386773109436, Elapsed time for epoch : 0.5918738166491191
Epoch 4, Batch 60, train loss:1.0369113683700562, Elapsed time for epoch : 0.7076678077379862
Epoch 4, Batch 70, train loss:0.926778256893158, Elapsed time for epoch : 0.8236398180325826
Epoch 4, Batch 80, train loss:0.8331975340843201, Elapsed time for epoch : 0.9397924343744913
Epoch 4, Batch 90, train loss:0.968751072883606, Elapsed time for epoch : 1.0556736588478088
Epoch 4, Batch 100, train loss:0.9753999710083008, Elapsed time for epoch : 1.171692399183909
Epoch 4, Batch 110, train loss:0.9521971940994263, Elapsed time for epoch : 1.2877448558807374
Batch 0, val loss:8.002314567565918
Batch 10, val loss:2.2984747886657715
Batch 20, val loss:19.263139724731445
Batch 30, val loss:1.9474610090255737
Epoch 4, Train Loss:0.9391766739928205, Val loss:10.546193020211327
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÖ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93918
wandb:         Val Loss 10.54619
wandb:      train_batch 110
wandb: train_batch_loss 0.9522
wandb:        val_batch 30
wandb:   val_batch_loss 1.94746
wandb: 
wandb: üöÄ View run charmed-river-317 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xjekmdsp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_004016-xjekmdsp/logs
Seed completed execution! 113 0.6_1
------------------------------------------------------------------
Experiment complete 0.6_1
==========================================================================
Running experiment for setting 0.6_2
==========================================================================
Running for seed 1 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_004802-baatc14l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-water-319
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/baatc14l
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.012476623058319092
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.12822799682617186
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.24396894772847494
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.35959213177363075
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.47587868372599285
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5918209274609884
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7074352661768596
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8231157660484314
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9391232252120971
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0547593514124551
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1705329934755961
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.2863242864608764
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
Epoch 1, Batch 0, train loss:1.5519860982894897, Elapsed time for epoch : 0.011742615699768066
Epoch 1, Batch 10, train loss:1.4443174600601196, Elapsed time for epoch : 0.1273315747578939
Epoch 1, Batch 20, train loss:1.2092701196670532, Elapsed time for epoch : 0.24340625603993735
Epoch 1, Batch 30, train loss:1.512547492980957, Elapsed time for epoch : 0.3594129721323649
Epoch 1, Batch 40, train loss:1.2831720113754272, Elapsed time for epoch : 0.47527757883071897
Epoch 1, Batch 50, train loss:1.2865995168685913, Elapsed time for epoch : 0.5911852757136027
Epoch 1, Batch 60, train loss:1.2534452676773071, Elapsed time for epoch : 0.7072333693504333
Epoch 1, Batch 70, train loss:1.2001628875732422, Elapsed time for epoch : 0.8229905009269715
Epoch 1, Batch 80, train loss:1.1923569440841675, Elapsed time for epoch : 0.9392478148142497
Epoch 1, Batch 90, train loss:1.1845539808273315, Elapsed time for epoch : 1.0553513884544372
Epoch 1, Batch 100, train loss:0.8761093020439148, Elapsed time for epoch : 1.1712730924288433
Epoch 1, Batch 110, train loss:1.0176893472671509, Elapsed time for epoch : 1.2874445597330728
Batch 0, val loss:2.3120594024658203
Batch 10, val loss:3.1623895168304443
Batch 20, val loss:2.1613471508026123
Batch 30, val loss:1.9537602663040161
Epoch 1, Train Loss:1.236130588469298, Val loss:4.995238833957249
Epoch 2, Batch 0, train loss:1.0287134647369385, Elapsed time for epoch : 0.01172255277633667
Epoch 2, Batch 10, train loss:0.9540262222290039, Elapsed time for epoch : 0.12762280305226645
Epoch 2, Batch 20, train loss:1.0608279705047607, Elapsed time for epoch : 0.24363916317621867
Epoch 2, Batch 30, train loss:0.9660180807113647, Elapsed time for epoch : 0.35986011823018393
Epoch 2, Batch 40, train loss:0.9426432847976685, Elapsed time for epoch : 0.4762264251708984
Epoch 2, Batch 50, train loss:1.0277059078216553, Elapsed time for epoch : 0.5919153730074564
Epoch 2, Batch 60, train loss:0.9092874526977539, Elapsed time for epoch : 0.7082147399584452
Epoch 2, Batch 70, train loss:0.8038166761398315, Elapsed time for epoch : 0.8242496093114217
Epoch 2, Batch 80, train loss:1.0482909679412842, Elapsed time for epoch : 0.9409337202707927
Epoch 2, Batch 90, train loss:0.8918736577033997, Elapsed time for epoch : 1.0575968305269876
Epoch 2, Batch 100, train loss:0.9361854195594788, Elapsed time for epoch : 1.1738855679829916
Epoch 2, Batch 110, train loss:0.9204474091529846, Elapsed time for epoch : 1.28973392645518
Batch 0, val loss:1.9127575159072876
Batch 10, val loss:4.382065296173096
Batch 20, val loss:4.30318546295166
Batch 30, val loss:6.023943901062012
Epoch 2, Train Loss:0.9587588569392329, Val loss:5.537418540981081
Epoch 3, Batch 0, train loss:0.9114006757736206, Elapsed time for epoch : 0.01160660187403361
Epoch 3, Batch 10, train loss:0.8783732652664185, Elapsed time for epoch : 0.12783515453338623
Epoch 3, Batch 20, train loss:0.8587350845336914, Elapsed time for epoch : 0.24410020112991332
Epoch 3, Batch 30, train loss:0.9166836142539978, Elapsed time for epoch : 0.35977062384287517
Epoch 3, Batch 40, train loss:0.926134467124939, Elapsed time for epoch : 0.47591814200083415
Epoch 3, Batch 50, train loss:0.6085221171379089, Elapsed time for epoch : 0.5918617010116577
Epoch 3, Batch 60, train loss:0.792026937007904, Elapsed time for epoch : 0.707879900932312
Epoch 3, Batch 70, train loss:0.9118708372116089, Elapsed time for epoch : 0.8237634181976319
Epoch 3, Batch 80, train loss:0.9081965684890747, Elapsed time for epoch : 0.9397428711255391
Epoch 3, Batch 90, train loss:0.8007227182388306, Elapsed time for epoch : 1.0562229792277018
Epoch 3, Batch 100, train loss:0.5062412023544312, Elapsed time for epoch : 1.1721397757530212
Epoch 3, Batch 110, train loss:0.7457603812217712, Elapsed time for epoch : 1.2881370345751444
Batch 0, val loss:6.3070502281188965
Batch 10, val loss:3.7878639698028564
Batch 20, val loss:2.84297251701355
Batch 30, val loss:3.294538974761963
Epoch 3, Train Loss:0.8090542648149573, Val loss:6.165570101804203
Epoch 4, Batch 0, train loss:0.8745941519737244, Elapsed time for epoch : 0.011701583862304688
Epoch 4, Batch 10, train loss:0.5393694043159485, Elapsed time for epoch : 0.12765936056772867
Epoch 4, Batch 20, train loss:0.4861161708831787, Elapsed time for epoch : 0.2438366413116455
Epoch 4, Batch 30, train loss:0.6779271960258484, Elapsed time for epoch : 0.35968802372614544
Epoch 4, Batch 40, train loss:0.7431595921516418, Elapsed time for epoch : 0.4756193161010742
Epoch 4, Batch 50, train loss:0.7025474905967712, Elapsed time for epoch : 0.5915730714797973
Epoch 4, Batch 60, train loss:0.653239905834198, Elapsed time for epoch : 0.7073938886324564
Epoch 4, Batch 70, train loss:0.6857838034629822, Elapsed time for epoch : 0.8238751490910848
Epoch 4, Batch 80, train loss:0.4984578490257263, Elapsed time for epoch : 0.9398655931154887
Epoch 4, Batch 90, train loss:0.6572631001472473, Elapsed time for epoch : 1.0557090719540914
Epoch 4, Batch 100, train loss:0.6392361521720886, Elapsed time for epoch : 1.1717607180277507
Epoch 4, Batch 110, train loss:0.642008364200592, Elapsed time for epoch : 1.2879469593365986
Batch 0, val loss:1.1679452657699585
Batch 10, val loss:1.6427408456802368
Batch 20, val loss:16.940135955810547
Batch 30, val loss:4.5015058517456055
Epoch 4, Train Loss:0.6327615999657175, Val loss:5.86206898258792
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.63276
wandb:         Val Loss 5.86207
wandb:      train_batch 110
wandb: train_batch_loss 0.64201
wandb:        val_batch 30
wandb:   val_batch_loss 4.50151
wandb: 
wandb: üöÄ View run polished-water-319 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/baatc14l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_004802-baatc14l/logs
Seed completed execution! 1 0.6_2
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_005548-992z9k48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-water-321
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/992z9k48
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.013280379772186279
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.12840818961461384
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.24420409599939982
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.3601553519566854
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.4757511893908183
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.591520611445109
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7075369993845622
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8235790054003398
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9396021723747253
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0555923501650493
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1714872678120931
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.2881355086962383
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
Epoch 1, Batch 0, train loss:1.5519860982894897, Elapsed time for epoch : 0.011660786469777425
Epoch 1, Batch 10, train loss:1.4443174600601196, Elapsed time for epoch : 0.12724068959554036
Epoch 1, Batch 20, train loss:1.2092701196670532, Elapsed time for epoch : 0.2434746265411377
Epoch 1, Batch 30, train loss:1.512547492980957, Elapsed time for epoch : 0.3593662579854329
Epoch 1, Batch 40, train loss:1.2831720113754272, Elapsed time for epoch : 0.47503923972447715
Epoch 1, Batch 50, train loss:1.2865995168685913, Elapsed time for epoch : 0.5909660220146179
Epoch 1, Batch 60, train loss:1.2534452676773071, Elapsed time for epoch : 0.7068661173184713
Epoch 1, Batch 70, train loss:1.2001628875732422, Elapsed time for epoch : 0.82320237159729
Epoch 1, Batch 80, train loss:1.1923569440841675, Elapsed time for epoch : 0.9397296269734701
Epoch 1, Batch 90, train loss:1.1845539808273315, Elapsed time for epoch : 1.0559946854909261
Epoch 1, Batch 100, train loss:0.8761093020439148, Elapsed time for epoch : 1.1714616338411967
Epoch 1, Batch 110, train loss:1.0176893472671509, Elapsed time for epoch : 1.2878071586290996
Batch 0, val loss:2.3120594024658203
Batch 10, val loss:3.1623895168304443
Batch 20, val loss:2.1613471508026123
Batch 30, val loss:1.9537602663040161
Epoch 1, Train Loss:1.236130588469298, Val loss:4.995238833957249
Epoch 2, Batch 0, train loss:1.0287134647369385, Elapsed time for epoch : 0.011681671937306721
Epoch 2, Batch 10, train loss:0.9540262222290039, Elapsed time for epoch : 0.12791427373886108
Epoch 2, Batch 20, train loss:1.0608279705047607, Elapsed time for epoch : 0.24392038186391193
Epoch 2, Batch 30, train loss:0.9660180807113647, Elapsed time for epoch : 0.35948839982350667
Epoch 2, Batch 40, train loss:0.9426432847976685, Elapsed time for epoch : 0.47638004223505653
Epoch 2, Batch 50, train loss:1.0277059078216553, Elapsed time for epoch : 0.5928401668866475
Epoch 2, Batch 60, train loss:0.9092874526977539, Elapsed time for epoch : 0.7085442264874776
Epoch 2, Batch 70, train loss:0.8038166761398315, Elapsed time for epoch : 0.8245172937711079
Epoch 2, Batch 80, train loss:1.0482909679412842, Elapsed time for epoch : 0.9403030832608541
Epoch 2, Batch 90, train loss:0.8918736577033997, Elapsed time for epoch : 1.0562705477078755
Epoch 2, Batch 100, train loss:0.9361854195594788, Elapsed time for epoch : 1.172464875380198
Epoch 2, Batch 110, train loss:0.9204474091529846, Elapsed time for epoch : 1.288508371512095
Batch 0, val loss:1.9127575159072876
Batch 10, val loss:4.382065296173096
Batch 20, val loss:4.30318546295166
Batch 30, val loss:6.023943901062012
Epoch 2, Train Loss:0.9587588569392329, Val loss:5.537418540981081
Epoch 3, Batch 0, train loss:0.9114006757736206, Elapsed time for epoch : 0.01183708906173706
Epoch 3, Batch 10, train loss:0.8783732652664185, Elapsed time for epoch : 0.1276274561882019
Epoch 3, Batch 20, train loss:0.8587350845336914, Elapsed time for epoch : 0.24374805291493734
Epoch 3, Batch 30, train loss:0.9166836142539978, Elapsed time for epoch : 0.3598384221394857
Epoch 3, Batch 40, train loss:0.926134467124939, Elapsed time for epoch : 0.475636617342631
Epoch 3, Batch 50, train loss:0.6085221171379089, Elapsed time for epoch : 0.5914662837982178
Epoch 3, Batch 60, train loss:0.792026937007904, Elapsed time for epoch : 0.707328486442566
Epoch 3, Batch 70, train loss:0.9118708372116089, Elapsed time for epoch : 0.823340364297231
Epoch 3, Batch 80, train loss:0.9081965684890747, Elapsed time for epoch : 0.9395432511965434
Epoch 3, Batch 90, train loss:0.8007227182388306, Elapsed time for epoch : 1.0553829034169515
Epoch 3, Batch 100, train loss:0.5062412023544312, Elapsed time for epoch : 1.1713635484377543
Epoch 3, Batch 110, train loss:0.7457603812217712, Elapsed time for epoch : 1.2872849106788635
Batch 0, val loss:6.3070502281188965
Batch 10, val loss:3.7878639698028564
Batch 20, val loss:2.84297251701355
Batch 30, val loss:3.294538974761963
Epoch 3, Train Loss:0.8090542648149573, Val loss:6.165570101804203
Epoch 4, Batch 0, train loss:0.8745941519737244, Elapsed time for epoch : 0.01173696517944336
Epoch 4, Batch 10, train loss:0.5393694043159485, Elapsed time for epoch : 0.12767176628112792
Epoch 4, Batch 20, train loss:0.4861161708831787, Elapsed time for epoch : 0.2438673496246338
Epoch 4, Batch 30, train loss:0.6779271960258484, Elapsed time for epoch : 0.35953922271728517
Epoch 4, Batch 40, train loss:0.7431595921516418, Elapsed time for epoch : 0.47560126781463624
Epoch 4, Batch 50, train loss:0.7025474905967712, Elapsed time for epoch : 0.5919329206148783
Epoch 4, Batch 60, train loss:0.653239905834198, Elapsed time for epoch : 0.707625154654185
Epoch 4, Batch 70, train loss:0.6857838034629822, Elapsed time for epoch : 0.8233443776766459
Epoch 4, Batch 80, train loss:0.4984578490257263, Elapsed time for epoch : 0.9390333970387776
Epoch 4, Batch 90, train loss:0.6572631001472473, Elapsed time for epoch : 1.054688843091329
Epoch 4, Batch 100, train loss:0.6392361521720886, Elapsed time for epoch : 1.1706619262695312
Epoch 4, Batch 110, train loss:0.642008364200592, Elapsed time for epoch : 1.2867847561836243
Batch 0, val loss:1.1679452657699585
Batch 10, val loss:1.6427408456802368
Batch 20, val loss:16.940135955810547
Batch 30, val loss:4.5015058517456055
Epoch 4, Train Loss:0.6327615999657175, Val loss:5.86206898258792
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.63276
wandb:         Val Loss 5.86207
wandb:      train_batch 110
wandb: train_batch_loss 0.64201
wandb:        val_batch 30
wandb:   val_batch_loss 4.50151
wandb: 
wandb: üöÄ View run effortless-water-321 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/992z9k48
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_005548-992z9k48/logs
Seed completed execution! 42 0.6_2
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_010334-ehp9ff2y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-waterfall-323
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ehp9ff2y
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.012337728341420492
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.1277714729309082
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.2433736205101013
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.3590691049893697
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.4753846565882365
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5911487142244974
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.706919276714325
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8230355858802796
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9389204382896423
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0547125021616617
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1704708735148113
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.2862492601076763
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
Epoch 1, Batch 0, train loss:1.5519860982894897, Elapsed time for epoch : 0.011615339914957683
Epoch 1, Batch 10, train loss:1.4443174600601196, Elapsed time for epoch : 0.1274365743001302
Epoch 1, Batch 20, train loss:1.2092701196670532, Elapsed time for epoch : 0.2435683012008667
Epoch 1, Batch 30, train loss:1.512547492980957, Elapsed time for epoch : 0.3593650658925374
Epoch 1, Batch 40, train loss:1.2831720113754272, Elapsed time for epoch : 0.4756268342336019
Epoch 1, Batch 50, train loss:1.2865995168685913, Elapsed time for epoch : 0.5917982260386149
Epoch 1, Batch 60, train loss:1.2534452676773071, Elapsed time for epoch : 0.7077051758766174
Epoch 1, Batch 70, train loss:1.2001628875732422, Elapsed time for epoch : 0.823635458946228
Epoch 1, Batch 80, train loss:1.1923569440841675, Elapsed time for epoch : 0.9404128273328145
Epoch 1, Batch 90, train loss:1.1845539808273315, Elapsed time for epoch : 1.056102422873179
Epoch 1, Batch 100, train loss:0.8761093020439148, Elapsed time for epoch : 1.1721731265385946
Epoch 1, Batch 110, train loss:1.0176893472671509, Elapsed time for epoch : 1.2880327741305033
Batch 0, val loss:2.3120594024658203
Batch 10, val loss:3.1623895168304443
Batch 20, val loss:2.1613471508026123
Batch 30, val loss:1.9537602663040161
Epoch 1, Train Loss:1.236130588469298, Val loss:4.995238833957249
Epoch 2, Batch 0, train loss:1.0287134647369385, Elapsed time for epoch : 0.011716326077779135
Epoch 2, Batch 10, train loss:0.9540262222290039, Elapsed time for epoch : 0.12770802179972332
Epoch 2, Batch 20, train loss:1.0608279705047607, Elapsed time for epoch : 0.24376311699549358
Epoch 2, Batch 30, train loss:0.9660180807113647, Elapsed time for epoch : 0.35989831686019896
Epoch 2, Batch 40, train loss:0.9426432847976685, Elapsed time for epoch : 0.47605892419815066
Epoch 2, Batch 50, train loss:1.0277059078216553, Elapsed time for epoch : 0.5921008308728536
Epoch 2, Batch 60, train loss:0.9092874526977539, Elapsed time for epoch : 0.7081866502761841
Epoch 2, Batch 70, train loss:0.8038166761398315, Elapsed time for epoch : 0.824072269598643
Epoch 2, Batch 80, train loss:1.0482909679412842, Elapsed time for epoch : 0.939941136042277
Epoch 2, Batch 90, train loss:0.8918736577033997, Elapsed time for epoch : 1.0558862924575805
Epoch 2, Batch 100, train loss:0.9361854195594788, Elapsed time for epoch : 1.1722339153289796
Epoch 2, Batch 110, train loss:0.9204474091529846, Elapsed time for epoch : 1.2880415876706441
Batch 0, val loss:1.9127575159072876
Batch 10, val loss:4.382065296173096
Batch 20, val loss:4.30318546295166
Batch 30, val loss:6.023943901062012
Epoch 2, Train Loss:0.9587588569392329, Val loss:5.537418540981081
Epoch 3, Batch 0, train loss:0.9114006757736206, Elapsed time for epoch : 0.01171636978785197
Epoch 3, Batch 10, train loss:0.8783732652664185, Elapsed time for epoch : 0.12754125595092775
Epoch 3, Batch 20, train loss:0.8587350845336914, Elapsed time for epoch : 0.24385212659835814
Epoch 3, Batch 30, train loss:0.9166836142539978, Elapsed time for epoch : 0.3596747080485026
Epoch 3, Batch 40, train loss:0.926134467124939, Elapsed time for epoch : 0.4755304376284281
Epoch 3, Batch 50, train loss:0.6085221171379089, Elapsed time for epoch : 0.591463041305542
Epoch 3, Batch 60, train loss:0.792026937007904, Elapsed time for epoch : 0.7075019796689351
Epoch 3, Batch 70, train loss:0.9118708372116089, Elapsed time for epoch : 0.8232447028160095
Epoch 3, Batch 80, train loss:0.9081965684890747, Elapsed time for epoch : 0.9392890135447184
Epoch 3, Batch 90, train loss:0.8007227182388306, Elapsed time for epoch : 1.0556343237559
Epoch 3, Batch 100, train loss:0.5062412023544312, Elapsed time for epoch : 1.1714895606040954
Epoch 3, Batch 110, train loss:0.7457603812217712, Elapsed time for epoch : 1.2873183687527974
Batch 0, val loss:6.3070502281188965
Batch 10, val loss:3.7878639698028564
Batch 20, val loss:2.84297251701355
Batch 30, val loss:3.294538974761963
Epoch 3, Train Loss:0.8090542648149573, Val loss:6.165570101804203
Epoch 4, Batch 0, train loss:0.8745941519737244, Elapsed time for epoch : 0.011585307121276856
Epoch 4, Batch 10, train loss:0.5393694043159485, Elapsed time for epoch : 0.12753175099690756
Epoch 4, Batch 20, train loss:0.4861161708831787, Elapsed time for epoch : 0.2432401140530904
Epoch 4, Batch 30, train loss:0.6779271960258484, Elapsed time for epoch : 0.35903868277867634
Epoch 4, Batch 40, train loss:0.7431595921516418, Elapsed time for epoch : 0.4747149070103963
Epoch 4, Batch 50, train loss:0.7025474905967712, Elapsed time for epoch : 0.5907414793968201
Epoch 4, Batch 60, train loss:0.653239905834198, Elapsed time for epoch : 0.70655277967453
Epoch 4, Batch 70, train loss:0.6857838034629822, Elapsed time for epoch : 0.8225007017453512
Epoch 4, Batch 80, train loss:0.4984578490257263, Elapsed time for epoch : 0.9385368903477986
Epoch 4, Batch 90, train loss:0.6572631001472473, Elapsed time for epoch : 1.0542426586151123
Epoch 4, Batch 100, train loss:0.6392361521720886, Elapsed time for epoch : 1.1705817023913065
Epoch 4, Batch 110, train loss:0.642008364200592, Elapsed time for epoch : 1.2864538828531902
Batch 0, val loss:1.1679452657699585
Batch 10, val loss:1.6427408456802368
Batch 20, val loss:16.940135955810547
Batch 30, val loss:4.5015058517456055
Epoch 4, Train Loss:0.6327615999657175, Val loss:5.86206898258792
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.63276
wandb:         Val Loss 5.86207
wandb:      train_batch 110
wandb: train_batch_loss 0.64201
wandb:        val_batch 30
wandb:   val_batch_loss 4.50151
wandb: 
wandb: üöÄ View run royal-waterfall-323 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/ehp9ff2y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_010334-ehp9ff2y/logs
Seed completed execution! 89 0.6_2
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_011119-4vs9bwyh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-morning-325
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4vs9bwyh
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.012972505887349446
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.12828221321105956
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.2435577392578125
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.35916748046875
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.475484299659729
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5913174788157145
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7075019081433614
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8232300718625386
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9390609622001648
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0550346771876018
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1711863279342651
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.2869516809781392
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
Epoch 1, Batch 0, train loss:1.5519860982894897, Elapsed time for epoch : 0.011664267381032307
Epoch 1, Batch 10, train loss:1.4443174600601196, Elapsed time for epoch : 0.12757594188054402
Epoch 1, Batch 20, train loss:1.2092701196670532, Elapsed time for epoch : 0.2432307481765747
Epoch 1, Batch 30, train loss:1.512547492980957, Elapsed time for epoch : 0.35910757780075075
Epoch 1, Batch 40, train loss:1.2831720113754272, Elapsed time for epoch : 0.4747559984525045
Epoch 1, Batch 50, train loss:1.2865995168685913, Elapsed time for epoch : 0.5909978588422139
Epoch 1, Batch 60, train loss:1.2534452676773071, Elapsed time for epoch : 0.7070505181948344
Epoch 1, Batch 70, train loss:1.2001628875732422, Elapsed time for epoch : 0.8228378812472026
Epoch 1, Batch 80, train loss:1.1923569440841675, Elapsed time for epoch : 0.9389234105745952
Epoch 1, Batch 90, train loss:1.1845539808273315, Elapsed time for epoch : 1.0547077496846518
Epoch 1, Batch 100, train loss:0.8761093020439148, Elapsed time for epoch : 1.1708160161972045
Epoch 1, Batch 110, train loss:1.0176893472671509, Elapsed time for epoch : 1.287134826183319
Batch 0, val loss:2.3120594024658203
Batch 10, val loss:3.1623895168304443
Batch 20, val loss:2.1613471508026123
Batch 30, val loss:1.9537602663040161
Epoch 1, Train Loss:1.236130588469298, Val loss:4.995238833957249
Epoch 2, Batch 0, train loss:1.0287134647369385, Elapsed time for epoch : 0.011603105068206786
Epoch 2, Batch 10, train loss:0.9540262222290039, Elapsed time for epoch : 0.12756009101867677
Epoch 2, Batch 20, train loss:1.0608279705047607, Elapsed time for epoch : 0.24410091241200765
Epoch 2, Batch 30, train loss:0.9660180807113647, Elapsed time for epoch : 0.3598417719205221
Epoch 2, Batch 40, train loss:0.9426432847976685, Elapsed time for epoch : 0.4763073960940043
Epoch 2, Batch 50, train loss:1.0277059078216553, Elapsed time for epoch : 0.5924091855684916
Epoch 2, Batch 60, train loss:0.9092874526977539, Elapsed time for epoch : 0.7085517048835754
Epoch 2, Batch 70, train loss:0.8038166761398315, Elapsed time for epoch : 0.8243876854578654
Epoch 2, Batch 80, train loss:1.0482909679412842, Elapsed time for epoch : 0.9404207229614258
Epoch 2, Batch 90, train loss:0.8918736577033997, Elapsed time for epoch : 1.0563655455907186
Epoch 2, Batch 100, train loss:0.9361854195594788, Elapsed time for epoch : 1.1723987658818562
Epoch 2, Batch 110, train loss:0.9204474091529846, Elapsed time for epoch : 1.2881746530532836
Batch 0, val loss:1.9127575159072876
Batch 10, val loss:4.382065296173096
Batch 20, val loss:4.30318546295166
Batch 30, val loss:6.023943901062012
Epoch 2, Train Loss:0.9587588569392329, Val loss:5.537418540981081
Epoch 3, Batch 0, train loss:0.9114006757736206, Elapsed time for epoch : 0.011657186349232991
Epoch 3, Batch 10, train loss:0.8783732652664185, Elapsed time for epoch : 0.1273580034573873
Epoch 3, Batch 20, train loss:0.8587350845336914, Elapsed time for epoch : 0.24316064914067587
Epoch 3, Batch 30, train loss:0.9166836142539978, Elapsed time for epoch : 0.3589498559633891
Epoch 3, Batch 40, train loss:0.926134467124939, Elapsed time for epoch : 0.4748130361239115
Epoch 3, Batch 50, train loss:0.6085221171379089, Elapsed time for epoch : 0.5903125961621603
Epoch 3, Batch 60, train loss:0.792026937007904, Elapsed time for epoch : 0.7061096588770549
Epoch 3, Batch 70, train loss:0.9118708372116089, Elapsed time for epoch : 0.822024925549825
Epoch 3, Batch 80, train loss:0.9081965684890747, Elapsed time for epoch : 0.93788001537323
Epoch 3, Batch 90, train loss:0.8007227182388306, Elapsed time for epoch : 1.0537254254023234
Epoch 3, Batch 100, train loss:0.5062412023544312, Elapsed time for epoch : 1.1695417881011962
Epoch 3, Batch 110, train loss:0.7457603812217712, Elapsed time for epoch : 1.2858839511871338
Batch 0, val loss:6.3070502281188965
Batch 10, val loss:3.7878639698028564
Batch 20, val loss:2.84297251701355
Batch 30, val loss:3.294538974761963
Epoch 3, Train Loss:0.8090542648149573, Val loss:6.165570101804203
Epoch 4, Batch 0, train loss:0.8745941519737244, Elapsed time for epoch : 0.011589117844899495
Epoch 4, Batch 10, train loss:0.5393694043159485, Elapsed time for epoch : 0.12759356101353964
Epoch 4, Batch 20, train loss:0.4861161708831787, Elapsed time for epoch : 0.2435854434967041
Epoch 4, Batch 30, train loss:0.6779271960258484, Elapsed time for epoch : 0.3594432314236959
Epoch 4, Batch 40, train loss:0.7431595921516418, Elapsed time for epoch : 0.4755090316136678
Epoch 4, Batch 50, train loss:0.7025474905967712, Elapsed time for epoch : 0.5920053402582804
Epoch 4, Batch 60, train loss:0.653239905834198, Elapsed time for epoch : 0.7077846884727478
Epoch 4, Batch 70, train loss:0.6857838034629822, Elapsed time for epoch : 0.824187715848287
Epoch 4, Batch 80, train loss:0.4984578490257263, Elapsed time for epoch : 0.9400848110516866
Epoch 4, Batch 90, train loss:0.6572631001472473, Elapsed time for epoch : 1.055902632077535
Epoch 4, Batch 100, train loss:0.6392361521720886, Elapsed time for epoch : 1.171787428855896
Epoch 4, Batch 110, train loss:0.642008364200592, Elapsed time for epoch : 1.2878576000531514
Batch 0, val loss:1.1679452657699585
Batch 10, val loss:1.6427408456802368
Batch 20, val loss:16.940135955810547
Batch 30, val loss:4.5015058517456055
Epoch 4, Train Loss:0.6327615999657175, Val loss:5.86206898258792
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.63276
wandb:         Val Loss 5.86207
wandb:      train_batch 110
wandb: train_batch_loss 0.64201
wandb:        val_batch 30
wandb:   val_batch_loss 4.50151
wandb: 
wandb: üöÄ View run swept-morning-325 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/4vs9bwyh
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_011119-4vs9bwyh/logs
Seed completed execution! 23 0.6_2
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_011902-mghoecv5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-puddle-327
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/mghoecv5
Epoch 0, Batch 0, train loss:8.076019287109375, Elapsed time for epoch : 0.013611888885498047
Epoch 0, Batch 10, train loss:3.6126110553741455, Elapsed time for epoch : 0.13042147556940714
Epoch 0, Batch 20, train loss:3.0048885345458984, Elapsed time for epoch : 0.2470527688662211
Epoch 0, Batch 30, train loss:2.5433719158172607, Elapsed time for epoch : 0.3636061708132426
Epoch 0, Batch 40, train loss:2.2187340259552, Elapsed time for epoch : 0.48009213209152224
Epoch 0, Batch 50, train loss:2.1253087520599365, Elapsed time for epoch : 0.5974118272463481
Epoch 0, Batch 60, train loss:1.935720443725586, Elapsed time for epoch : 0.7140573143959046
Epoch 0, Batch 70, train loss:1.9726771116256714, Elapsed time for epoch : 0.8310301303863525
Epoch 0, Batch 80, train loss:1.9342073202133179, Elapsed time for epoch : 0.9481688499450683
Epoch 0, Batch 90, train loss:1.7427997589111328, Elapsed time for epoch : 1.0659284194310505
Epoch 0, Batch 100, train loss:1.5247939825057983, Elapsed time for epoch : 1.1827831308046977
Epoch 0, Batch 110, train loss:1.4078329801559448, Elapsed time for epoch : 1.2994412461916605
Batch 0, val loss:4.238219261169434
Batch 10, val loss:8.622987747192383
Batch 20, val loss:4.837219715118408
Batch 30, val loss:7.081770896911621
Epoch 0, Train Loss:2.9249830836835113, Val loss:3.995532747772005
Epoch 1, Batch 0, train loss:1.5519860982894897, Elapsed time for epoch : 0.011648805936177571
Epoch 1, Batch 10, train loss:1.4443174600601196, Elapsed time for epoch : 0.1273068110148112
Epoch 1, Batch 20, train loss:1.2092701196670532, Elapsed time for epoch : 0.24323286215464274
Epoch 1, Batch 30, train loss:1.512547492980957, Elapsed time for epoch : 0.35945902665456136
Epoch 1, Batch 40, train loss:1.2831720113754272, Elapsed time for epoch : 0.47511592706044514
Epoch 1, Batch 50, train loss:1.2865995168685913, Elapsed time for epoch : 0.591294276714325
Epoch 1, Batch 60, train loss:1.2534452676773071, Elapsed time for epoch : 0.7075076937675476
Epoch 1, Batch 70, train loss:1.2001628875732422, Elapsed time for epoch : 0.8233907580375671
Epoch 1, Batch 80, train loss:1.1923569440841675, Elapsed time for epoch : 0.9393234093983968
Epoch 1, Batch 90, train loss:1.1845539808273315, Elapsed time for epoch : 1.055396525065104
Epoch 1, Batch 100, train loss:0.8761093020439148, Elapsed time for epoch : 1.1712405721346537
Epoch 1, Batch 110, train loss:1.0176893472671509, Elapsed time for epoch : 1.2873855471611022
Batch 0, val loss:2.3120594024658203
Batch 10, val loss:3.1623895168304443
Batch 20, val loss:2.1613471508026123
Batch 30, val loss:1.9537602663040161
Epoch 1, Train Loss:1.236130588469298, Val loss:4.995238833957249
Epoch 2, Batch 0, train loss:1.0287134647369385, Elapsed time for epoch : 0.011643481254577637
Epoch 2, Batch 10, train loss:0.9540262222290039, Elapsed time for epoch : 0.12769137223561605
Epoch 2, Batch 20, train loss:1.0608279705047607, Elapsed time for epoch : 0.24364816347757975
Epoch 2, Batch 30, train loss:0.9660180807113647, Elapsed time for epoch : 0.3598349769910177
Epoch 2, Batch 40, train loss:0.9426432847976685, Elapsed time for epoch : 0.4762268265088399
Epoch 2, Batch 50, train loss:1.0277059078216553, Elapsed time for epoch : 0.5923616091410319
Epoch 2, Batch 60, train loss:0.9092874526977539, Elapsed time for epoch : 0.7084928115208944
Epoch 2, Batch 70, train loss:0.8038166761398315, Elapsed time for epoch : 0.8243179480234782
Epoch 2, Batch 80, train loss:1.0482909679412842, Elapsed time for epoch : 0.9402610222498576
Epoch 2, Batch 90, train loss:0.8918736577033997, Elapsed time for epoch : 1.057007642587026
Epoch 2, Batch 100, train loss:0.9361854195594788, Elapsed time for epoch : 1.1731107989947
Epoch 2, Batch 110, train loss:0.9204474091529846, Elapsed time for epoch : 1.2889104008674621
Batch 0, val loss:1.9127575159072876
Batch 10, val loss:4.382065296173096
Batch 20, val loss:4.30318546295166
Batch 30, val loss:6.023943901062012
Epoch 2, Train Loss:0.9587588569392329, Val loss:5.537418540981081
Epoch 3, Batch 0, train loss:0.9114006757736206, Elapsed time for epoch : 0.011593520641326904
Epoch 3, Batch 10, train loss:0.8783732652664185, Elapsed time for epoch : 0.12768656015396118
Epoch 3, Batch 20, train loss:0.8587350845336914, Elapsed time for epoch : 0.24352368513743083
Epoch 3, Batch 30, train loss:0.9166836142539978, Elapsed time for epoch : 0.3591752330462138
Epoch 3, Batch 40, train loss:0.926134467124939, Elapsed time for epoch : 0.47507115205128986
Epoch 3, Batch 50, train loss:0.6085221171379089, Elapsed time for epoch : 0.5907672087351481
Epoch 3, Batch 60, train loss:0.792026937007904, Elapsed time for epoch : 0.7068504770596822
Epoch 3, Batch 70, train loss:0.9118708372116089, Elapsed time for epoch : 0.8228398084640502
Epoch 3, Batch 80, train loss:0.9081965684890747, Elapsed time for epoch : 0.9387496908505758
Epoch 3, Batch 90, train loss:0.8007227182388306, Elapsed time for epoch : 1.054795221487681
Epoch 3, Batch 100, train loss:0.5062412023544312, Elapsed time for epoch : 1.1706122080485026
Epoch 3, Batch 110, train loss:0.7457603812217712, Elapsed time for epoch : 1.286461591720581
Batch 0, val loss:6.3070502281188965
Batch 10, val loss:3.7878639698028564
Batch 20, val loss:2.84297251701355
Batch 30, val loss:3.294538974761963
Epoch 3, Train Loss:0.8090542648149573, Val loss:6.165570101804203
Epoch 4, Batch 0, train loss:0.8745941519737244, Elapsed time for epoch : 0.011640278498331706
Epoch 4, Batch 10, train loss:0.5393694043159485, Elapsed time for epoch : 0.12760870854059855
Epoch 4, Batch 20, train loss:0.4861161708831787, Elapsed time for epoch : 0.24350208441416424
Epoch 4, Batch 30, train loss:0.6779271960258484, Elapsed time for epoch : 0.35926403999328616
Epoch 4, Batch 40, train loss:0.7431595921516418, Elapsed time for epoch : 0.47516339222590126
Epoch 4, Batch 50, train loss:0.7025474905967712, Elapsed time for epoch : 0.591171669960022
Epoch 4, Batch 60, train loss:0.653239905834198, Elapsed time for epoch : 0.7074369510014852
Epoch 4, Batch 70, train loss:0.6857838034629822, Elapsed time for epoch : 0.8231229106585185
Epoch 4, Batch 80, train loss:0.4984578490257263, Elapsed time for epoch : 0.939040458202362
Epoch 4, Batch 90, train loss:0.6572631001472473, Elapsed time for epoch : 1.054814596970876
Epoch 4, Batch 100, train loss:0.6392361521720886, Elapsed time for epoch : 1.170628583431244
Epoch 4, Batch 110, train loss:0.642008364200592, Elapsed time for epoch : 1.2866627415021261
Batch 0, val loss:1.1679452657699585
Batch 10, val loss:1.6427408456802368
Batch 20, val loss:16.940135955810547
Batch 30, val loss:4.5015058517456055
Epoch 4, Train Loss:0.6327615999657175, Val loss:5.86206898258792
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñà‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.63276
wandb:         Val Loss 5.86207
wandb:      train_batch 110
wandb: train_batch_loss 0.64201
wandb:        val_batch 30
wandb:   val_batch_loss 4.50151
wandb: 
wandb: üöÄ View run copper-puddle-327 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/mghoecv5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_011902-mghoecv5/logs
Seed completed execution! 113 0.6_2
------------------------------------------------------------------
Experiment complete 0.6_2
==========================================================================
Running experiment for setting 0.6_3
==========================================================================
Running for seed 1 of experiment 0.6_3
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_012648-y6mgr6z1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-thunder-329
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/y6mgr6z1
Epoch 0, Batch 0, train loss:8.347854614257812, Elapsed time for epoch : 0.013835020860036214
Epoch 0, Batch 10, train loss:4.4242095947265625, Elapsed time for epoch : 0.13694818019866944
Epoch 0, Batch 20, train loss:3.316420793533325, Elapsed time for epoch : 0.2594824155171712
Epoch 0, Batch 30, train loss:3.2183239459991455, Elapsed time for epoch : 0.38324567874272664
Epoch 0, Batch 40, train loss:2.9943695068359375, Elapsed time for epoch : 0.5070916453997294
Epoch 0, Batch 50, train loss:2.6217434406280518, Elapsed time for epoch : 0.6296482404073079
Epoch 0, Batch 60, train loss:2.438206911087036, Elapsed time for epoch : 0.7522272109985352
Epoch 0, Batch 70, train loss:2.6038661003112793, Elapsed time for epoch : 0.8773653189341227
Epoch 0, Batch 80, train loss:2.4781172275543213, Elapsed time for epoch : 1.0022235751152038
Epoch 0, Batch 90, train loss:2.157114028930664, Elapsed time for epoch : 1.1254380504290262
Epoch 0, Batch 100, train loss:1.9878718852996826, Elapsed time for epoch : 1.2508652408917744
Epoch 0, Batch 110, train loss:2.0660946369171143, Elapsed time for epoch : 1.3758343974749248
Batch 0, val loss:3.182356119155884
Batch 10, val loss:6.03884744644165
Batch 20, val loss:4.6288533210754395
Batch 30, val loss:4.553445339202881
Epoch 0, Train Loss:3.080253503633582, Val loss:3.679094215234121
Epoch 1, Batch 0, train loss:1.976865530014038, Elapsed time for epoch : 0.011603295803070068
Epoch 1, Batch 10, train loss:1.7361021041870117, Elapsed time for epoch : 0.12788617610931396
Epoch 1, Batch 20, train loss:1.4024637937545776, Elapsed time for epoch : 0.24414493242899576
Epoch 1, Batch 30, train loss:1.6138273477554321, Elapsed time for epoch : 0.36065921386082966
Epoch 1, Batch 40, train loss:1.4194071292877197, Elapsed time for epoch : 0.47689496278762816
Epoch 1, Batch 50, train loss:1.515070915222168, Elapsed time for epoch : 0.5930628577868143
Epoch 1, Batch 60, train loss:1.5286911725997925, Elapsed time for epoch : 0.7091985583305359
Epoch 1, Batch 70, train loss:1.386800765991211, Elapsed time for epoch : 0.8258484482765198
Epoch 1, Batch 80, train loss:1.2681890726089478, Elapsed time for epoch : 0.9419174790382385
Epoch 1, Batch 90, train loss:1.2399702072143555, Elapsed time for epoch : 1.0577780882517496
Epoch 1, Batch 100, train loss:1.216152548789978, Elapsed time for epoch : 1.1736908157666524
Epoch 1, Batch 110, train loss:0.951231062412262, Elapsed time for epoch : 1.289952826499939
Batch 0, val loss:2.414094924926758
Batch 10, val loss:6.720540523529053
Batch 20, val loss:3.6803410053253174
Batch 30, val loss:2.798220157623291
Epoch 1, Train Loss:1.4086078633432804, Val loss:5.4510283172130585
Epoch 2, Batch 0, train loss:0.975439727306366, Elapsed time for epoch : 0.011568558216094971
Epoch 2, Batch 10, train loss:0.9305253624916077, Elapsed time for epoch : 0.12758771975835165
Epoch 2, Batch 20, train loss:1.0028232336044312, Elapsed time for epoch : 0.24391674598058063
Epoch 2, Batch 30, train loss:0.9999862313270569, Elapsed time for epoch : 0.36026710669199624
Epoch 2, Batch 40, train loss:1.0160218477249146, Elapsed time for epoch : 0.47665650049845376
Epoch 2, Batch 50, train loss:0.9715244174003601, Elapsed time for epoch : 0.5931551496187846
Epoch 2, Batch 60, train loss:0.995136559009552, Elapsed time for epoch : 0.7095122774442036
Epoch 2, Batch 70, train loss:0.5409894585609436, Elapsed time for epoch : 0.8258234262466431
Epoch 2, Batch 80, train loss:0.9897194504737854, Elapsed time for epoch : 0.9423320174217225
Epoch 2, Batch 90, train loss:0.7743126153945923, Elapsed time for epoch : 1.0586275657018025
Epoch 2, Batch 100, train loss:0.7968353629112244, Elapsed time for epoch : 1.1750774224599203
Epoch 2, Batch 110, train loss:0.8439378142356873, Elapsed time for epoch : 1.2917960206667582
Batch 0, val loss:2.8109586238861084
Batch 10, val loss:4.474595546722412
Batch 20, val loss:3.4122352600097656
Batch 30, val loss:7.172788143157959
Epoch 2, Train Loss:0.8851430514584417, Val loss:8.611885597308477
Epoch 3, Batch 0, train loss:0.6720537543296814, Elapsed time for epoch : 0.011613285541534424
Epoch 3, Batch 10, train loss:0.7985845804214478, Elapsed time for epoch : 0.12784777084986368
Epoch 3, Batch 20, train loss:0.8522709608078003, Elapsed time for epoch : 0.2439513087272644
Epoch 3, Batch 30, train loss:0.703675389289856, Elapsed time for epoch : 0.36023610830307007
Epoch 3, Batch 40, train loss:0.6881948113441467, Elapsed time for epoch : 0.4765946626663208
Epoch 3, Batch 50, train loss:0.32886457443237305, Elapsed time for epoch : 0.5928901632626852
Epoch 3, Batch 60, train loss:0.731735348701477, Elapsed time for epoch : 0.7091673254966736
Epoch 3, Batch 70, train loss:0.6987885236740112, Elapsed time for epoch : 0.8257203340530396
Epoch 3, Batch 80, train loss:0.6680322289466858, Elapsed time for epoch : 0.9418764670689901
Epoch 3, Batch 90, train loss:0.610522449016571, Elapsed time for epoch : 1.0581587076187133
Epoch 3, Batch 100, train loss:0.187259703874588, Elapsed time for epoch : 1.1748688022295635
Epoch 3, Batch 110, train loss:0.5731604695320129, Elapsed time for epoch : 1.2919109781583151
Batch 0, val loss:4.465527534484863
Batch 10, val loss:21.48220443725586
Batch 20, val loss:6.4302659034729
Batch 30, val loss:2.9591827392578125
Epoch 3, Train Loss:0.6343399332917254, Val loss:13.741713649696774
Epoch 4, Batch 0, train loss:1.523667335510254, Elapsed time for epoch : 0.011878116925557455
Epoch 4, Batch 10, train loss:0.2455824315547943, Elapsed time for epoch : 0.12803483804066976
Epoch 4, Batch 20, train loss:0.17868871986865997, Elapsed time for epoch : 0.24440704186757406
Epoch 4, Batch 30, train loss:0.4454323649406433, Elapsed time for epoch : 0.3606929421424866
Epoch 4, Batch 40, train loss:0.508226215839386, Elapsed time for epoch : 0.4768664320309957
Epoch 4, Batch 50, train loss:0.42619258165359497, Elapsed time for epoch : 0.5935184359550476
Epoch 4, Batch 60, train loss:0.4102863371372223, Elapsed time for epoch : 0.7101547122001648
Epoch 4, Batch 70, train loss:0.36404916644096375, Elapsed time for epoch : 0.8272780934969585
Epoch 4, Batch 80, train loss:0.18098974227905273, Elapsed time for epoch : 0.9436938246091207
Epoch 4, Batch 90, train loss:0.4282021224498749, Elapsed time for epoch : 1.0603156129519145
Epoch 4, Batch 100, train loss:0.43726682662963867, Elapsed time for epoch : 1.1769006609916688
Epoch 4, Batch 110, train loss:0.3795817196369171, Elapsed time for epoch : 1.2932346185048422
Batch 0, val loss:10.589686393737793
Batch 10, val loss:3.181077718734741
Batch 20, val loss:12.736327171325684
Batch 30, val loss:4.266078472137451
Epoch 4, Train Loss:0.40660241783961004, Val loss:10.355023539728588
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.4066
wandb:         Val Loss 10.35502
wandb:      train_batch 110
wandb: train_batch_loss 0.37958
wandb:        val_batch 30
wandb:   val_batch_loss 4.26608
wandb: 
wandb: üöÄ View run fine-thunder-329 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/y6mgr6z1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_012648-y6mgr6z1/logs
Seed completed execution! 1 0.6_3
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_013444-pwxa1tvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-leaf-331
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/pwxa1tvx
Epoch 0, Batch 0, train loss:8.347854614257812, Elapsed time for epoch : 0.013477603594462076
Epoch 0, Batch 10, train loss:4.4242095947265625, Elapsed time for epoch : 0.130014967918396
Epoch 0, Batch 20, train loss:3.316420793533325, Elapsed time for epoch : 0.24580118656158448
Epoch 0, Batch 30, train loss:3.2183239459991455, Elapsed time for epoch : 0.36192823251088463
Epoch 0, Batch 40, train loss:2.9943695068359375, Elapsed time for epoch : 0.47787430286407473
Epoch 0, Batch 50, train loss:2.6217434406280518, Elapsed time for epoch : 0.5940993030865988
Epoch 0, Batch 60, train loss:2.438206911087036, Elapsed time for epoch : 0.7108475009600321
Epoch 0, Batch 70, train loss:2.6038661003112793, Elapsed time for epoch : 0.8268929839134216
Epoch 0, Batch 80, train loss:2.4781172275543213, Elapsed time for epoch : 0.9435645659764608
Epoch 0, Batch 90, train loss:2.157114028930664, Elapsed time for epoch : 1.0601005593935648
Epoch 0, Batch 100, train loss:1.9878718852996826, Elapsed time for epoch : 1.1761151790618896
Epoch 0, Batch 110, train loss:2.0660946369171143, Elapsed time for epoch : 1.2926251451174418
Batch 0, val loss:3.182356119155884
Batch 10, val loss:6.03884744644165
Batch 20, val loss:4.6288533210754395
Batch 30, val loss:4.553445339202881
Epoch 0, Train Loss:3.080253503633582, Val loss:3.679094215234121
Epoch 1, Batch 0, train loss:1.976865530014038, Elapsed time for epoch : 0.01162409782409668
Epoch 1, Batch 10, train loss:1.7361021041870117, Elapsed time for epoch : 0.12815816402435304
Epoch 1, Batch 20, train loss:1.4024637937545776, Elapsed time for epoch : 0.2445197621981303
Epoch 1, Batch 30, train loss:1.6138273477554321, Elapsed time for epoch : 0.3616726040840149
Epoch 1, Batch 40, train loss:1.4194071292877197, Elapsed time for epoch : 0.4789338350296021
Epoch 1, Batch 50, train loss:1.515070915222168, Elapsed time for epoch : 0.5952788949012756
Epoch 1, Batch 60, train loss:1.5286911725997925, Elapsed time for epoch : 0.7122051318486532
Epoch 1, Batch 70, train loss:1.386800765991211, Elapsed time for epoch : 0.8295531193415324
Epoch 1, Batch 80, train loss:1.2681890726089478, Elapsed time for epoch : 0.9458614985148112
Epoch 1, Batch 90, train loss:1.2399702072143555, Elapsed time for epoch : 1.0624102393786112
Epoch 1, Batch 100, train loss:1.216152548789978, Elapsed time for epoch : 1.1786801139513652
Epoch 1, Batch 110, train loss:0.951231062412262, Elapsed time for epoch : 1.2954648931821187
Batch 0, val loss:2.414094924926758
Batch 10, val loss:6.720540523529053
Batch 20, val loss:3.6803410053253174
Batch 30, val loss:2.798220157623291
Epoch 1, Train Loss:1.4086078633432804, Val loss:5.4510283172130585
Epoch 2, Batch 0, train loss:0.975439727306366, Elapsed time for epoch : 0.01161256233851115
Epoch 2, Batch 10, train loss:0.9305253624916077, Elapsed time for epoch : 0.12797391414642334
Epoch 2, Batch 20, train loss:1.0028232336044312, Elapsed time for epoch : 0.24458274443944295
Epoch 2, Batch 30, train loss:0.9999862313270569, Elapsed time for epoch : 0.3612011392911275
Epoch 2, Batch 40, train loss:1.0160218477249146, Elapsed time for epoch : 0.47799880504608155
Epoch 2, Batch 50, train loss:0.9715244174003601, Elapsed time for epoch : 0.5947537223498026
Epoch 2, Batch 60, train loss:0.995136559009552, Elapsed time for epoch : 0.7111349304517111
Epoch 2, Batch 70, train loss:0.5409894585609436, Elapsed time for epoch : 0.8276944001515706
Epoch 2, Batch 80, train loss:0.9897194504737854, Elapsed time for epoch : 0.9445446451505025
Epoch 2, Batch 90, train loss:0.7743126153945923, Elapsed time for epoch : 1.060952079296112
Epoch 2, Batch 100, train loss:0.7968353629112244, Elapsed time for epoch : 1.1775289495786032
Epoch 2, Batch 110, train loss:0.8439378142356873, Elapsed time for epoch : 1.2939392685890199
Batch 0, val loss:2.8109586238861084
Batch 10, val loss:4.474595546722412
Batch 20, val loss:3.4122352600097656
Batch 30, val loss:7.172788143157959
Epoch 2, Train Loss:0.8851430514584417, Val loss:8.611885597308477
Epoch 3, Batch 0, train loss:0.6720537543296814, Elapsed time for epoch : 0.01185464064280192
Epoch 3, Batch 10, train loss:0.7985845804214478, Elapsed time for epoch : 0.12817833423614503
Epoch 3, Batch 20, train loss:0.8522709608078003, Elapsed time for epoch : 0.24487645626068116
Epoch 3, Batch 30, train loss:0.703675389289856, Elapsed time for epoch : 0.36138692299524944
Epoch 3, Batch 40, train loss:0.6881948113441467, Elapsed time for epoch : 0.47767602602640785
Epoch 3, Batch 50, train loss:0.32886457443237305, Elapsed time for epoch : 0.5945949673652648
Epoch 3, Batch 60, train loss:0.731735348701477, Elapsed time for epoch : 0.7108261307080587
Epoch 3, Batch 70, train loss:0.6987885236740112, Elapsed time for epoch : 0.8276409228642782
Epoch 3, Batch 80, train loss:0.6680322289466858, Elapsed time for epoch : 0.9439556956291199
Epoch 3, Batch 90, train loss:0.610522449016571, Elapsed time for epoch : 1.0604376276334126
Epoch 3, Batch 100, train loss:0.187259703874588, Elapsed time for epoch : 1.176839045683543
Epoch 3, Batch 110, train loss:0.5731604695320129, Elapsed time for epoch : 1.2935784459114075
Batch 0, val loss:4.465527534484863
Batch 10, val loss:21.48220443725586
Batch 20, val loss:6.4302659034729
Batch 30, val loss:2.9591827392578125
Epoch 3, Train Loss:0.6343399332917254, Val loss:13.741713649696774
Epoch 4, Batch 0, train loss:1.523667335510254, Elapsed time for epoch : 0.011699287096659343
Epoch 4, Batch 10, train loss:0.2455824315547943, Elapsed time for epoch : 0.12802242040634154
Epoch 4, Batch 20, train loss:0.17868871986865997, Elapsed time for epoch : 0.24444324970245362
Epoch 4, Batch 30, train loss:0.4454323649406433, Elapsed time for epoch : 0.361050017674764
Epoch 4, Batch 40, train loss:0.508226215839386, Elapsed time for epoch : 0.4778923988342285
Epoch 4, Batch 50, train loss:0.42619258165359497, Elapsed time for epoch : 0.5943077325820922
Epoch 4, Batch 60, train loss:0.4102863371372223, Elapsed time for epoch : 0.7107152541478475
Epoch 4, Batch 70, train loss:0.36404916644096375, Elapsed time for epoch : 0.8271181186040243
Epoch 4, Batch 80, train loss:0.18098974227905273, Elapsed time for epoch : 0.9439962824185689
Epoch 4, Batch 90, train loss:0.4282021224498749, Elapsed time for epoch : 1.0603978872299193
Epoch 4, Batch 100, train loss:0.43726682662963867, Elapsed time for epoch : 1.1771316289901734
Epoch 4, Batch 110, train loss:0.3795817196369171, Elapsed time for epoch : 1.2934921582539876
Batch 0, val loss:10.589686393737793
Batch 10, val loss:3.181077718734741
Batch 20, val loss:12.736327171325684
Batch 30, val loss:4.266078472137451
Epoch 4, Train Loss:0.40660241783961004, Val loss:10.355023539728588
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.4066
wandb:         Val Loss 10.35502
wandb:      train_batch 110
wandb: train_batch_loss 0.37958
wandb:        val_batch 30
wandb:   val_batch_loss 4.26608
wandb: 
wandb: üöÄ View run breezy-leaf-331 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/pwxa1tvx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_013444-pwxa1tvx/logs
Seed completed execution! 42 0.6_3
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_014232-e7ewglig
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-field-333
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e7ewglig
Epoch 0, Batch 0, train loss:8.347854614257812, Elapsed time for epoch : 0.013297414779663086
Epoch 0, Batch 10, train loss:4.4242095947265625, Elapsed time for epoch : 0.1291387399037679
Epoch 0, Batch 20, train loss:3.316420793533325, Elapsed time for epoch : 0.24474135239919026
Epoch 0, Batch 30, train loss:3.2183239459991455, Elapsed time for epoch : 0.3608872890472412
Epoch 0, Batch 40, train loss:2.9943695068359375, Elapsed time for epoch : 0.4766513784726461
Epoch 0, Batch 50, train loss:2.6217434406280518, Elapsed time for epoch : 0.5932398160298665
Epoch 0, Batch 60, train loss:2.438206911087036, Elapsed time for epoch : 0.7095426559448242
Epoch 0, Batch 70, train loss:2.6038661003112793, Elapsed time for epoch : 0.8257862567901612
Epoch 0, Batch 80, train loss:2.4781172275543213, Elapsed time for epoch : 0.9418385823567709
Epoch 0, Batch 90, train loss:2.157114028930664, Elapsed time for epoch : 1.0581623315811157
Epoch 0, Batch 100, train loss:1.9878718852996826, Elapsed time for epoch : 1.1746004462242126
Epoch 0, Batch 110, train loss:2.0660946369171143, Elapsed time for epoch : 1.2911033153533935
Batch 0, val loss:3.182356119155884
Batch 10, val loss:6.03884744644165
Batch 20, val loss:4.6288533210754395
Batch 30, val loss:4.553445339202881
Epoch 0, Train Loss:3.080253503633582, Val loss:3.679094215234121
Epoch 1, Batch 0, train loss:1.976865530014038, Elapsed time for epoch : 0.01161102056503296
Epoch 1, Batch 10, train loss:1.7361021041870117, Elapsed time for epoch : 0.1277400255203247
Epoch 1, Batch 20, train loss:1.4024637937545776, Elapsed time for epoch : 0.24438730478286744
Epoch 1, Batch 30, train loss:1.6138273477554321, Elapsed time for epoch : 0.3607268969217936
Epoch 1, Batch 40, train loss:1.4194071292877197, Elapsed time for epoch : 0.4773520032564799
Epoch 1, Batch 50, train loss:1.515070915222168, Elapsed time for epoch : 0.5938999692598979
Epoch 1, Batch 60, train loss:1.5286911725997925, Elapsed time for epoch : 0.7104122837384542
Epoch 1, Batch 70, train loss:1.386800765991211, Elapsed time for epoch : 0.8271768053372701
Epoch 1, Batch 80, train loss:1.2681890726089478, Elapsed time for epoch : 0.9433099428812662
Epoch 1, Batch 90, train loss:1.2399702072143555, Elapsed time for epoch : 1.059787654876709
Epoch 1, Batch 100, train loss:1.216152548789978, Elapsed time for epoch : 1.1766617059707642
Epoch 1, Batch 110, train loss:0.951231062412262, Elapsed time for epoch : 1.2927761276563008
Batch 0, val loss:2.414094924926758
Batch 10, val loss:6.720540523529053
Batch 20, val loss:3.6803410053253174
Batch 30, val loss:2.798220157623291
Epoch 1, Train Loss:1.4086078633432804, Val loss:5.4510283172130585
Epoch 2, Batch 0, train loss:0.975439727306366, Elapsed time for epoch : 0.011644232273101806
Epoch 2, Batch 10, train loss:0.9305253624916077, Elapsed time for epoch : 0.12834353844324747
Epoch 2, Batch 20, train loss:1.0028232336044312, Elapsed time for epoch : 0.2446959336598714
Epoch 2, Batch 30, train loss:0.9999862313270569, Elapsed time for epoch : 0.3611775318781535
Epoch 2, Batch 40, train loss:1.0160218477249146, Elapsed time for epoch : 0.4775646209716797
Epoch 2, Batch 50, train loss:0.9715244174003601, Elapsed time for epoch : 0.5940723617871603
Epoch 2, Batch 60, train loss:0.995136559009552, Elapsed time for epoch : 0.710822343826294
Epoch 2, Batch 70, train loss:0.5409894585609436, Elapsed time for epoch : 0.8274922688802083
Epoch 2, Batch 80, train loss:0.9897194504737854, Elapsed time for epoch : 0.9439842859903972
Epoch 2, Batch 90, train loss:0.7743126153945923, Elapsed time for epoch : 1.060488231976827
Epoch 2, Batch 100, train loss:0.7968353629112244, Elapsed time for epoch : 1.1767542123794557
Epoch 2, Batch 110, train loss:0.8439378142356873, Elapsed time for epoch : 1.2933376669883727
Batch 0, val loss:2.8109586238861084
Batch 10, val loss:4.474595546722412
Batch 20, val loss:3.4122352600097656
Batch 30, val loss:7.172788143157959
Epoch 2, Train Loss:0.8851430514584417, Val loss:8.611885597308477
Epoch 3, Batch 0, train loss:0.6720537543296814, Elapsed time for epoch : 0.011661839485168458
Epoch 3, Batch 10, train loss:0.7985845804214478, Elapsed time for epoch : 0.1279947876930237
Epoch 3, Batch 20, train loss:0.8522709608078003, Elapsed time for epoch : 0.24438242117563883
Epoch 3, Batch 30, train loss:0.703675389289856, Elapsed time for epoch : 0.3607534448305766
Epoch 3, Batch 40, train loss:0.6881948113441467, Elapsed time for epoch : 0.4771329720815023
Epoch 3, Batch 50, train loss:0.32886457443237305, Elapsed time for epoch : 0.5936134735743205
Epoch 3, Batch 60, train loss:0.731735348701477, Elapsed time for epoch : 0.7100196917851765
Epoch 3, Batch 70, train loss:0.6987885236740112, Elapsed time for epoch : 0.8266445239384969
Epoch 3, Batch 80, train loss:0.6680322289466858, Elapsed time for epoch : 0.9430292884508769
Epoch 3, Batch 90, train loss:0.610522449016571, Elapsed time for epoch : 1.0593671282132466
Epoch 3, Batch 100, train loss:0.187259703874588, Elapsed time for epoch : 1.1756998658180238
Epoch 3, Batch 110, train loss:0.5731604695320129, Elapsed time for epoch : 1.2923498709996541
Batch 0, val loss:4.465527534484863
Batch 10, val loss:21.48220443725586
Batch 20, val loss:6.4302659034729
Batch 30, val loss:2.9591827392578125
Epoch 3, Train Loss:0.6343399332917254, Val loss:13.741713649696774
Epoch 4, Batch 0, train loss:1.523667335510254, Elapsed time for epoch : 0.011770578225453694
Epoch 4, Batch 10, train loss:0.2455824315547943, Elapsed time for epoch : 0.12855583031972248
Epoch 4, Batch 20, train loss:0.17868871986865997, Elapsed time for epoch : 0.2452300230662028
Epoch 4, Batch 30, train loss:0.4454323649406433, Elapsed time for epoch : 0.3616815090179443
Epoch 4, Batch 40, train loss:0.508226215839386, Elapsed time for epoch : 0.47792916695276894
Epoch 4, Batch 50, train loss:0.42619258165359497, Elapsed time for epoch : 0.5946289380391439
Epoch 4, Batch 60, train loss:0.4102863371372223, Elapsed time for epoch : 0.7112759431203206
Epoch 4, Batch 70, train loss:0.36404916644096375, Elapsed time for epoch : 0.8278274615605672
Epoch 4, Batch 80, train loss:0.18098974227905273, Elapsed time for epoch : 0.9442307035128276
Epoch 4, Batch 90, train loss:0.4282021224498749, Elapsed time for epoch : 1.0607521494229635
Epoch 4, Batch 100, train loss:0.43726682662963867, Elapsed time for epoch : 1.1776148517926535
Epoch 4, Batch 110, train loss:0.3795817196369171, Elapsed time for epoch : 1.2943307956059773
Batch 0, val loss:10.589686393737793
Batch 10, val loss:3.181077718734741
Batch 20, val loss:12.736327171325684
Batch 30, val loss:4.266078472137451
Epoch 4, Train Loss:0.40660241783961004, Val loss:10.355023539728588
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.4066
wandb:         Val Loss 10.35502
wandb:      train_batch 110
wandb: train_batch_loss 0.37958
wandb:        val_batch 30
wandb:   val_batch_loss 4.26608
wandb: 
wandb: üöÄ View run rural-field-333 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e7ewglig
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_014232-e7ewglig/logs
Seed completed execution! 89 0.6_3
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_015018-wjepxrvn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-armadillo-335
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wjepxrvn
Epoch 0, Batch 0, train loss:8.347854614257812, Elapsed time for epoch : 0.01329961617787679
Epoch 0, Batch 10, train loss:4.4242095947265625, Elapsed time for epoch : 0.129282017548879
Epoch 0, Batch 20, train loss:3.316420793533325, Elapsed time for epoch : 0.24515103101730346
Epoch 0, Batch 30, train loss:3.2183239459991455, Elapsed time for epoch : 0.36157063643137616
Epoch 0, Batch 40, train loss:2.9943695068359375, Elapsed time for epoch : 0.4782760818799337
Epoch 0, Batch 50, train loss:2.6217434406280518, Elapsed time for epoch : 0.5946302493413289
Epoch 0, Batch 60, train loss:2.438206911087036, Elapsed time for epoch : 0.7106882333755493
Epoch 0, Batch 70, train loss:2.6038661003112793, Elapsed time for epoch : 0.8269906560579936
Epoch 0, Batch 80, train loss:2.4781172275543213, Elapsed time for epoch : 0.9433322707811992
Epoch 0, Batch 90, train loss:2.157114028930664, Elapsed time for epoch : 1.0595228950182596
Epoch 0, Batch 100, train loss:1.9878718852996826, Elapsed time for epoch : 1.1757382074991862
Epoch 0, Batch 110, train loss:2.0660946369171143, Elapsed time for epoch : 1.2920895099639893
Batch 0, val loss:3.182356119155884
Batch 10, val loss:6.03884744644165
Batch 20, val loss:4.6288533210754395
Batch 30, val loss:4.553445339202881
Epoch 0, Train Loss:3.080253503633582, Val loss:3.679094215234121
Epoch 1, Batch 0, train loss:1.976865530014038, Elapsed time for epoch : 0.011647462844848633
Epoch 1, Batch 10, train loss:1.7361021041870117, Elapsed time for epoch : 0.1280128041903178
Epoch 1, Batch 20, train loss:1.4024637937545776, Elapsed time for epoch : 0.24457938273747762
Epoch 1, Batch 30, train loss:1.6138273477554321, Elapsed time for epoch : 0.3610992431640625
Epoch 1, Batch 40, train loss:1.4194071292877197, Elapsed time for epoch : 0.477982501188914
Epoch 1, Batch 50, train loss:1.515070915222168, Elapsed time for epoch : 0.5940316716829935
Epoch 1, Batch 60, train loss:1.5286911725997925, Elapsed time for epoch : 0.7106736222902934
Epoch 1, Batch 70, train loss:1.386800765991211, Elapsed time for epoch : 0.8271162907282511
Epoch 1, Batch 80, train loss:1.2681890726089478, Elapsed time for epoch : 0.9432071129480998
Epoch 1, Batch 90, train loss:1.2399702072143555, Elapsed time for epoch : 1.059808580080668
Epoch 1, Batch 100, train loss:1.216152548789978, Elapsed time for epoch : 1.1761608163515727
Epoch 1, Batch 110, train loss:0.951231062412262, Elapsed time for epoch : 1.292959217230479
Batch 0, val loss:2.414094924926758
Batch 10, val loss:6.720540523529053
Batch 20, val loss:3.6803410053253174
Batch 30, val loss:2.798220157623291
Epoch 1, Train Loss:1.4086078633432804, Val loss:5.4510283172130585
Epoch 2, Batch 0, train loss:0.975439727306366, Elapsed time for epoch : 0.011689464251200357
Epoch 2, Batch 10, train loss:0.9305253624916077, Elapsed time for epoch : 0.12788335084915162
Epoch 2, Batch 20, train loss:1.0028232336044312, Elapsed time for epoch : 0.24438215891520182
Epoch 2, Batch 30, train loss:0.9999862313270569, Elapsed time for epoch : 0.36107850074768066
Epoch 2, Batch 40, train loss:1.0160218477249146, Elapsed time for epoch : 0.4776349107424418
Epoch 2, Batch 50, train loss:0.9715244174003601, Elapsed time for epoch : 0.593913996219635
Epoch 2, Batch 60, train loss:0.995136559009552, Elapsed time for epoch : 0.7104839245478313
Epoch 2, Batch 70, train loss:0.5409894585609436, Elapsed time for epoch : 0.826658829053243
Epoch 2, Batch 80, train loss:0.9897194504737854, Elapsed time for epoch : 0.9430763880411784
Epoch 2, Batch 90, train loss:0.7743126153945923, Elapsed time for epoch : 1.059628184636434
Epoch 2, Batch 100, train loss:0.7968353629112244, Elapsed time for epoch : 1.1759264866511028
Epoch 2, Batch 110, train loss:0.8439378142356873, Elapsed time for epoch : 1.2924663782119752
Batch 0, val loss:2.8109586238861084
Batch 10, val loss:4.474595546722412
Batch 20, val loss:3.4122352600097656
Batch 30, val loss:7.172788143157959
Epoch 2, Train Loss:0.8851430514584417, Val loss:8.611885597308477
Epoch 3, Batch 0, train loss:0.6720537543296814, Elapsed time for epoch : 0.01200257937113444
Epoch 3, Batch 10, train loss:0.7985845804214478, Elapsed time for epoch : 0.12822491725285848
Epoch 3, Batch 20, train loss:0.8522709608078003, Elapsed time for epoch : 0.24498637517293295
Epoch 3, Batch 30, train loss:0.703675389289856, Elapsed time for epoch : 0.3614026427268982
Epoch 3, Batch 40, train loss:0.6881948113441467, Elapsed time for epoch : 0.4777234355608622
Epoch 3, Batch 50, train loss:0.32886457443237305, Elapsed time for epoch : 0.5942719260851542
Epoch 3, Batch 60, train loss:0.731735348701477, Elapsed time for epoch : 0.7110418200492858
Epoch 3, Batch 70, train loss:0.6987885236740112, Elapsed time for epoch : 0.8274670402208965
Epoch 3, Batch 80, train loss:0.6680322289466858, Elapsed time for epoch : 0.9439575831095378
Epoch 3, Batch 90, train loss:0.610522449016571, Elapsed time for epoch : 1.060167920589447
Epoch 3, Batch 100, train loss:0.187259703874588, Elapsed time for epoch : 1.1761619925498963
Epoch 3, Batch 110, train loss:0.5731604695320129, Elapsed time for epoch : 1.2926626245180766
Batch 0, val loss:4.465527534484863
Batch 10, val loss:21.48220443725586
Batch 20, val loss:6.4302659034729
Batch 30, val loss:2.9591827392578125
Epoch 3, Train Loss:0.6343399332917254, Val loss:13.741713649696774
Epoch 4, Batch 0, train loss:1.523667335510254, Elapsed time for epoch : 0.011636686325073243
Epoch 4, Batch 10, train loss:0.2455824315547943, Elapsed time for epoch : 0.12823590834935505
Epoch 4, Batch 20, train loss:0.17868871986865997, Elapsed time for epoch : 0.24462848504384357
Epoch 4, Batch 30, train loss:0.4454323649406433, Elapsed time for epoch : 0.3612134138743083
Epoch 4, Batch 40, train loss:0.508226215839386, Elapsed time for epoch : 0.47802161375681557
Epoch 4, Batch 50, train loss:0.42619258165359497, Elapsed time for epoch : 0.5946502645810445
Epoch 4, Batch 60, train loss:0.4102863371372223, Elapsed time for epoch : 0.71126842101415
Epoch 4, Batch 70, train loss:0.36404916644096375, Elapsed time for epoch : 0.8286026597023011
Epoch 4, Batch 80, train loss:0.18098974227905273, Elapsed time for epoch : 0.945360255241394
Epoch 4, Batch 90, train loss:0.4282021224498749, Elapsed time for epoch : 1.0619605660438538
Epoch 4, Batch 100, train loss:0.43726682662963867, Elapsed time for epoch : 1.178579036394755
Epoch 4, Batch 110, train loss:0.3795817196369171, Elapsed time for epoch : 1.2948566754659017
Batch 0, val loss:10.589686393737793
Batch 10, val loss:3.181077718734741
Batch 20, val loss:12.736327171325684
Batch 30, val loss:4.266078472137451
Epoch 4, Train Loss:0.40660241783961004, Val loss:10.355023539728588
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.4066
wandb:         Val Loss 10.35502
wandb:      train_batch 110
wandb: train_batch_loss 0.37958
wandb:        val_batch 30
wandb:   val_batch_loss 4.26608
wandb: 
wandb: üöÄ View run whole-armadillo-335 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wjepxrvn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_015018-wjepxrvn/logs
Seed completed execution! 23 0.6_3
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_015804-pgrlq7pd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-pine-337
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/pgrlq7pd
Epoch 0, Batch 0, train loss:8.347854614257812, Elapsed time for epoch : 0.013001028696695964
Epoch 0, Batch 10, train loss:4.4242095947265625, Elapsed time for epoch : 0.1291162411371867
Epoch 0, Batch 20, train loss:3.316420793533325, Elapsed time for epoch : 0.24561721483866375
Epoch 0, Batch 30, train loss:3.2183239459991455, Elapsed time for epoch : 0.3623120109240214
Epoch 0, Batch 40, train loss:2.9943695068359375, Elapsed time for epoch : 0.48034185568491616
Epoch 0, Batch 50, train loss:2.6217434406280518, Elapsed time for epoch : 0.596981672445933
Epoch 0, Batch 60, train loss:2.438206911087036, Elapsed time for epoch : 0.7147122780481975
Epoch 0, Batch 70, train loss:2.6038661003112793, Elapsed time for epoch : 0.8312049865722656
Epoch 0, Batch 80, train loss:2.4781172275543213, Elapsed time for epoch : 0.9490212837855021
Epoch 0, Batch 90, train loss:2.157114028930664, Elapsed time for epoch : 1.0657054980595906
Epoch 0, Batch 100, train loss:1.9878718852996826, Elapsed time for epoch : 1.1829684774080913
Epoch 0, Batch 110, train loss:2.0660946369171143, Elapsed time for epoch : 1.299636181195577
Batch 0, val loss:3.182356119155884
Batch 10, val loss:6.03884744644165
Batch 20, val loss:4.6288533210754395
Batch 30, val loss:4.553445339202881
Epoch 0, Train Loss:3.080253503633582, Val loss:3.679094215234121
Epoch 1, Batch 0, train loss:1.976865530014038, Elapsed time for epoch : 0.011667537689208984
Epoch 1, Batch 10, train loss:1.7361021041870117, Elapsed time for epoch : 0.12795983552932738
Epoch 1, Batch 20, train loss:1.4024637937545776, Elapsed time for epoch : 0.24447844823201498
Epoch 1, Batch 30, train loss:1.6138273477554321, Elapsed time for epoch : 0.36079415877660115
Epoch 1, Batch 40, train loss:1.4194071292877197, Elapsed time for epoch : 0.4770358363787333
Epoch 1, Batch 50, train loss:1.515070915222168, Elapsed time for epoch : 0.593986451625824
Epoch 1, Batch 60, train loss:1.5286911725997925, Elapsed time for epoch : 0.7107983827590942
Epoch 1, Batch 70, train loss:1.386800765991211, Elapsed time for epoch : 0.8280218044916788
Epoch 1, Batch 80, train loss:1.2681890726089478, Elapsed time for epoch : 0.9450681368509929
Epoch 1, Batch 90, train loss:1.2399702072143555, Elapsed time for epoch : 1.0616728027661642
Epoch 1, Batch 100, train loss:1.216152548789978, Elapsed time for epoch : 1.1787226398785908
Epoch 1, Batch 110, train loss:0.951231062412262, Elapsed time for epoch : 1.2957169651985168
Batch 0, val loss:2.414094924926758
Batch 10, val loss:6.720540523529053
Batch 20, val loss:3.6803410053253174
Batch 30, val loss:2.798220157623291
Epoch 1, Train Loss:1.4086078633432804, Val loss:5.4510283172130585
Epoch 2, Batch 0, train loss:0.975439727306366, Elapsed time for epoch : 0.011645110448201497
Epoch 2, Batch 10, train loss:0.9305253624916077, Elapsed time for epoch : 0.127620263894399
Epoch 2, Batch 20, train loss:1.0028232336044312, Elapsed time for epoch : 0.24401382207870484
Epoch 2, Batch 30, train loss:0.9999862313270569, Elapsed time for epoch : 0.36012330452601116
Epoch 2, Batch 40, train loss:1.0160218477249146, Elapsed time for epoch : 0.47653572956720985
Epoch 2, Batch 50, train loss:0.9715244174003601, Elapsed time for epoch : 0.5927149136861165
Epoch 2, Batch 60, train loss:0.995136559009552, Elapsed time for epoch : 0.7089005788167317
Epoch 2, Batch 70, train loss:0.5409894585609436, Elapsed time for epoch : 0.8251403927803039
Epoch 2, Batch 80, train loss:0.9897194504737854, Elapsed time for epoch : 0.9414623657862345
Epoch 2, Batch 90, train loss:0.7743126153945923, Elapsed time for epoch : 1.0576758186022441
Epoch 2, Batch 100, train loss:0.7968353629112244, Elapsed time for epoch : 1.1740830024083455
Epoch 2, Batch 110, train loss:0.8439378142356873, Elapsed time for epoch : 1.2904893477757773
Batch 0, val loss:2.8109586238861084
Batch 10, val loss:4.474595546722412
Batch 20, val loss:3.4122352600097656
Batch 30, val loss:7.172788143157959
Epoch 2, Train Loss:0.8851430514584417, Val loss:8.611885597308477
Epoch 3, Batch 0, train loss:0.6720537543296814, Elapsed time for epoch : 0.011614370346069335
Epoch 3, Batch 10, train loss:0.7985845804214478, Elapsed time for epoch : 0.1280521829922994
Epoch 3, Batch 20, train loss:0.8522709608078003, Elapsed time for epoch : 0.24442477226257325
Epoch 3, Batch 30, train loss:0.703675389289856, Elapsed time for epoch : 0.3606266140937805
Epoch 3, Batch 40, train loss:0.6881948113441467, Elapsed time for epoch : 0.477765691280365
Epoch 3, Batch 50, train loss:0.32886457443237305, Elapsed time for epoch : 0.593903382619222
Epoch 3, Batch 60, train loss:0.731735348701477, Elapsed time for epoch : 0.7101763168970744
Epoch 3, Batch 70, train loss:0.6987885236740112, Elapsed time for epoch : 0.826848296324412
Epoch 3, Batch 80, train loss:0.6680322289466858, Elapsed time for epoch : 0.9432350595792135
Epoch 3, Batch 90, train loss:0.610522449016571, Elapsed time for epoch : 1.0595489581425985
Epoch 3, Batch 100, train loss:0.187259703874588, Elapsed time for epoch : 1.1760791579882304
Epoch 3, Batch 110, train loss:0.5731604695320129, Elapsed time for epoch : 1.2928560972213745
Batch 0, val loss:4.465527534484863
Batch 10, val loss:21.48220443725586
Batch 20, val loss:6.4302659034729
Batch 30, val loss:2.9591827392578125
Epoch 3, Train Loss:0.6343399332917254, Val loss:13.741713649696774
Epoch 4, Batch 0, train loss:1.523667335510254, Elapsed time for epoch : 0.011616651217142742
Epoch 4, Batch 10, train loss:0.2455824315547943, Elapsed time for epoch : 0.1281013607978821
Epoch 4, Batch 20, train loss:0.17868871986865997, Elapsed time for epoch : 0.244610595703125
Epoch 4, Batch 30, train loss:0.4454323649406433, Elapsed time for epoch : 0.36080954074859617
Epoch 4, Batch 40, train loss:0.508226215839386, Elapsed time for epoch : 0.47721659342447914
Epoch 4, Batch 50, train loss:0.42619258165359497, Elapsed time for epoch : 0.5941693623860677
Epoch 4, Batch 60, train loss:0.4102863371372223, Elapsed time for epoch : 0.7112087726593017
Epoch 4, Batch 70, train loss:0.36404916644096375, Elapsed time for epoch : 0.8280736645062764
Epoch 4, Batch 80, train loss:0.18098974227905273, Elapsed time for epoch : 0.9444105625152588
Epoch 4, Batch 90, train loss:0.4282021224498749, Elapsed time for epoch : 1.0612524191538493
Epoch 4, Batch 100, train loss:0.43726682662963867, Elapsed time for epoch : 1.1777347366015116
Epoch 4, Batch 110, train loss:0.3795817196369171, Elapsed time for epoch : 1.293761920928955
Batch 0, val loss:10.589686393737793
Batch 10, val loss:3.181077718734741
Batch 20, val loss:12.736327171325684
Batch 30, val loss:4.266078472137451
Epoch 4, Train Loss:0.40660241783961004, Val loss:10.355023539728588
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.157 MB of 0.171 MB uploadedwandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÑ‚ñà‚ñÜ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.4066
wandb:         Val Loss 10.35502
wandb:      train_batch 110
wandb: train_batch_loss 0.37958
wandb:        val_batch 30
wandb:   val_batch_loss 4.26608
wandb: 
wandb: üöÄ View run true-pine-337 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/pgrlq7pd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_015804-pgrlq7pd/logs
Seed completed execution! 113 0.6_3
------------------------------------------------------------------
Experiment complete 0.6_3
==========================================================================
Running experiment for setting 0.6_4
==========================================================================
Running for seed 1 of experiment 0.6_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_020555-8p9dndn0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-glitter-339
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8p9dndn0
Epoch 0, Batch 0, train loss:8.201916694641113, Elapsed time for epoch : 0.01347566843032837
Epoch 0, Batch 10, train loss:3.8698408603668213, Elapsed time for epoch : 0.13674673636754353
Epoch 0, Batch 20, train loss:3.6051363945007324, Elapsed time for epoch : 0.2613579789797465
Epoch 0, Batch 30, train loss:4.275908946990967, Elapsed time for epoch : 0.3858405868212382
Epoch 0, Batch 40, train loss:2.9354474544525146, Elapsed time for epoch : 0.5094232797622681
Epoch 0, Batch 50, train loss:2.6893601417541504, Elapsed time for epoch : 0.6344022631645203
Epoch 0, Batch 60, train loss:2.839431047439575, Elapsed time for epoch : 0.7586143056551615
Epoch 0, Batch 70, train loss:2.566209077835083, Elapsed time for epoch : 0.8825643420219421
Epoch 0, Batch 80, train loss:2.6841788291931152, Elapsed time for epoch : 1.0088259776433308
Epoch 0, Batch 90, train loss:2.4063525199890137, Elapsed time for epoch : 1.1332369963328044
Epoch 0, Batch 100, train loss:2.1383519172668457, Elapsed time for epoch : 1.2591298977533976
Epoch 0, Batch 110, train loss:1.9129552841186523, Elapsed time for epoch : 1.3834877371788026
Batch 0, val loss:3.415508270263672
Batch 10, val loss:3.2805163860321045
Batch 20, val loss:3.02168607711792
Batch 30, val loss:5.252872467041016
Epoch 0, Train Loss:3.201694825421209, Val loss:3.5564355585310192
Epoch 1, Batch 0, train loss:1.9505021572113037, Elapsed time for epoch : 0.011568117141723632
Epoch 1, Batch 10, train loss:1.682411789894104, Elapsed time for epoch : 0.12734474738438925
Epoch 1, Batch 20, train loss:1.5219151973724365, Elapsed time for epoch : 0.243373171488444
Epoch 1, Batch 30, train loss:1.5836563110351562, Elapsed time for epoch : 0.35939433574676516
Epoch 1, Batch 40, train loss:1.6305571794509888, Elapsed time for epoch : 0.47566317319869994
Epoch 1, Batch 50, train loss:1.4893138408660889, Elapsed time for epoch : 0.5918145457903544
Epoch 1, Batch 60, train loss:1.4390026330947876, Elapsed time for epoch : 0.7077269554138184
Epoch 1, Batch 70, train loss:1.4922553300857544, Elapsed time for epoch : 0.8238775889078777
Epoch 1, Batch 80, train loss:1.4381282329559326, Elapsed time for epoch : 0.9402958234151204
Epoch 1, Batch 90, train loss:1.2558658123016357, Elapsed time for epoch : 1.0563366373380025
Epoch 1, Batch 100, train loss:0.8420397043228149, Elapsed time for epoch : 1.1724942843119304
Epoch 1, Batch 110, train loss:1.0849696397781372, Elapsed time for epoch : 1.2891479055086772
Batch 0, val loss:2.5975821018218994
Batch 10, val loss:2.1801302433013916
Batch 20, val loss:2.942821741104126
Batch 30, val loss:2.8271260261535645
Epoch 1, Train Loss:1.4181738423264545, Val loss:3.496010508802202
Epoch 2, Batch 0, train loss:1.0650008916854858, Elapsed time for epoch : 0.011622011661529541
Epoch 2, Batch 10, train loss:1.1297918558120728, Elapsed time for epoch : 0.12763097683588664
Epoch 2, Batch 20, train loss:1.1125010251998901, Elapsed time for epoch : 0.24375683069229126
Epoch 2, Batch 30, train loss:1.1242914199829102, Elapsed time for epoch : 0.3603281855583191
Epoch 2, Batch 40, train loss:1.07365882396698, Elapsed time for epoch : 0.47674251794815065
Epoch 2, Batch 50, train loss:1.1082686185836792, Elapsed time for epoch : 0.5927457849184672
Epoch 2, Batch 60, train loss:1.0945847034454346, Elapsed time for epoch : 0.7092219551404317
Epoch 2, Batch 70, train loss:0.5997000932693481, Elapsed time for epoch : 0.8257310867309571
Epoch 2, Batch 80, train loss:0.9659397006034851, Elapsed time for epoch : 0.9420022805531819
Epoch 2, Batch 90, train loss:0.9919089078903198, Elapsed time for epoch : 1.0581004619598389
Epoch 2, Batch 100, train loss:1.065001130104065, Elapsed time for epoch : 1.1743000070254008
Epoch 2, Batch 110, train loss:0.9280611276626587, Elapsed time for epoch : 1.2909908572832742
Batch 0, val loss:0.8294686079025269
Batch 10, val loss:1.382712483406067
Batch 20, val loss:2.7751553058624268
Batch 30, val loss:2.5172600746154785
Epoch 2, Train Loss:0.9750066593937252, Val loss:3.3526390625370874
Epoch 3, Batch 0, train loss:0.8847484588623047, Elapsed time for epoch : 0.01161882479985555
Epoch 3, Batch 10, train loss:0.9026559591293335, Elapsed time for epoch : 0.12749028205871582
Epoch 3, Batch 20, train loss:0.8741903901100159, Elapsed time for epoch : 0.24368073145548502
Epoch 3, Batch 30, train loss:0.857914388179779, Elapsed time for epoch : 0.35971233050028484
Epoch 3, Batch 40, train loss:0.9126269221305847, Elapsed time for epoch : 0.4760690768559774
Epoch 3, Batch 50, train loss:0.4289584755897522, Elapsed time for epoch : 0.5921571373939514
Epoch 3, Batch 60, train loss:0.8106607794761658, Elapsed time for epoch : 0.7088269273440043
Epoch 3, Batch 70, train loss:0.7859748005867004, Elapsed time for epoch : 0.8254799723625184
Epoch 3, Batch 80, train loss:0.7857734560966492, Elapsed time for epoch : 0.9413365880648296
Epoch 3, Batch 90, train loss:0.7492435574531555, Elapsed time for epoch : 1.0575913190841675
Epoch 3, Batch 100, train loss:0.3358091413974762, Elapsed time for epoch : 1.173810076713562
Epoch 3, Batch 110, train loss:0.7073282599449158, Elapsed time for epoch : 1.2900931119918824
Batch 0, val loss:2.0225942134857178
Batch 10, val loss:2.4303231239318848
Batch 20, val loss:6.2027435302734375
Batch 30, val loss:2.9949536323547363
Epoch 3, Train Loss:0.7412770582282024, Val loss:4.0993592242399854
Epoch 4, Batch 0, train loss:0.8073777556419373, Elapsed time for epoch : 0.011610984802246094
Epoch 4, Batch 10, train loss:0.3460673987865448, Elapsed time for epoch : 0.1279174009958903
Epoch 4, Batch 20, train loss:0.26878732442855835, Elapsed time for epoch : 0.24413634538650514
Epoch 4, Batch 30, train loss:0.6960501670837402, Elapsed time for epoch : 0.3606059233347575
Epoch 4, Batch 40, train loss:0.5888643264770508, Elapsed time for epoch : 0.4770578424135844
Epoch 4, Batch 50, train loss:0.7680158615112305, Elapsed time for epoch : 0.5936750451723735
Epoch 4, Batch 60, train loss:0.5435041189193726, Elapsed time for epoch : 0.7100535154342651
Epoch 4, Batch 70, train loss:0.6059016585350037, Elapsed time for epoch : 0.8268287817637126
Epoch 4, Batch 80, train loss:0.318770170211792, Elapsed time for epoch : 0.9430719176928203
Epoch 4, Batch 90, train loss:0.5295670032501221, Elapsed time for epoch : 1.0592180569966634
Epoch 4, Batch 100, train loss:0.5973321199417114, Elapsed time for epoch : 1.175414784749349
Epoch 4, Batch 110, train loss:0.5226215720176697, Elapsed time for epoch : 1.2918100158373516
Batch 0, val loss:1.1867196559906006
Batch 10, val loss:2.0738983154296875
Batch 20, val loss:3.467634916305542
Batch 30, val loss:2.5823700428009033
Epoch 4, Train Loss:0.5364998360042986, Val loss:4.2264144122600555
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.5365
wandb:         Val Loss 4.22641
wandb:      train_batch 110
wandb: train_batch_loss 0.52262
wandb:        val_batch 30
wandb:   val_batch_loss 2.58237
wandb: 
wandb: üöÄ View run silver-glitter-339 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8p9dndn0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_020555-8p9dndn0/logs
Seed completed execution! 1 0.6_4
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_021349-bftoqz96
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-paper-341
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bftoqz96
Epoch 0, Batch 0, train loss:8.201916694641113, Elapsed time for epoch : 0.01343068281809489
Epoch 0, Batch 10, train loss:3.8698408603668213, Elapsed time for epoch : 0.12890207370122272
Epoch 0, Batch 20, train loss:3.6051363945007324, Elapsed time for epoch : 0.24488532940546673
Epoch 0, Batch 30, train loss:4.275908946990967, Elapsed time for epoch : 0.3608026742935181
Epoch 0, Batch 40, train loss:2.9354474544525146, Elapsed time for epoch : 0.47682839234670005
Epoch 0, Batch 50, train loss:2.6893601417541504, Elapsed time for epoch : 0.5929434299468994
Epoch 0, Batch 60, train loss:2.839431047439575, Elapsed time for epoch : 0.7093258659044902
Epoch 0, Batch 70, train loss:2.566209077835083, Elapsed time for epoch : 0.8254436055819193
Epoch 0, Batch 80, train loss:2.6841788291931152, Elapsed time for epoch : 0.9414069334665934
Epoch 0, Batch 90, train loss:2.4063525199890137, Elapsed time for epoch : 1.057128409544627
Epoch 0, Batch 100, train loss:2.1383519172668457, Elapsed time for epoch : 1.173361392815908
Epoch 0, Batch 110, train loss:1.9129552841186523, Elapsed time for epoch : 1.2896740595499674
Batch 0, val loss:3.415508270263672
Batch 10, val loss:3.2805163860321045
Batch 20, val loss:3.02168607711792
Batch 30, val loss:5.252872467041016
Epoch 0, Train Loss:3.201694825421209, Val loss:3.5564355585310192
Epoch 1, Batch 0, train loss:1.9505021572113037, Elapsed time for epoch : 0.011689281463623047
Epoch 1, Batch 10, train loss:1.682411789894104, Elapsed time for epoch : 0.12760570844014485
Epoch 1, Batch 20, train loss:1.5219151973724365, Elapsed time for epoch : 0.24400150775909424
Epoch 1, Batch 30, train loss:1.5836563110351562, Elapsed time for epoch : 0.36014742056528726
Epoch 1, Batch 40, train loss:1.6305571794509888, Elapsed time for epoch : 0.47607033252716063
Epoch 1, Batch 50, train loss:1.4893138408660889, Elapsed time for epoch : 0.592881727218628
Epoch 1, Batch 60, train loss:1.4390026330947876, Elapsed time for epoch : 0.709299639860789
Epoch 1, Batch 70, train loss:1.4922553300857544, Elapsed time for epoch : 0.8256349960962931
Epoch 1, Batch 80, train loss:1.4381282329559326, Elapsed time for epoch : 0.9417376120885214
Epoch 1, Batch 90, train loss:1.2558658123016357, Elapsed time for epoch : 1.0579644044240315
Epoch 1, Batch 100, train loss:0.8420397043228149, Elapsed time for epoch : 1.174256114164988
Epoch 1, Batch 110, train loss:1.0849696397781372, Elapsed time for epoch : 1.2907396316528321
Batch 0, val loss:2.5975821018218994
Batch 10, val loss:2.1801302433013916
Batch 20, val loss:2.942821741104126
Batch 30, val loss:2.8271260261535645
Epoch 1, Train Loss:1.4181738423264545, Val loss:3.496010508802202
Epoch 2, Batch 0, train loss:1.0650008916854858, Elapsed time for epoch : 0.011623195807139079
Epoch 2, Batch 10, train loss:1.1297918558120728, Elapsed time for epoch : 0.1276300589243571
Epoch 2, Batch 20, train loss:1.1125010251998901, Elapsed time for epoch : 0.2443438688913981
Epoch 2, Batch 30, train loss:1.1242914199829102, Elapsed time for epoch : 0.3606124480565389
Epoch 2, Batch 40, train loss:1.07365882396698, Elapsed time for epoch : 0.47710389693578087
Epoch 2, Batch 50, train loss:1.1082686185836792, Elapsed time for epoch : 0.5936992367108663
Epoch 2, Batch 60, train loss:1.0945847034454346, Elapsed time for epoch : 0.7100556532541911
Epoch 2, Batch 70, train loss:0.5997000932693481, Elapsed time for epoch : 0.8262629310290018
Epoch 2, Batch 80, train loss:0.9659397006034851, Elapsed time for epoch : 0.9426300883293152
Epoch 2, Batch 90, train loss:0.9919089078903198, Elapsed time for epoch : 1.0588807344436646
Epoch 2, Batch 100, train loss:1.065001130104065, Elapsed time for epoch : 1.1749524871508281
Epoch 2, Batch 110, train loss:0.9280611276626587, Elapsed time for epoch : 1.2913910786310832
Batch 0, val loss:0.8294686079025269
Batch 10, val loss:1.382712483406067
Batch 20, val loss:2.7751553058624268
Batch 30, val loss:2.5172600746154785
Epoch 2, Train Loss:0.9750066593937252, Val loss:3.3526390625370874
Epoch 3, Batch 0, train loss:0.8847484588623047, Elapsed time for epoch : 0.01175673007965088
Epoch 3, Batch 10, train loss:0.9026559591293335, Elapsed time for epoch : 0.12788623968760174
Epoch 3, Batch 20, train loss:0.8741903901100159, Elapsed time for epoch : 0.2441876769065857
Epoch 3, Batch 30, train loss:0.857914388179779, Elapsed time for epoch : 0.3609306772549947
Epoch 3, Batch 40, train loss:0.9126269221305847, Elapsed time for epoch : 0.47737937768300376
Epoch 3, Batch 50, train loss:0.4289584755897522, Elapsed time for epoch : 0.593599271774292
Epoch 3, Batch 60, train loss:0.8106607794761658, Elapsed time for epoch : 0.7103498379389445
Epoch 3, Batch 70, train loss:0.7859748005867004, Elapsed time for epoch : 0.8268861850102742
Epoch 3, Batch 80, train loss:0.7857734560966492, Elapsed time for epoch : 0.9430262605349223
Epoch 3, Batch 90, train loss:0.7492435574531555, Elapsed time for epoch : 1.0595973451932272
Epoch 3, Batch 100, train loss:0.3358091413974762, Elapsed time for epoch : 1.1763441602389018
Epoch 3, Batch 110, train loss:0.7073282599449158, Elapsed time for epoch : 1.2927944858868916
Batch 0, val loss:2.0225942134857178
Batch 10, val loss:2.4303231239318848
Batch 20, val loss:6.2027435302734375
Batch 30, val loss:2.9949536323547363
Epoch 3, Train Loss:0.7412770582282024, Val loss:4.0993592242399854
Epoch 4, Batch 0, train loss:0.8073777556419373, Elapsed time for epoch : 0.011625707149505615
Epoch 4, Batch 10, train loss:0.3460673987865448, Elapsed time for epoch : 0.12789045174916586
Epoch 4, Batch 20, train loss:0.26878732442855835, Elapsed time for epoch : 0.24439290364583333
Epoch 4, Batch 30, train loss:0.6960501670837402, Elapsed time for epoch : 0.3607800563176473
Epoch 4, Batch 40, train loss:0.5888643264770508, Elapsed time for epoch : 0.4768062988917033
Epoch 4, Batch 50, train loss:0.7680158615112305, Elapsed time for epoch : 0.5931992928187052
Epoch 4, Batch 60, train loss:0.5435041189193726, Elapsed time for epoch : 0.7092414895693461
Epoch 4, Batch 70, train loss:0.6059016585350037, Elapsed time for epoch : 0.8256369868914286
Epoch 4, Batch 80, train loss:0.318770170211792, Elapsed time for epoch : 0.9421000123023987
Epoch 4, Batch 90, train loss:0.5295670032501221, Elapsed time for epoch : 1.0582494378089904
Epoch 4, Batch 100, train loss:0.5973321199417114, Elapsed time for epoch : 1.1747217535972596
Epoch 4, Batch 110, train loss:0.5226215720176697, Elapsed time for epoch : 1.29106156428655
Batch 0, val loss:1.1867196559906006
Batch 10, val loss:2.0738983154296875
Batch 20, val loss:3.467634916305542
Batch 30, val loss:2.5823700428009033
Epoch 4, Train Loss:0.5364998360042986, Val loss:4.2264144122600555
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.5365
wandb:         Val Loss 4.22641
wandb:      train_batch 110
wandb: train_batch_loss 0.52262
wandb:        val_batch 30
wandb:   val_batch_loss 2.58237
wandb: 
wandb: üöÄ View run fallen-paper-341 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/bftoqz96
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_021349-bftoqz96/logs
Seed completed execution! 42 0.6_4
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_022135-xfzze0e7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-fog-343
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xfzze0e7
Epoch 0, Batch 0, train loss:8.201916694641113, Elapsed time for epoch : 0.01333537499109904
Epoch 0, Batch 10, train loss:3.8698408603668213, Elapsed time for epoch : 0.12945361534754435
Epoch 0, Batch 20, train loss:3.6051363945007324, Elapsed time for epoch : 0.24533523718516032
Epoch 0, Batch 30, train loss:4.275908946990967, Elapsed time for epoch : 0.361018443107605
Epoch 0, Batch 40, train loss:2.9354474544525146, Elapsed time for epoch : 0.47701934973398846
Epoch 0, Batch 50, train loss:2.6893601417541504, Elapsed time for epoch : 0.592762005329132
Epoch 0, Batch 60, train loss:2.839431047439575, Elapsed time for epoch : 0.7086556116739909
Epoch 0, Batch 70, train loss:2.566209077835083, Elapsed time for epoch : 0.8245706359545389
Epoch 0, Batch 80, train loss:2.6841788291931152, Elapsed time for epoch : 0.9406397104263305
Epoch 0, Batch 90, train loss:2.4063525199890137, Elapsed time for epoch : 1.0571380813916524
Epoch 0, Batch 100, train loss:2.1383519172668457, Elapsed time for epoch : 1.1738242626190185
Epoch 0, Batch 110, train loss:1.9129552841186523, Elapsed time for epoch : 1.2905184706052144
Batch 0, val loss:3.415508270263672
Batch 10, val loss:3.2805163860321045
Batch 20, val loss:3.02168607711792
Batch 30, val loss:5.252872467041016
Epoch 0, Train Loss:3.201694825421209, Val loss:3.5564355585310192
Epoch 1, Batch 0, train loss:1.9505021572113037, Elapsed time for epoch : 0.011652910709381103
Epoch 1, Batch 10, train loss:1.682411789894104, Elapsed time for epoch : 0.12765222787857056
Epoch 1, Batch 20, train loss:1.5219151973724365, Elapsed time for epoch : 0.24418809016545615
Epoch 1, Batch 30, train loss:1.5836563110351562, Elapsed time for epoch : 0.36056600014368695
Epoch 1, Batch 40, train loss:1.6305571794509888, Elapsed time for epoch : 0.47643040418624877
Epoch 1, Batch 50, train loss:1.4893138408660889, Elapsed time for epoch : 0.5924011866251627
Epoch 1, Batch 60, train loss:1.4390026330947876, Elapsed time for epoch : 0.7082745393117269
Epoch 1, Batch 70, train loss:1.4922553300857544, Elapsed time for epoch : 0.8249368151028951
Epoch 1, Batch 80, train loss:1.4381282329559326, Elapsed time for epoch : 0.9418063720067342
Epoch 1, Batch 90, train loss:1.2558658123016357, Elapsed time for epoch : 1.057738709449768
Epoch 1, Batch 100, train loss:0.8420397043228149, Elapsed time for epoch : 1.1741713166236878
Epoch 1, Batch 110, train loss:1.0849696397781372, Elapsed time for epoch : 1.2907615900039673
Batch 0, val loss:2.5975821018218994
Batch 10, val loss:2.1801302433013916
Batch 20, val loss:2.942821741104126
Batch 30, val loss:2.8271260261535645
Epoch 1, Train Loss:1.4181738423264545, Val loss:3.496010508802202
Epoch 2, Batch 0, train loss:1.0650008916854858, Elapsed time for epoch : 0.01163718303044637
Epoch 2, Batch 10, train loss:1.1297918558120728, Elapsed time for epoch : 0.12786531050999958
Epoch 2, Batch 20, train loss:1.1125010251998901, Elapsed time for epoch : 0.24432020982106525
Epoch 2, Batch 30, train loss:1.1242914199829102, Elapsed time for epoch : 0.36091335217158
Epoch 2, Batch 40, train loss:1.07365882396698, Elapsed time for epoch : 0.4773152987162272
Epoch 2, Batch 50, train loss:1.1082686185836792, Elapsed time for epoch : 0.5939242998758952
Epoch 2, Batch 60, train loss:1.0945847034454346, Elapsed time for epoch : 0.7100330789883932
Epoch 2, Batch 70, train loss:0.5997000932693481, Elapsed time for epoch : 0.826114829381307
Epoch 2, Batch 80, train loss:0.9659397006034851, Elapsed time for epoch : 0.942962117989858
Epoch 2, Batch 90, train loss:0.9919089078903198, Elapsed time for epoch : 1.0592934489250183
Epoch 2, Batch 100, train loss:1.065001130104065, Elapsed time for epoch : 1.1755219181378682
Epoch 2, Batch 110, train loss:0.9280611276626587, Elapsed time for epoch : 1.2920228600502015
Batch 0, val loss:0.8294686079025269
Batch 10, val loss:1.382712483406067
Batch 20, val loss:2.7751553058624268
Batch 30, val loss:2.5172600746154785
Epoch 2, Train Loss:0.9750066593937252, Val loss:3.3526390625370874
Epoch 3, Batch 0, train loss:0.8847484588623047, Elapsed time for epoch : 0.011775954564412435
Epoch 3, Batch 10, train loss:0.9026559591293335, Elapsed time for epoch : 0.1278281291325887
Epoch 3, Batch 20, train loss:0.8741903901100159, Elapsed time for epoch : 0.24397014776865641
Epoch 3, Batch 30, train loss:0.857914388179779, Elapsed time for epoch : 0.36084157625834146
Epoch 3, Batch 40, train loss:0.9126269221305847, Elapsed time for epoch : 0.4770003358523051
Epoch 3, Batch 50, train loss:0.4289584755897522, Elapsed time for epoch : 0.5931649327278137
Epoch 3, Batch 60, train loss:0.8106607794761658, Elapsed time for epoch : 0.709790563583374
Epoch 3, Batch 70, train loss:0.7859748005867004, Elapsed time for epoch : 0.8261863907178243
Epoch 3, Batch 80, train loss:0.7857734560966492, Elapsed time for epoch : 0.9424222111701965
Epoch 3, Batch 90, train loss:0.7492435574531555, Elapsed time for epoch : 1.0592194159825643
Epoch 3, Batch 100, train loss:0.3358091413974762, Elapsed time for epoch : 1.1757461786270142
Epoch 3, Batch 110, train loss:0.7073282599449158, Elapsed time for epoch : 1.2923346598943075
Batch 0, val loss:2.0225942134857178
Batch 10, val loss:2.4303231239318848
Batch 20, val loss:6.2027435302734375
Batch 30, val loss:2.9949536323547363
Epoch 3, Train Loss:0.7412770582282024, Val loss:4.0993592242399854
Epoch 4, Batch 0, train loss:0.8073777556419373, Elapsed time for epoch : 0.011896653970082601
Epoch 4, Batch 10, train loss:0.3460673987865448, Elapsed time for epoch : 0.12850422859191896
Epoch 4, Batch 20, train loss:0.26878732442855835, Elapsed time for epoch : 0.245048189163208
Epoch 4, Batch 30, train loss:0.6960501670837402, Elapsed time for epoch : 0.3614718794822693
Epoch 4, Batch 40, train loss:0.5888643264770508, Elapsed time for epoch : 0.47780025005340576
Epoch 4, Batch 50, train loss:0.7680158615112305, Elapsed time for epoch : 0.5943708340326945
Epoch 4, Batch 60, train loss:0.5435041189193726, Elapsed time for epoch : 0.7107876817385356
Epoch 4, Batch 70, train loss:0.6059016585350037, Elapsed time for epoch : 0.8274553577105205
Epoch 4, Batch 80, train loss:0.318770170211792, Elapsed time for epoch : 0.9438563545544942
Epoch 4, Batch 90, train loss:0.5295670032501221, Elapsed time for epoch : 1.0604692379633585
Epoch 4, Batch 100, train loss:0.5973321199417114, Elapsed time for epoch : 1.176837960879008
Epoch 4, Batch 110, train loss:0.5226215720176697, Elapsed time for epoch : 1.293099053700765
Batch 0, val loss:1.1867196559906006
Batch 10, val loss:2.0738983154296875
Batch 20, val loss:3.467634916305542
Batch 30, val loss:2.5823700428009033
Epoch 4, Train Loss:0.5364998360042986, Val loss:4.2264144122600555
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.5365
wandb:         Val Loss 4.22641
wandb:      train_batch 110
wandb: train_batch_loss 0.52262
wandb:        val_batch 30
wandb:   val_batch_loss 2.58237
wandb: 
wandb: üöÄ View run crisp-fog-343 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xfzze0e7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_022135-xfzze0e7/logs
Seed completed execution! 89 0.6_4
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_022921-wlvkhf1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-serenity-345
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wlvkhf1q
Epoch 0, Batch 0, train loss:8.201916694641113, Elapsed time for epoch : 0.013211278120676677
Epoch 0, Batch 10, train loss:3.8698408603668213, Elapsed time for epoch : 0.12927283048629762
Epoch 0, Batch 20, train loss:3.6051363945007324, Elapsed time for epoch : 0.2449838121732076
Epoch 0, Batch 30, train loss:4.275908946990967, Elapsed time for epoch : 0.3611268401145935
Epoch 0, Batch 40, train loss:2.9354474544525146, Elapsed time for epoch : 0.4780584692955017
Epoch 0, Batch 50, train loss:2.6893601417541504, Elapsed time for epoch : 0.5946777900060017
Epoch 0, Batch 60, train loss:2.839431047439575, Elapsed time for epoch : 0.711039145787557
Epoch 0, Batch 70, train loss:2.566209077835083, Elapsed time for epoch : 0.8276672283808391
Epoch 0, Batch 80, train loss:2.6841788291931152, Elapsed time for epoch : 0.9442458828290303
Epoch 0, Batch 90, train loss:2.4063525199890137, Elapsed time for epoch : 1.060552422205607
Epoch 0, Batch 100, train loss:2.1383519172668457, Elapsed time for epoch : 1.177015443642934
Epoch 0, Batch 110, train loss:1.9129552841186523, Elapsed time for epoch : 1.2932871063550313
Batch 0, val loss:3.415508270263672
Batch 10, val loss:3.2805163860321045
Batch 20, val loss:3.02168607711792
Batch 30, val loss:5.252872467041016
Epoch 0, Train Loss:3.201694825421209, Val loss:3.5564355585310192
Epoch 1, Batch 0, train loss:1.9505021572113037, Elapsed time for epoch : 0.011640425523122151
Epoch 1, Batch 10, train loss:1.682411789894104, Elapsed time for epoch : 0.12797156969706217
Epoch 1, Batch 20, train loss:1.5219151973724365, Elapsed time for epoch : 0.24421069224675496
Epoch 1, Batch 30, train loss:1.5836563110351562, Elapsed time for epoch : 0.3604088187217712
Epoch 1, Batch 40, train loss:1.6305571794509888, Elapsed time for epoch : 0.4772321502367655
Epoch 1, Batch 50, train loss:1.4893138408660889, Elapsed time for epoch : 0.5935156027475993
Epoch 1, Batch 60, train loss:1.4390026330947876, Elapsed time for epoch : 0.7098626534144084
Epoch 1, Batch 70, train loss:1.4922553300857544, Elapsed time for epoch : 0.8263610601425171
Epoch 1, Batch 80, train loss:1.4381282329559326, Elapsed time for epoch : 0.9425683895746867
Epoch 1, Batch 90, train loss:1.2558658123016357, Elapsed time for epoch : 1.0589870929718017
Epoch 1, Batch 100, train loss:0.8420397043228149, Elapsed time for epoch : 1.175689935684204
Epoch 1, Batch 110, train loss:1.0849696397781372, Elapsed time for epoch : 1.291958991686503
Batch 0, val loss:2.5975821018218994
Batch 10, val loss:2.1801302433013916
Batch 20, val loss:2.942821741104126
Batch 30, val loss:2.8271260261535645
Epoch 1, Train Loss:1.4181738423264545, Val loss:3.496010508802202
Epoch 2, Batch 0, train loss:1.0650008916854858, Elapsed time for epoch : 0.011641037464141846
Epoch 2, Batch 10, train loss:1.1297918558120728, Elapsed time for epoch : 0.12797677119572956
Epoch 2, Batch 20, train loss:1.1125010251998901, Elapsed time for epoch : 0.24419175783793132
Epoch 2, Batch 30, train loss:1.1242914199829102, Elapsed time for epoch : 0.36074368556340536
Epoch 2, Batch 40, train loss:1.07365882396698, Elapsed time for epoch : 0.47726964950561523
Epoch 2, Batch 50, train loss:1.1082686185836792, Elapsed time for epoch : 0.593656845887502
Epoch 2, Batch 60, train loss:1.0945847034454346, Elapsed time for epoch : 0.7097066481908162
Epoch 2, Batch 70, train loss:0.5997000932693481, Elapsed time for epoch : 0.8259650230407715
Epoch 2, Batch 80, train loss:0.9659397006034851, Elapsed time for epoch : 0.9426851709683736
Epoch 2, Batch 90, train loss:0.9919089078903198, Elapsed time for epoch : 1.0588918407758077
Epoch 2, Batch 100, train loss:1.065001130104065, Elapsed time for epoch : 1.1751721382141114
Epoch 2, Batch 110, train loss:0.9280611276626587, Elapsed time for epoch : 1.2918514927228293
Batch 0, val loss:0.8294686079025269
Batch 10, val loss:1.382712483406067
Batch 20, val loss:2.7751553058624268
Batch 30, val loss:2.5172600746154785
Epoch 2, Train Loss:0.9750066593937252, Val loss:3.3526390625370874
Epoch 3, Batch 0, train loss:0.8847484588623047, Elapsed time for epoch : 0.011627320448557537
Epoch 3, Batch 10, train loss:0.9026559591293335, Elapsed time for epoch : 0.1280160864194234
Epoch 3, Batch 20, train loss:0.8741903901100159, Elapsed time for epoch : 0.2444565176963806
Epoch 3, Batch 30, train loss:0.857914388179779, Elapsed time for epoch : 0.3608352780342102
Epoch 3, Batch 40, train loss:0.9126269221305847, Elapsed time for epoch : 0.47724661429723103
Epoch 3, Batch 50, train loss:0.4289584755897522, Elapsed time for epoch : 0.5932289918263753
Epoch 3, Batch 60, train loss:0.8106607794761658, Elapsed time for epoch : 0.709786840279897
Epoch 3, Batch 70, train loss:0.7859748005867004, Elapsed time for epoch : 0.8262394030888875
Epoch 3, Batch 80, train loss:0.7857734560966492, Elapsed time for epoch : 0.9430942972501118
Epoch 3, Batch 90, train loss:0.7492435574531555, Elapsed time for epoch : 1.0598141511281332
Epoch 3, Batch 100, train loss:0.3358091413974762, Elapsed time for epoch : 1.1759795109430948
Epoch 3, Batch 110, train loss:0.7073282599449158, Elapsed time for epoch : 1.2925079544385274
Batch 0, val loss:2.0225942134857178
Batch 10, val loss:2.4303231239318848
Batch 20, val loss:6.2027435302734375
Batch 30, val loss:2.9949536323547363
Epoch 3, Train Loss:0.7412770582282024, Val loss:4.0993592242399854
Epoch 4, Batch 0, train loss:0.8073777556419373, Elapsed time for epoch : 0.011637115478515625
Epoch 4, Batch 10, train loss:0.3460673987865448, Elapsed time for epoch : 0.12789602677027384
Epoch 4, Batch 20, train loss:0.26878732442855835, Elapsed time for epoch : 0.24427144924799601
Epoch 4, Batch 30, train loss:0.6960501670837402, Elapsed time for epoch : 0.36060767571131386
Epoch 4, Batch 40, train loss:0.5888643264770508, Elapsed time for epoch : 0.476626984278361
Epoch 4, Batch 50, train loss:0.7680158615112305, Elapsed time for epoch : 0.5930279731750489
Epoch 4, Batch 60, train loss:0.5435041189193726, Elapsed time for epoch : 0.7092514514923096
Epoch 4, Batch 70, train loss:0.6059016585350037, Elapsed time for epoch : 0.8256653785705567
Epoch 4, Batch 80, train loss:0.318770170211792, Elapsed time for epoch : 0.942047941684723
Epoch 4, Batch 90, train loss:0.5295670032501221, Elapsed time for epoch : 1.0585162361462912
Epoch 4, Batch 100, train loss:0.5973321199417114, Elapsed time for epoch : 1.1749021649360656
Epoch 4, Batch 110, train loss:0.5226215720176697, Elapsed time for epoch : 1.2912919123967488
Batch 0, val loss:1.1867196559906006
Batch 10, val loss:2.0738983154296875
Batch 20, val loss:3.467634916305542
Batch 30, val loss:2.5823700428009033
Epoch 4, Train Loss:0.5364998360042986, Val loss:4.2264144122600555
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.5365
wandb:         Val Loss 4.22641
wandb:      train_batch 110
wandb: train_batch_loss 0.52262
wandb:        val_batch 30
wandb:   val_batch_loss 2.58237
wandb: 
wandb: üöÄ View run cool-serenity-345 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wlvkhf1q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_022921-wlvkhf1q/logs
Seed completed execution! 23 0.6_4
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_023711-z3krmjpl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-butterfly-347
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/z3krmjpl
Epoch 0, Batch 0, train loss:8.201916694641113, Elapsed time for epoch : 0.01301181713740031
Epoch 0, Batch 10, train loss:3.8698408603668213, Elapsed time for epoch : 0.13058130343755087
Epoch 0, Batch 20, train loss:3.6051363945007324, Elapsed time for epoch : 0.24756424029668173
Epoch 0, Batch 30, train loss:4.275908946990967, Elapsed time for epoch : 0.3640554984410604
Epoch 0, Batch 40, train loss:2.9354474544525146, Elapsed time for epoch : 0.4807118773460388
Epoch 0, Batch 50, train loss:2.6893601417541504, Elapsed time for epoch : 0.597495683034261
Epoch 0, Batch 60, train loss:2.839431047439575, Elapsed time for epoch : 0.7139309565226237
Epoch 0, Batch 70, train loss:2.566209077835083, Elapsed time for epoch : 0.830667237440745
Epoch 0, Batch 80, train loss:2.6841788291931152, Elapsed time for epoch : 0.9483258763949076
Epoch 0, Batch 90, train loss:2.4063525199890137, Elapsed time for epoch : 1.0650212208429972
Epoch 0, Batch 100, train loss:2.1383519172668457, Elapsed time for epoch : 1.1820913434028626
Epoch 0, Batch 110, train loss:1.9129552841186523, Elapsed time for epoch : 1.299082628885905
Batch 0, val loss:3.415508270263672
Batch 10, val loss:3.2805163860321045
Batch 20, val loss:3.02168607711792
Batch 30, val loss:5.252872467041016
Epoch 0, Train Loss:3.201694825421209, Val loss:3.5564355585310192
Epoch 1, Batch 0, train loss:1.9505021572113037, Elapsed time for epoch : 0.011841098467508951
Epoch 1, Batch 10, train loss:1.682411789894104, Elapsed time for epoch : 0.127914293607076
Epoch 1, Batch 20, train loss:1.5219151973724365, Elapsed time for epoch : 0.24422396421432496
Epoch 1, Batch 30, train loss:1.5836563110351562, Elapsed time for epoch : 0.3603667418162028
Epoch 1, Batch 40, train loss:1.6305571794509888, Elapsed time for epoch : 0.4769297877947489
Epoch 1, Batch 50, train loss:1.4893138408660889, Elapsed time for epoch : 0.5930714289347331
Epoch 1, Batch 60, train loss:1.4390026330947876, Elapsed time for epoch : 0.7091002821922302
Epoch 1, Batch 70, train loss:1.4922553300857544, Elapsed time for epoch : 0.8255617062250773
Epoch 1, Batch 80, train loss:1.4381282329559326, Elapsed time for epoch : 0.9419308066368103
Epoch 1, Batch 90, train loss:1.2558658123016357, Elapsed time for epoch : 1.0585585514704385
Epoch 1, Batch 100, train loss:0.8420397043228149, Elapsed time for epoch : 1.174913465976715
Epoch 1, Batch 110, train loss:1.0849696397781372, Elapsed time for epoch : 1.2912686586380004
Batch 0, val loss:2.5975821018218994
Batch 10, val loss:2.1801302433013916
Batch 20, val loss:2.942821741104126
Batch 30, val loss:2.8271260261535645
Epoch 1, Train Loss:1.4181738423264545, Val loss:3.496010508802202
Epoch 2, Batch 0, train loss:1.0650008916854858, Elapsed time for epoch : 0.011622679233551026
Epoch 2, Batch 10, train loss:1.1297918558120728, Elapsed time for epoch : 0.12812415758768717
Epoch 2, Batch 20, train loss:1.1125010251998901, Elapsed time for epoch : 0.2445254643758138
Epoch 2, Batch 30, train loss:1.1242914199829102, Elapsed time for epoch : 0.36102678775787356
Epoch 2, Batch 40, train loss:1.07365882396698, Elapsed time for epoch : 0.4785165270169576
Epoch 2, Batch 50, train loss:1.1082686185836792, Elapsed time for epoch : 0.5950833002726237
Epoch 2, Batch 60, train loss:1.0945847034454346, Elapsed time for epoch : 0.7113777160644531
Epoch 2, Batch 70, train loss:0.5997000932693481, Elapsed time for epoch : 0.8278472463289896
Epoch 2, Batch 80, train loss:0.9659397006034851, Elapsed time for epoch : 0.9444560209910074
Epoch 2, Batch 90, train loss:0.9919089078903198, Elapsed time for epoch : 1.0607937296231589
Epoch 2, Batch 100, train loss:1.065001130104065, Elapsed time for epoch : 1.1773349205652872
Epoch 2, Batch 110, train loss:0.9280611276626587, Elapsed time for epoch : 1.2936448613802591
Batch 0, val loss:0.8294686079025269
Batch 10, val loss:1.382712483406067
Batch 20, val loss:2.7751553058624268
Batch 30, val loss:2.5172600746154785
Epoch 2, Train Loss:0.9750066593937252, Val loss:3.3526390625370874
Epoch 3, Batch 0, train loss:0.8847484588623047, Elapsed time for epoch : 0.011630074183146159
Epoch 3, Batch 10, train loss:0.9026559591293335, Elapsed time for epoch : 0.12811252673467
Epoch 3, Batch 20, train loss:0.8741903901100159, Elapsed time for epoch : 0.24442152976989745
Epoch 3, Batch 30, train loss:0.857914388179779, Elapsed time for epoch : 0.3606302857398987
Epoch 3, Batch 40, train loss:0.9126269221305847, Elapsed time for epoch : 0.47714606126149495
Epoch 3, Batch 50, train loss:0.4289584755897522, Elapsed time for epoch : 0.5935003757476807
Epoch 3, Batch 60, train loss:0.8106607794761658, Elapsed time for epoch : 0.7099366664886475
Epoch 3, Batch 70, train loss:0.7859748005867004, Elapsed time for epoch : 0.8261764287948609
Epoch 3, Batch 80, train loss:0.7857734560966492, Elapsed time for epoch : 0.9432304898897806
Epoch 3, Batch 90, train loss:0.7492435574531555, Elapsed time for epoch : 1.0593936363855998
Epoch 3, Batch 100, train loss:0.3358091413974762, Elapsed time for epoch : 1.1756391684214273
Epoch 3, Batch 110, train loss:0.7073282599449158, Elapsed time for epoch : 1.2917908787727357
Batch 0, val loss:2.0225942134857178
Batch 10, val loss:2.4303231239318848
Batch 20, val loss:6.2027435302734375
Batch 30, val loss:2.9949536323547363
Epoch 3, Train Loss:0.7412770582282024, Val loss:4.0993592242399854
Epoch 4, Batch 0, train loss:0.8073777556419373, Elapsed time for epoch : 0.01161354382832845
Epoch 4, Batch 10, train loss:0.3460673987865448, Elapsed time for epoch : 0.1283124844233195
Epoch 4, Batch 20, train loss:0.26878732442855835, Elapsed time for epoch : 0.2448487361272176
Epoch 4, Batch 30, train loss:0.6960501670837402, Elapsed time for epoch : 0.36152780055999756
Epoch 4, Batch 40, train loss:0.5888643264770508, Elapsed time for epoch : 0.47811899582544964
Epoch 4, Batch 50, train loss:0.7680158615112305, Elapsed time for epoch : 0.5945265968640645
Epoch 4, Batch 60, train loss:0.5435041189193726, Elapsed time for epoch : 0.711193327109019
Epoch 4, Batch 70, train loss:0.6059016585350037, Elapsed time for epoch : 0.8276434938112894
Epoch 4, Batch 80, train loss:0.318770170211792, Elapsed time for epoch : 0.9443710565567016
Epoch 4, Batch 90, train loss:0.5295670032501221, Elapsed time for epoch : 1.0608899394671123
Epoch 4, Batch 100, train loss:0.5973321199417114, Elapsed time for epoch : 1.177195429801941
Epoch 4, Batch 110, train loss:0.5226215720176697, Elapsed time for epoch : 1.2941142797470093
Batch 0, val loss:1.1867196559906006
Batch 10, val loss:2.0738983154296875
Batch 20, val loss:3.467634916305542
Batch 30, val loss:2.5823700428009033
Epoch 4, Train Loss:0.5364998360042986, Val loss:4.2264144122600555
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.5365
wandb:         Val Loss 4.22641
wandb:      train_batch 110
wandb: train_batch_loss 0.52262
wandb:        val_batch 30
wandb:   val_batch_loss 2.58237
wandb: 
wandb: üöÄ View run youthful-butterfly-347 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/z3krmjpl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_023711-z3krmjpl/logs
Seed completed execution! 113 0.6_4
------------------------------------------------------------------
Experiment complete 0.6_4
==========================================================================
Running experiment for setting 0.6_5
==========================================================================
Running for seed 1 of experiment 0.6_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_024459-cnarx65j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fast-morning-349
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/cnarx65j
Epoch 0, Batch 0, train loss:8.643214225769043, Elapsed time for epoch : 0.01353780428568522
Epoch 0, Batch 10, train loss:3.8921637535095215, Elapsed time for epoch : 0.13753648201624552
Epoch 0, Batch 20, train loss:3.770404100418091, Elapsed time for epoch : 0.26249761978785197
Epoch 0, Batch 30, train loss:3.1424560546875, Elapsed time for epoch : 0.3867024699846903
Epoch 0, Batch 40, train loss:3.313298463821411, Elapsed time for epoch : 0.5131205081939697
Epoch 0, Batch 50, train loss:3.0310306549072266, Elapsed time for epoch : 0.6370768229166667
Epoch 0, Batch 60, train loss:2.7981626987457275, Elapsed time for epoch : 0.7603054642677307
Epoch 0, Batch 70, train loss:2.776331663131714, Elapsed time for epoch : 0.8848649342854817
Epoch 0, Batch 80, train loss:2.757930278778076, Elapsed time for epoch : 1.008847955862681
Epoch 0, Batch 90, train loss:2.4158496856689453, Elapsed time for epoch : 1.1342174132665
Epoch 0, Batch 100, train loss:2.073129177093506, Elapsed time for epoch : 1.258765172958374
Epoch 0, Batch 110, train loss:1.73664391040802, Elapsed time for epoch : 1.3831966439882915
Batch 0, val loss:4.206125736236572
Batch 10, val loss:4.668110370635986
Batch 20, val loss:5.256968975067139
Batch 30, val loss:7.410508155822754
Epoch 0, Train Loss:3.2239717172539755, Val loss:3.9734070102373757
Epoch 1, Batch 0, train loss:1.985734224319458, Elapsed time for epoch : 0.011618645985921223
Epoch 1, Batch 10, train loss:1.660772442817688, Elapsed time for epoch : 0.12794411579767864
Epoch 1, Batch 20, train loss:1.4561463594436646, Elapsed time for epoch : 0.2443938930829366
Epoch 1, Batch 30, train loss:1.446944236755371, Elapsed time for epoch : 0.3616207718849182
Epoch 1, Batch 40, train loss:1.4317090511322021, Elapsed time for epoch : 0.4777340372403463
Epoch 1, Batch 50, train loss:1.5135356187820435, Elapsed time for epoch : 0.5941497405370076
Epoch 1, Batch 60, train loss:1.3298909664154053, Elapsed time for epoch : 0.710454793771108
Epoch 1, Batch 70, train loss:1.4478814601898193, Elapsed time for epoch : 0.8265154639879863
Epoch 1, Batch 80, train loss:1.2132679224014282, Elapsed time for epoch : 0.9425040443738302
Epoch 1, Batch 90, train loss:1.1306602954864502, Elapsed time for epoch : 1.05906746784846
Epoch 1, Batch 100, train loss:0.7405967712402344, Elapsed time for epoch : 1.175178317228953
Epoch 1, Batch 110, train loss:0.8867823481559753, Elapsed time for epoch : 1.2913401246070861
Batch 0, val loss:2.0924103260040283
Batch 10, val loss:5.186868190765381
Batch 20, val loss:4.072741508483887
Batch 30, val loss:3.073139190673828
Epoch 1, Train Loss:1.3731824807498767, Val loss:4.381882177458869
Epoch 2, Batch 0, train loss:0.9406401515007019, Elapsed time for epoch : 0.011667815844217937
Epoch 2, Batch 10, train loss:0.8953272700309753, Elapsed time for epoch : 0.1276682178179423
Epoch 2, Batch 20, train loss:0.9607328772544861, Elapsed time for epoch : 0.24378204345703125
Epoch 2, Batch 30, train loss:0.9441419243812561, Elapsed time for epoch : 0.36009838183720905
Epoch 2, Batch 40, train loss:0.8460099101066589, Elapsed time for epoch : 0.47666208346684774
Epoch 2, Batch 50, train loss:0.8674074411392212, Elapsed time for epoch : 0.593366531531016
Epoch 2, Batch 60, train loss:0.8420037031173706, Elapsed time for epoch : 0.7096341013908386
Epoch 2, Batch 70, train loss:0.3096703588962555, Elapsed time for epoch : 0.8258734941482544
Epoch 2, Batch 80, train loss:0.7481856942176819, Elapsed time for epoch : 0.9425342917442322
Epoch 2, Batch 90, train loss:0.7771557569503784, Elapsed time for epoch : 1.0589101672172547
Epoch 2, Batch 100, train loss:0.6995777487754822, Elapsed time for epoch : 1.1750632286071778
Epoch 2, Batch 110, train loss:0.572585940361023, Elapsed time for epoch : 1.2915378610293071
Batch 0, val loss:5.782897472381592
Batch 10, val loss:4.468949317932129
Batch 20, val loss:1.2868751287460327
Batch 30, val loss:2.788513422012329
Epoch 2, Train Loss:0.745130959412326, Val loss:5.587613551980919
Epoch 3, Batch 0, train loss:0.6336698532104492, Elapsed time for epoch : 0.01165844996770223
Epoch 3, Batch 10, train loss:0.6685689687728882, Elapsed time for epoch : 0.1285546859105428
Epoch 3, Batch 20, train loss:0.6131027340888977, Elapsed time for epoch : 0.24497913122177123
Epoch 3, Batch 30, train loss:0.4923393428325653, Elapsed time for epoch : 0.3614096482594808
Epoch 3, Batch 40, train loss:0.5007937550544739, Elapsed time for epoch : 0.4779707948366801
Epoch 3, Batch 50, train loss:0.1706073135137558, Elapsed time for epoch : 0.5941364884376525
Epoch 3, Batch 60, train loss:0.6031185388565063, Elapsed time for epoch : 0.7104506095250448
Epoch 3, Batch 70, train loss:0.5493858456611633, Elapsed time for epoch : 0.8270301222801208
Epoch 3, Batch 80, train loss:0.6305850148200989, Elapsed time for epoch : 0.9434878269831339
Epoch 3, Batch 90, train loss:0.3783111870288849, Elapsed time for epoch : 1.0599010149637857
Epoch 3, Batch 100, train loss:0.09215305745601654, Elapsed time for epoch : 1.176386030515035
Epoch 3, Batch 110, train loss:0.44199952483177185, Elapsed time for epoch : 1.2931015094121296
Batch 0, val loss:1.5948055982589722
Batch 10, val loss:11.096246719360352
Batch 20, val loss:5.005831241607666
Batch 30, val loss:4.594207763671875
Epoch 3, Train Loss:0.4631816416978836, Val loss:6.808852861324946
Epoch 4, Batch 0, train loss:0.3670259118080139, Elapsed time for epoch : 0.011701730887095134
Epoch 4, Batch 10, train loss:0.10715524852275848, Elapsed time for epoch : 0.12845181624094645
Epoch 4, Batch 20, train loss:0.10330758988857269, Elapsed time for epoch : 0.2453083078066508
Epoch 4, Batch 30, train loss:0.3746670186519623, Elapsed time for epoch : 0.36195264259974164
Epoch 4, Batch 40, train loss:0.26850035786628723, Elapsed time for epoch : 0.4783257762591044
Epoch 4, Batch 50, train loss:0.3456670939922333, Elapsed time for epoch : 0.5948023875554402
Epoch 4, Batch 60, train loss:0.2816741466522217, Elapsed time for epoch : 0.7112083911895752
Epoch 4, Batch 70, train loss:0.21855983138084412, Elapsed time for epoch : 0.8275553544362386
Epoch 4, Batch 80, train loss:0.13051767647266388, Elapsed time for epoch : 0.9440385103225708
Epoch 4, Batch 90, train loss:0.2931506335735321, Elapsed time for epoch : 1.0601371248563132
Epoch 4, Batch 100, train loss:0.2943452298641205, Elapsed time for epoch : 1.176657783985138
Epoch 4, Batch 110, train loss:0.23676936328411102, Elapsed time for epoch : 1.293406617641449
Batch 0, val loss:8.60964584350586
Batch 10, val loss:2.2042508125305176
Batch 20, val loss:4.892833709716797
Batch 30, val loss:1.705439567565918
Epoch 4, Train Loss:0.27842941880226135, Val loss:5.2184028459919825
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27843
wandb:         Val Loss 5.2184
wandb:      train_batch 110
wandb: train_batch_loss 0.23677
wandb:        val_batch 30
wandb:   val_batch_loss 1.70544
wandb: 
wandb: üöÄ View run fast-morning-349 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/cnarx65j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_024459-cnarx65j/logs
Seed completed execution! 1 0.6_5
------------------------------------------------------------------
Running for seed 42 of experiment 0.6_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_025253-9e3lpc68
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-serenity-351
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9e3lpc68
Epoch 0, Batch 0, train loss:8.643214225769043, Elapsed time for epoch : 0.013362562656402588
Epoch 0, Batch 10, train loss:3.8921637535095215, Elapsed time for epoch : 0.12958988746007283
Epoch 0, Batch 20, train loss:3.770404100418091, Elapsed time for epoch : 0.2460096597671509
Epoch 0, Batch 30, train loss:3.1424560546875, Elapsed time for epoch : 0.3619092106819153
Epoch 0, Batch 40, train loss:3.313298463821411, Elapsed time for epoch : 0.4790973941485087
Epoch 0, Batch 50, train loss:3.0310306549072266, Elapsed time for epoch : 0.5956889708836873
Epoch 0, Batch 60, train loss:2.7981626987457275, Elapsed time for epoch : 0.7115782976150513
Epoch 0, Batch 70, train loss:2.776331663131714, Elapsed time for epoch : 0.8279561837514241
Epoch 0, Batch 80, train loss:2.757930278778076, Elapsed time for epoch : 0.9442913889884949
Epoch 0, Batch 90, train loss:2.4158496856689453, Elapsed time for epoch : 1.0607130646705627
Epoch 0, Batch 100, train loss:2.073129177093506, Elapsed time for epoch : 1.1767789681752523
Epoch 0, Batch 110, train loss:1.73664391040802, Elapsed time for epoch : 1.2933284521102906
Batch 0, val loss:4.206125736236572
Batch 10, val loss:4.668110370635986
Batch 20, val loss:5.256968975067139
Batch 30, val loss:7.410508155822754
Epoch 0, Train Loss:3.2239717172539755, Val loss:3.9734070102373757
Epoch 1, Batch 0, train loss:1.985734224319458, Elapsed time for epoch : 0.011642646789550782
Epoch 1, Batch 10, train loss:1.660772442817688, Elapsed time for epoch : 0.1280214269955953
Epoch 1, Batch 20, train loss:1.4561463594436646, Elapsed time for epoch : 0.24402558008829753
Epoch 1, Batch 30, train loss:1.446944236755371, Elapsed time for epoch : 0.36017245054244995
Epoch 1, Batch 40, train loss:1.4317090511322021, Elapsed time for epoch : 0.4764824350674947
Epoch 1, Batch 50, train loss:1.5135356187820435, Elapsed time for epoch : 0.592574683825175
Epoch 1, Batch 60, train loss:1.3298909664154053, Elapsed time for epoch : 0.7089873075485229
Epoch 1, Batch 70, train loss:1.4478814601898193, Elapsed time for epoch : 0.825389568010966
Epoch 1, Batch 80, train loss:1.2132679224014282, Elapsed time for epoch : 0.9416241486867268
Epoch 1, Batch 90, train loss:1.1306602954864502, Elapsed time for epoch : 1.0579730113347372
Epoch 1, Batch 100, train loss:0.7405967712402344, Elapsed time for epoch : 1.174329364299774
Epoch 1, Batch 110, train loss:0.8867823481559753, Elapsed time for epoch : 1.2909972111384074
Batch 0, val loss:2.0924103260040283
Batch 10, val loss:5.186868190765381
Batch 20, val loss:4.072741508483887
Batch 30, val loss:3.073139190673828
Epoch 1, Train Loss:1.3731824807498767, Val loss:4.381882177458869
Epoch 2, Batch 0, train loss:0.9406401515007019, Elapsed time for epoch : 0.011878812313079834
Epoch 2, Batch 10, train loss:0.8953272700309753, Elapsed time for epoch : 0.12822798490524293
Epoch 2, Batch 20, train loss:0.9607328772544861, Elapsed time for epoch : 0.24464722474416098
Epoch 2, Batch 30, train loss:0.9441419243812561, Elapsed time for epoch : 0.36136635939280193
Epoch 2, Batch 40, train loss:0.8460099101066589, Elapsed time for epoch : 0.4784022649129232
Epoch 2, Batch 50, train loss:0.8674074411392212, Elapsed time for epoch : 0.5951974550882976
Epoch 2, Batch 60, train loss:0.8420037031173706, Elapsed time for epoch : 0.7118660171826681
Epoch 2, Batch 70, train loss:0.3096703588962555, Elapsed time for epoch : 0.8281976302464803
Epoch 2, Batch 80, train loss:0.7481856942176819, Elapsed time for epoch : 0.9446165879567464
Epoch 2, Batch 90, train loss:0.7771557569503784, Elapsed time for epoch : 1.0609069148699442
Epoch 2, Batch 100, train loss:0.6995777487754822, Elapsed time for epoch : 1.1770588239034017
Epoch 2, Batch 110, train loss:0.572585940361023, Elapsed time for epoch : 1.293339983622233
Batch 0, val loss:5.782897472381592
Batch 10, val loss:4.468949317932129
Batch 20, val loss:1.2868751287460327
Batch 30, val loss:2.788513422012329
Epoch 2, Train Loss:0.745130959412326, Val loss:5.587613551980919
Epoch 3, Batch 0, train loss:0.6336698532104492, Elapsed time for epoch : 0.011673104763031007
Epoch 3, Batch 10, train loss:0.6685689687728882, Elapsed time for epoch : 0.1282763163248698
Epoch 3, Batch 20, train loss:0.6131027340888977, Elapsed time for epoch : 0.24454174041748047
Epoch 3, Batch 30, train loss:0.4923393428325653, Elapsed time for epoch : 0.36069980065027873
Epoch 3, Batch 40, train loss:0.5007937550544739, Elapsed time for epoch : 0.47782000303268435
Epoch 3, Batch 50, train loss:0.1706073135137558, Elapsed time for epoch : 0.5939209938049317
Epoch 3, Batch 60, train loss:0.6031185388565063, Elapsed time for epoch : 0.7106540242830912
Epoch 3, Batch 70, train loss:0.5493858456611633, Elapsed time for epoch : 0.8270170013109843
Epoch 3, Batch 80, train loss:0.6305850148200989, Elapsed time for epoch : 0.9432191093762715
Epoch 3, Batch 90, train loss:0.3783111870288849, Elapsed time for epoch : 1.059858997662862
Epoch 3, Batch 100, train loss:0.09215305745601654, Elapsed time for epoch : 1.1762492855389912
Epoch 3, Batch 110, train loss:0.44199952483177185, Elapsed time for epoch : 1.2927338163057962
Batch 0, val loss:1.5948055982589722
Batch 10, val loss:11.096246719360352
Batch 20, val loss:5.005831241607666
Batch 30, val loss:4.594207763671875
Epoch 3, Train Loss:0.4631816416978836, Val loss:6.808852861324946
Epoch 4, Batch 0, train loss:0.3670259118080139, Elapsed time for epoch : 0.011832217375437418
Epoch 4, Batch 10, train loss:0.10715524852275848, Elapsed time for epoch : 0.1286446213722229
Epoch 4, Batch 20, train loss:0.10330758988857269, Elapsed time for epoch : 0.2449236472447713
Epoch 4, Batch 30, train loss:0.3746670186519623, Elapsed time for epoch : 0.3614007910092672
Epoch 4, Batch 40, train loss:0.26850035786628723, Elapsed time for epoch : 0.4780929883321126
Epoch 4, Batch 50, train loss:0.3456670939922333, Elapsed time for epoch : 0.5948158423105876
Epoch 4, Batch 60, train loss:0.2816741466522217, Elapsed time for epoch : 0.7114380319913228
Epoch 4, Batch 70, train loss:0.21855983138084412, Elapsed time for epoch : 0.827843451499939
Epoch 4, Batch 80, train loss:0.13051767647266388, Elapsed time for epoch : 0.944466507434845
Epoch 4, Batch 90, train loss:0.2931506335735321, Elapsed time for epoch : 1.0607778231302898
Epoch 4, Batch 100, train loss:0.2943452298641205, Elapsed time for epoch : 1.177373735109965
Epoch 4, Batch 110, train loss:0.23676936328411102, Elapsed time for epoch : 1.293875793615977
Batch 0, val loss:8.60964584350586
Batch 10, val loss:2.2042508125305176
Batch 20, val loss:4.892833709716797
Batch 30, val loss:1.705439567565918
Epoch 4, Train Loss:0.27842941880226135, Val loss:5.2184028459919825
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27843
wandb:         Val Loss 5.2184
wandb:      train_batch 110
wandb: train_batch_loss 0.23677
wandb:        val_batch 30
wandb:   val_batch_loss 1.70544
wandb: 
wandb: üöÄ View run dandy-serenity-351 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9e3lpc68
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_025253-9e3lpc68/logs
Seed completed execution! 42 0.6_5
------------------------------------------------------------------
Running for seed 89 of experiment 0.6_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_030039-8r8i3ler
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-violet-353
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8r8i3ler
Epoch 0, Batch 0, train loss:8.643214225769043, Elapsed time for epoch : 0.01191041866938273
Epoch 0, Batch 10, train loss:3.8921637535095215, Elapsed time for epoch : 0.12758264938990274
Epoch 0, Batch 20, train loss:3.770404100418091, Elapsed time for epoch : 0.2435310403505961
Epoch 0, Batch 30, train loss:3.1424560546875, Elapsed time for epoch : 0.35971452792485553
Epoch 0, Batch 40, train loss:3.313298463821411, Elapsed time for epoch : 0.47608537673950196
Epoch 0, Batch 50, train loss:3.0310306549072266, Elapsed time for epoch : 0.5925503849983216
Epoch 0, Batch 60, train loss:2.7981626987457275, Elapsed time for epoch : 0.7104859431584676
Epoch 0, Batch 70, train loss:2.776331663131714, Elapsed time for epoch : 0.8272461096445719
Epoch 0, Batch 80, train loss:2.757930278778076, Elapsed time for epoch : 0.943374236424764
Epoch 0, Batch 90, train loss:2.4158496856689453, Elapsed time for epoch : 1.0595551013946534
Epoch 0, Batch 100, train loss:2.073129177093506, Elapsed time for epoch : 1.1757344365119935
Epoch 0, Batch 110, train loss:1.73664391040802, Elapsed time for epoch : 1.2916718284289042
Batch 0, val loss:4.206125736236572
Batch 10, val loss:4.668110370635986
Batch 20, val loss:5.256968975067139
Batch 30, val loss:7.410508155822754
Epoch 0, Train Loss:3.2239717172539755, Val loss:3.9734070102373757
Epoch 1, Batch 0, train loss:1.985734224319458, Elapsed time for epoch : 0.011619095007578533
Epoch 1, Batch 10, train loss:1.660772442817688, Elapsed time for epoch : 0.12793075640996296
Epoch 1, Batch 20, train loss:1.4561463594436646, Elapsed time for epoch : 0.24426691929499308
Epoch 1, Batch 30, train loss:1.446944236755371, Elapsed time for epoch : 0.36054679155349734
Epoch 1, Batch 40, train loss:1.4317090511322021, Elapsed time for epoch : 0.4772080818812052
Epoch 1, Batch 50, train loss:1.5135356187820435, Elapsed time for epoch : 0.5932956417401631
Epoch 1, Batch 60, train loss:1.3298909664154053, Elapsed time for epoch : 0.7093419273694356
Epoch 1, Batch 70, train loss:1.4478814601898193, Elapsed time for epoch : 0.8257509112358093
Epoch 1, Batch 80, train loss:1.2132679224014282, Elapsed time for epoch : 0.9421096920967102
Epoch 1, Batch 90, train loss:1.1306602954864502, Elapsed time for epoch : 1.0585774620374044
Epoch 1, Batch 100, train loss:0.7405967712402344, Elapsed time for epoch : 1.1749247352282206
Epoch 1, Batch 110, train loss:0.8867823481559753, Elapsed time for epoch : 1.2915526787439982
Batch 0, val loss:2.0924103260040283
Batch 10, val loss:5.186868190765381
Batch 20, val loss:4.072741508483887
Batch 30, val loss:3.073139190673828
Epoch 1, Train Loss:1.3731824807498767, Val loss:4.381882177458869
Epoch 2, Batch 0, train loss:0.9406401515007019, Elapsed time for epoch : 0.01166235605875651
Epoch 2, Batch 10, train loss:0.8953272700309753, Elapsed time for epoch : 0.1283943255742391
Epoch 2, Batch 20, train loss:0.9607328772544861, Elapsed time for epoch : 0.24475253820419313
Epoch 2, Batch 30, train loss:0.9441419243812561, Elapsed time for epoch : 0.36142739454905193
Epoch 2, Batch 40, train loss:0.8460099101066589, Elapsed time for epoch : 0.47839786609013873
Epoch 2, Batch 50, train loss:0.8674074411392212, Elapsed time for epoch : 0.5948232889175415
Epoch 2, Batch 60, train loss:0.8420037031173706, Elapsed time for epoch : 0.7115921537081401
Epoch 2, Batch 70, train loss:0.3096703588962555, Elapsed time for epoch : 0.8278855919837952
Epoch 2, Batch 80, train loss:0.7481856942176819, Elapsed time for epoch : 0.944355829556783
Epoch 2, Batch 90, train loss:0.7771557569503784, Elapsed time for epoch : 1.060909887154897
Epoch 2, Batch 100, train loss:0.6995777487754822, Elapsed time for epoch : 1.177565868695577
Epoch 2, Batch 110, train loss:0.572585940361023, Elapsed time for epoch : 1.2944753646850586
Batch 0, val loss:5.782897472381592
Batch 10, val loss:4.468949317932129
Batch 20, val loss:1.2868751287460327
Batch 30, val loss:2.788513422012329
Epoch 2, Train Loss:0.745130959412326, Val loss:5.587613551980919
Epoch 3, Batch 0, train loss:0.6336698532104492, Elapsed time for epoch : 0.011727134386698404
Epoch 3, Batch 10, train loss:0.6685689687728882, Elapsed time for epoch : 0.12807641824086508
Epoch 3, Batch 20, train loss:0.6131027340888977, Elapsed time for epoch : 0.24456132253011068
Epoch 3, Batch 30, train loss:0.4923393428325653, Elapsed time for epoch : 0.36068787177403766
Epoch 3, Batch 40, train loss:0.5007937550544739, Elapsed time for epoch : 0.4770625948905945
Epoch 3, Batch 50, train loss:0.1706073135137558, Elapsed time for epoch : 0.5930691878000895
Epoch 3, Batch 60, train loss:0.6031185388565063, Elapsed time for epoch : 0.7092691540718079
Epoch 3, Batch 70, train loss:0.5493858456611633, Elapsed time for epoch : 0.8258291403452556
Epoch 3, Batch 80, train loss:0.6305850148200989, Elapsed time for epoch : 0.9417521754900614
Epoch 3, Batch 90, train loss:0.3783111870288849, Elapsed time for epoch : 1.058253494898478
Epoch 3, Batch 100, train loss:0.09215305745601654, Elapsed time for epoch : 1.1741155147552491
Epoch 3, Batch 110, train loss:0.44199952483177185, Elapsed time for epoch : 1.2904216210047403
Batch 0, val loss:1.5948055982589722
Batch 10, val loss:11.096246719360352
Batch 20, val loss:5.005831241607666
Batch 30, val loss:4.594207763671875
Epoch 3, Train Loss:0.4631816416978836, Val loss:6.808852861324946
Epoch 4, Batch 0, train loss:0.3670259118080139, Elapsed time for epoch : 0.011632649103800456
Epoch 4, Batch 10, train loss:0.10715524852275848, Elapsed time for epoch : 0.12785884539286296
Epoch 4, Batch 20, train loss:0.10330758988857269, Elapsed time for epoch : 0.24417558908462525
Epoch 4, Batch 30, train loss:0.3746670186519623, Elapsed time for epoch : 0.36025049686431887
Epoch 4, Batch 40, train loss:0.26850035786628723, Elapsed time for epoch : 0.4765761653582255
Epoch 4, Batch 50, train loss:0.3456670939922333, Elapsed time for epoch : 0.5929553469022115
Epoch 4, Batch 60, train loss:0.2816741466522217, Elapsed time for epoch : 0.7091834624608357
Epoch 4, Batch 70, train loss:0.21855983138084412, Elapsed time for epoch : 0.8258396625518799
Epoch 4, Batch 80, train loss:0.13051767647266388, Elapsed time for epoch : 0.942456583182017
Epoch 4, Batch 90, train loss:0.2931506335735321, Elapsed time for epoch : 1.0584731300671895
Epoch 4, Batch 100, train loss:0.2943452298641205, Elapsed time for epoch : 1.1747417251269023
Epoch 4, Batch 110, train loss:0.23676936328411102, Elapsed time for epoch : 1.291320792833964
Batch 0, val loss:8.60964584350586
Batch 10, val loss:2.2042508125305176
Batch 20, val loss:4.892833709716797
Batch 30, val loss:1.705439567565918
Epoch 4, Train Loss:0.27842941880226135, Val loss:5.2184028459919825
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27843
wandb:         Val Loss 5.2184
wandb:      train_batch 110
wandb: train_batch_loss 0.23677
wandb:        val_batch 30
wandb:   val_batch_loss 1.70544
wandb: 
wandb: üöÄ View run charmed-violet-353 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8r8i3ler
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_030039-8r8i3ler/logs
Seed completed execution! 89 0.6_5
------------------------------------------------------------------
Running for seed 23 of experiment 0.6_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_030825-6hy9si8w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-salad-355
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6hy9si8w
Epoch 0, Batch 0, train loss:8.643214225769043, Elapsed time for epoch : 0.013303275903065999
Epoch 0, Batch 10, train loss:3.8921637535095215, Elapsed time for epoch : 0.12900843620300292
Epoch 0, Batch 20, train loss:3.770404100418091, Elapsed time for epoch : 0.2448996861775716
Epoch 0, Batch 30, train loss:3.1424560546875, Elapsed time for epoch : 0.3610165516535441
Epoch 0, Batch 40, train loss:3.313298463821411, Elapsed time for epoch : 0.4768102288246155
Epoch 0, Batch 50, train loss:3.0310306549072266, Elapsed time for epoch : 0.5928359111150105
Epoch 0, Batch 60, train loss:2.7981626987457275, Elapsed time for epoch : 0.7093045989672343
Epoch 0, Batch 70, train loss:2.776331663131714, Elapsed time for epoch : 0.8259069561958313
Epoch 0, Batch 80, train loss:2.757930278778076, Elapsed time for epoch : 0.9422782699267069
Epoch 0, Batch 90, train loss:2.4158496856689453, Elapsed time for epoch : 1.0589411377906799
Epoch 0, Batch 100, train loss:2.073129177093506, Elapsed time for epoch : 1.1755996823310852
Epoch 0, Batch 110, train loss:1.73664391040802, Elapsed time for epoch : 1.2916216770807902
Batch 0, val loss:4.206125736236572
Batch 10, val loss:4.668110370635986
Batch 20, val loss:5.256968975067139
Batch 30, val loss:7.410508155822754
Epoch 0, Train Loss:3.2239717172539755, Val loss:3.9734070102373757
Epoch 1, Batch 0, train loss:1.985734224319458, Elapsed time for epoch : 0.011696251233418782
Epoch 1, Batch 10, train loss:1.660772442817688, Elapsed time for epoch : 0.12871952851613364
Epoch 1, Batch 20, train loss:1.4561463594436646, Elapsed time for epoch : 0.2448046366373698
Epoch 1, Batch 30, train loss:1.446944236755371, Elapsed time for epoch : 0.3615236560503642
Epoch 1, Batch 40, train loss:1.4317090511322021, Elapsed time for epoch : 0.4789364457130432
Epoch 1, Batch 50, train loss:1.5135356187820435, Elapsed time for epoch : 0.5950119256973266
Epoch 1, Batch 60, train loss:1.3298909664154053, Elapsed time for epoch : 0.7115665276845297
Epoch 1, Batch 70, train loss:1.4478814601898193, Elapsed time for epoch : 0.8283236900965373
Epoch 1, Batch 80, train loss:1.2132679224014282, Elapsed time for epoch : 0.9447041193644206
Epoch 1, Batch 90, train loss:1.1306602954864502, Elapsed time for epoch : 1.0615947365760803
Epoch 1, Batch 100, train loss:0.7405967712402344, Elapsed time for epoch : 1.1776886264483133
Epoch 1, Batch 110, train loss:0.8867823481559753, Elapsed time for epoch : 1.2943061272303262
Batch 0, val loss:2.0924103260040283
Batch 10, val loss:5.186868190765381
Batch 20, val loss:4.072741508483887
Batch 30, val loss:3.073139190673828
Epoch 1, Train Loss:1.3731824807498767, Val loss:4.381882177458869
Epoch 2, Batch 0, train loss:0.9406401515007019, Elapsed time for epoch : 0.011700137456258138
Epoch 2, Batch 10, train loss:0.8953272700309753, Elapsed time for epoch : 0.12811076243718464
Epoch 2, Batch 20, train loss:0.9607328772544861, Elapsed time for epoch : 0.2444578727086385
Epoch 2, Batch 30, train loss:0.9441419243812561, Elapsed time for epoch : 0.36094417969385784
Epoch 2, Batch 40, train loss:0.8460099101066589, Elapsed time for epoch : 0.47804262638092043
Epoch 2, Batch 50, train loss:0.8674074411392212, Elapsed time for epoch : 0.5949849883715311
Epoch 2, Batch 60, train loss:0.8420037031173706, Elapsed time for epoch : 0.7114879926045735
Epoch 2, Batch 70, train loss:0.3096703588962555, Elapsed time for epoch : 0.8283295512199402
Epoch 2, Batch 80, train loss:0.7481856942176819, Elapsed time for epoch : 0.9447298844655355
Epoch 2, Batch 90, train loss:0.7771557569503784, Elapsed time for epoch : 1.0612294912338256
Epoch 2, Batch 100, train loss:0.6995777487754822, Elapsed time for epoch : 1.177750587463379
Epoch 2, Batch 110, train loss:0.572585940361023, Elapsed time for epoch : 1.294161359469096
Batch 0, val loss:5.782897472381592
Batch 10, val loss:4.468949317932129
Batch 20, val loss:1.2868751287460327
Batch 30, val loss:2.788513422012329
Epoch 2, Train Loss:0.745130959412326, Val loss:5.587613551980919
Epoch 3, Batch 0, train loss:0.6336698532104492, Elapsed time for epoch : 0.01166614294052124
Epoch 3, Batch 10, train loss:0.6685689687728882, Elapsed time for epoch : 0.1281030813852946
Epoch 3, Batch 20, train loss:0.6131027340888977, Elapsed time for epoch : 0.24446450074513754
Epoch 3, Batch 30, train loss:0.4923393428325653, Elapsed time for epoch : 0.3609681725502014
Epoch 3, Batch 40, train loss:0.5007937550544739, Elapsed time for epoch : 0.47783902088801067
Epoch 3, Batch 50, train loss:0.1706073135137558, Elapsed time for epoch : 0.5943181276321411
Epoch 3, Batch 60, train loss:0.6031185388565063, Elapsed time for epoch : 0.7109445532162985
Epoch 3, Batch 70, train loss:0.5493858456611633, Elapsed time for epoch : 0.8277105530103047
Epoch 3, Batch 80, train loss:0.6305850148200989, Elapsed time for epoch : 0.9443366010983785
Epoch 3, Batch 90, train loss:0.3783111870288849, Elapsed time for epoch : 1.0606050888697307
Epoch 3, Batch 100, train loss:0.09215305745601654, Elapsed time for epoch : 1.1770199259122214
Epoch 3, Batch 110, train loss:0.44199952483177185, Elapsed time for epoch : 1.2933526555697124
Batch 0, val loss:1.5948055982589722
Batch 10, val loss:11.096246719360352
Batch 20, val loss:5.005831241607666
Batch 30, val loss:4.594207763671875
Epoch 3, Train Loss:0.4631816416978836, Val loss:6.808852861324946
Epoch 4, Batch 0, train loss:0.3670259118080139, Elapsed time for epoch : 0.011672508716583253
Epoch 4, Batch 10, train loss:0.10715524852275848, Elapsed time for epoch : 0.128496519724528
Epoch 4, Batch 20, train loss:0.10330758988857269, Elapsed time for epoch : 0.24492321809132894
Epoch 4, Batch 30, train loss:0.3746670186519623, Elapsed time for epoch : 0.36145195960998533
Epoch 4, Batch 40, train loss:0.26850035786628723, Elapsed time for epoch : 0.4783000667889913
Epoch 4, Batch 50, train loss:0.3456670939922333, Elapsed time for epoch : 0.5948920965194702
Epoch 4, Batch 60, train loss:0.2816741466522217, Elapsed time for epoch : 0.7112163265546163
Epoch 4, Batch 70, train loss:0.21855983138084412, Elapsed time for epoch : 0.8276980320612589
Epoch 4, Batch 80, train loss:0.13051767647266388, Elapsed time for epoch : 0.9439052939414978
Epoch 4, Batch 90, train loss:0.2931506335735321, Elapsed time for epoch : 1.0599869012832641
Epoch 4, Batch 100, train loss:0.2943452298641205, Elapsed time for epoch : 1.1762385010719298
Epoch 4, Batch 110, train loss:0.23676936328411102, Elapsed time for epoch : 1.2925587932268778
Batch 0, val loss:8.60964584350586
Batch 10, val loss:2.2042508125305176
Batch 20, val loss:4.892833709716797
Batch 30, val loss:1.705439567565918
Epoch 4, Train Loss:0.27842941880226135, Val loss:5.2184028459919825
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27843
wandb:         Val Loss 5.2184
wandb:      train_batch 110
wandb: train_batch_loss 0.23677
wandb:        val_batch 30
wandb:   val_batch_loss 1.70544
wandb: 
wandb: üöÄ View run eternal-salad-355 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/6hy9si8w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_030825-6hy9si8w/logs
Seed completed execution! 23 0.6_5
------------------------------------------------------------------
Running for seed 113 of experiment 0.6_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_031614-2rh40171
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-jazz-357
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/2rh40171
Epoch 0, Batch 0, train loss:8.643214225769043, Elapsed time for epoch : 0.012225743134816487
Epoch 0, Batch 10, train loss:3.8921637535095215, Elapsed time for epoch : 0.12966291109720865
Epoch 0, Batch 20, train loss:3.770404100418091, Elapsed time for epoch : 0.2466515342394511
Epoch 0, Batch 30, train loss:3.1424560546875, Elapsed time for epoch : 0.36492626667022704
Epoch 0, Batch 40, train loss:3.313298463821411, Elapsed time for epoch : 0.4827039082845052
Epoch 0, Batch 50, train loss:3.0310306549072266, Elapsed time for epoch : 0.6005586226781209
Epoch 0, Batch 60, train loss:2.7981626987457275, Elapsed time for epoch : 0.718452513217926
Epoch 0, Batch 70, train loss:2.776331663131714, Elapsed time for epoch : 0.8361393411954244
Epoch 0, Batch 80, train loss:2.757930278778076, Elapsed time for epoch : 0.9544345577557881
Epoch 0, Batch 90, train loss:2.4158496856689453, Elapsed time for epoch : 1.0711880763371786
Epoch 0, Batch 100, train loss:2.073129177093506, Elapsed time for epoch : 1.1880430539449056
Epoch 0, Batch 110, train loss:1.73664391040802, Elapsed time for epoch : 1.3048304557800292
Batch 0, val loss:4.206125736236572
Batch 10, val loss:4.668110370635986
Batch 20, val loss:5.256968975067139
Batch 30, val loss:7.410508155822754
Epoch 0, Train Loss:3.2239717172539755, Val loss:3.9734070102373757
Epoch 1, Batch 0, train loss:1.985734224319458, Elapsed time for epoch : 0.011692261695861817
Epoch 1, Batch 10, train loss:1.660772442817688, Elapsed time for epoch : 0.12803896268208823
Epoch 1, Batch 20, train loss:1.4561463594436646, Elapsed time for epoch : 0.2446386496225993
Epoch 1, Batch 30, train loss:1.446944236755371, Elapsed time for epoch : 0.36126342217127483
Epoch 1, Batch 40, train loss:1.4317090511322021, Elapsed time for epoch : 0.4780294338862101
Epoch 1, Batch 50, train loss:1.5135356187820435, Elapsed time for epoch : 0.5943335215250651
Epoch 1, Batch 60, train loss:1.3298909664154053, Elapsed time for epoch : 0.7109940568606059
Epoch 1, Batch 70, train loss:1.4478814601898193, Elapsed time for epoch : 0.8276150822639465
Epoch 1, Batch 80, train loss:1.2132679224014282, Elapsed time for epoch : 0.9436294198036194
Epoch 1, Batch 90, train loss:1.1306602954864502, Elapsed time for epoch : 1.0598092039426168
Epoch 1, Batch 100, train loss:0.7405967712402344, Elapsed time for epoch : 1.176317278544108
Epoch 1, Batch 110, train loss:0.8867823481559753, Elapsed time for epoch : 1.2928865710894266
Batch 0, val loss:2.0924103260040283
Batch 10, val loss:5.186868190765381
Batch 20, val loss:4.072741508483887
Batch 30, val loss:3.073139190673828
Epoch 1, Train Loss:1.3731824807498767, Val loss:4.381882177458869
Epoch 2, Batch 0, train loss:0.9406401515007019, Elapsed time for epoch : 0.011685617764790853
Epoch 2, Batch 10, train loss:0.8953272700309753, Elapsed time for epoch : 0.12814286549886067
Epoch 2, Batch 20, train loss:0.9607328772544861, Elapsed time for epoch : 0.24464888175328572
Epoch 2, Batch 30, train loss:0.9441419243812561, Elapsed time for epoch : 0.3610906998316447
Epoch 2, Batch 40, train loss:0.8460099101066589, Elapsed time for epoch : 0.47724492152531944
Epoch 2, Batch 50, train loss:0.8674074411392212, Elapsed time for epoch : 0.5937034924825032
Epoch 2, Batch 60, train loss:0.8420037031173706, Elapsed time for epoch : 0.7102875113487244
Epoch 2, Batch 70, train loss:0.3096703588962555, Elapsed time for epoch : 0.826912506421407
Epoch 2, Batch 80, train loss:0.7481856942176819, Elapsed time for epoch : 0.9431238174438477
Epoch 2, Batch 90, train loss:0.7771557569503784, Elapsed time for epoch : 1.0597062627474467
Epoch 2, Batch 100, train loss:0.6995777487754822, Elapsed time for epoch : 1.1760799090067546
Epoch 2, Batch 110, train loss:0.572585940361023, Elapsed time for epoch : 1.292715072631836
Batch 0, val loss:5.782897472381592
Batch 10, val loss:4.468949317932129
Batch 20, val loss:1.2868751287460327
Batch 30, val loss:2.788513422012329
Epoch 2, Train Loss:0.745130959412326, Val loss:5.587613551980919
Epoch 3, Batch 0, train loss:0.6336698532104492, Elapsed time for epoch : 0.011677376429239909
Epoch 3, Batch 10, train loss:0.6685689687728882, Elapsed time for epoch : 0.12807465791702272
Epoch 3, Batch 20, train loss:0.6131027340888977, Elapsed time for epoch : 0.24469566742579144
Epoch 3, Batch 30, train loss:0.4923393428325653, Elapsed time for epoch : 0.3608995795249939
Epoch 3, Batch 40, train loss:0.5007937550544739, Elapsed time for epoch : 0.47715616623560586
Epoch 3, Batch 50, train loss:0.1706073135137558, Elapsed time for epoch : 0.5938353061676025
Epoch 3, Batch 60, train loss:0.6031185388565063, Elapsed time for epoch : 0.7105064272880555
Epoch 3, Batch 70, train loss:0.5493858456611633, Elapsed time for epoch : 0.826924729347229
Epoch 3, Batch 80, train loss:0.6305850148200989, Elapsed time for epoch : 0.9435684680938721
Epoch 3, Batch 90, train loss:0.3783111870288849, Elapsed time for epoch : 1.060380760828654
Epoch 3, Batch 100, train loss:0.09215305745601654, Elapsed time for epoch : 1.1767087737719217
Epoch 3, Batch 110, train loss:0.44199952483177185, Elapsed time for epoch : 1.293489936987559
Batch 0, val loss:1.5948055982589722
Batch 10, val loss:11.096246719360352
Batch 20, val loss:5.005831241607666
Batch 30, val loss:4.594207763671875
Epoch 3, Train Loss:0.4631816416978836, Val loss:6.808852861324946
Epoch 4, Batch 0, train loss:0.3670259118080139, Elapsed time for epoch : 0.01168589194615682
Epoch 4, Batch 10, train loss:0.10715524852275848, Elapsed time for epoch : 0.12816968361536663
Epoch 4, Batch 20, train loss:0.10330758988857269, Elapsed time for epoch : 0.24457964499791462
Epoch 4, Batch 30, train loss:0.3746670186519623, Elapsed time for epoch : 0.3610117832819621
Epoch 4, Batch 40, train loss:0.26850035786628723, Elapsed time for epoch : 0.47709200382232664
Epoch 4, Batch 50, train loss:0.3456670939922333, Elapsed time for epoch : 0.5935611486434936
Epoch 4, Batch 60, train loss:0.2816741466522217, Elapsed time for epoch : 0.7104993263880411
Epoch 4, Batch 70, train loss:0.21855983138084412, Elapsed time for epoch : 0.826983920733134
Epoch 4, Batch 80, train loss:0.13051767647266388, Elapsed time for epoch : 0.9438714981079102
Epoch 4, Batch 90, train loss:0.2931506335735321, Elapsed time for epoch : 1.0604045788447063
Epoch 4, Batch 100, train loss:0.2943452298641205, Elapsed time for epoch : 1.1765456477801004
Epoch 4, Batch 110, train loss:0.23676936328411102, Elapsed time for epoch : 1.2932374676068623
Batch 0, val loss:8.60964584350586
Batch 10, val loss:2.2042508125305176
Batch 20, val loss:4.892833709716797
Batch 30, val loss:1.705439567565918
Epoch 4, Train Loss:0.27842941880226135, Val loss:5.2184028459919825
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.27843
wandb:         Val Loss 5.2184
wandb:      train_batch 110
wandb: train_batch_loss 0.23677
wandb:        val_batch 30
wandb:   val_batch_loss 1.70544
wandb: 
wandb: üöÄ View run wild-jazz-357 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/2rh40171
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_031614-2rh40171/logs
Seed completed execution! 113 0.6_5
------------------------------------------------------------------
Experiment complete 0.6_5
==========================================================================
Running experiment for setting 0.7_1
==========================================================================
Running for seed 1 of experiment 0.7_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_032402-acpwdun2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-shape-359
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/acpwdun2
Epoch 0, Batch 0, train loss:7.862167835235596, Elapsed time for epoch : 0.014706305662790934
Epoch 0, Batch 10, train loss:3.255730152130127, Elapsed time for epoch : 0.13851468563079833
Epoch 0, Batch 20, train loss:2.651141405105591, Elapsed time for epoch : 0.2622058868408203
Epoch 0, Batch 30, train loss:2.353551149368286, Elapsed time for epoch : 0.38546276489893594
Epoch 0, Batch 40, train loss:2.1881794929504395, Elapsed time for epoch : 0.5103041251500448
Epoch 0, Batch 50, train loss:1.9866702556610107, Elapsed time for epoch : 0.6332411289215087
Epoch 0, Batch 60, train loss:1.9703415632247925, Elapsed time for epoch : 0.758019483089447
Epoch 0, Batch 70, train loss:1.9410392045974731, Elapsed time for epoch : 0.8835385203361511
Epoch 0, Batch 80, train loss:1.9307727813720703, Elapsed time for epoch : 1.0072926998138427
Epoch 0, Batch 90, train loss:1.6565821170806885, Elapsed time for epoch : 1.132845139503479
Epoch 0, Batch 100, train loss:1.6503229141235352, Elapsed time for epoch : 1.2576158881187438
Epoch 0, Batch 110, train loss:1.5085197687149048, Elapsed time for epoch : 1.3827466289202373
Batch 0, val loss:7.020512104034424
Batch 10, val loss:4.581690311431885
Batch 20, val loss:3.577476739883423
Batch 30, val loss:16.800762176513672
Epoch 0, Train Loss:3.3330196100732556, Val loss:5.127083036634657
Epoch 1, Batch 0, train loss:1.6317623853683472, Elapsed time for epoch : 0.011603744824727376
Epoch 1, Batch 10, train loss:1.4623563289642334, Elapsed time for epoch : 0.12730741103490192
Epoch 1, Batch 20, train loss:1.5255845785140991, Elapsed time for epoch : 0.24322580099105834
Epoch 1, Batch 30, train loss:1.4484397172927856, Elapsed time for epoch : 0.3596489111582438
Epoch 1, Batch 40, train loss:1.4189138412475586, Elapsed time for epoch : 0.4758261481920878
Epoch 1, Batch 50, train loss:1.3809001445770264, Elapsed time for epoch : 0.5916586081186931
Epoch 1, Batch 60, train loss:1.3070915937423706, Elapsed time for epoch : 0.7073970635732015
Epoch 1, Batch 70, train loss:1.390528917312622, Elapsed time for epoch : 0.8236929853757222
Epoch 1, Batch 80, train loss:1.4257593154907227, Elapsed time for epoch : 0.9396359125773112
Epoch 1, Batch 90, train loss:1.287817358970642, Elapsed time for epoch : 1.0558134635289511
Epoch 1, Batch 100, train loss:1.1563799381256104, Elapsed time for epoch : 1.171469509601593
Epoch 1, Batch 110, train loss:1.2431243658065796, Elapsed time for epoch : 1.2874505718549092
Batch 0, val loss:2.039937973022461
Batch 10, val loss:9.388875007629395
Batch 20, val loss:2.1418869495391846
Batch 30, val loss:2.2809221744537354
Epoch 1, Train Loss:1.4126696959785794, Val loss:6.556566493378745
Epoch 2, Batch 0, train loss:1.245603322982788, Elapsed time for epoch : 0.011628381411234538
Epoch 2, Batch 10, train loss:1.262674331665039, Elapsed time for epoch : 0.12781480153401692
Epoch 2, Batch 20, train loss:1.1894943714141846, Elapsed time for epoch : 0.2442702531814575
Epoch 2, Batch 30, train loss:1.152649998664856, Elapsed time for epoch : 0.3605164130528768
Epoch 2, Batch 40, train loss:1.1326062679290771, Elapsed time for epoch : 0.476507838567098
Epoch 2, Batch 50, train loss:1.170149803161621, Elapsed time for epoch : 0.592982025941213
Epoch 2, Batch 60, train loss:1.1085522174835205, Elapsed time for epoch : 0.7094151178995768
Epoch 2, Batch 70, train loss:1.0400867462158203, Elapsed time for epoch : 0.8259716351826986
Epoch 2, Batch 80, train loss:1.1706706285476685, Elapsed time for epoch : 0.9422323147455851
Epoch 2, Batch 90, train loss:1.1201276779174805, Elapsed time for epoch : 1.0579828818639119
Epoch 2, Batch 100, train loss:1.1251094341278076, Elapsed time for epoch : 1.1741326014200846
Epoch 2, Batch 110, train loss:1.0961710214614868, Elapsed time for epoch : 1.2902547200520833
Batch 0, val loss:9.513059616088867
Batch 10, val loss:2.0773961544036865
Batch 20, val loss:5.383362293243408
Batch 30, val loss:7.7884440422058105
Epoch 2, Train Loss:1.132350142105766, Val loss:9.055281397369173
Epoch 3, Batch 0, train loss:1.09443199634552, Elapsed time for epoch : 0.011624467372894288
Epoch 3, Batch 10, train loss:1.049615740776062, Elapsed time for epoch : 0.12783198753992717
Epoch 3, Batch 20, train loss:1.0715888738632202, Elapsed time for epoch : 0.24382231632868448
Epoch 3, Batch 30, train loss:1.0999393463134766, Elapsed time for epoch : 0.3597528258959452
Epoch 3, Batch 40, train loss:1.1221307516098022, Elapsed time for epoch : 0.4758564511934916
Epoch 3, Batch 50, train loss:0.9366193413734436, Elapsed time for epoch : 0.5918747266133626
Epoch 3, Batch 60, train loss:1.050418496131897, Elapsed time for epoch : 0.7079699238141378
Epoch 3, Batch 70, train loss:1.0660995244979858, Elapsed time for epoch : 0.8241449554761251
Epoch 3, Batch 80, train loss:1.0528372526168823, Elapsed time for epoch : 0.9402724862098694
Epoch 3, Batch 90, train loss:1.0260642766952515, Elapsed time for epoch : 1.0565046588579814
Epoch 3, Batch 100, train loss:0.7672774195671082, Elapsed time for epoch : 1.172273329893748
Epoch 3, Batch 110, train loss:0.95247882604599, Elapsed time for epoch : 1.2882904132207236
Batch 0, val loss:4.790701389312744
Batch 10, val loss:16.238605499267578
Batch 20, val loss:15.224665641784668
Batch 30, val loss:3.409703493118286
Epoch 3, Train Loss:1.0092262345811596, Val loss:8.355670279926724
Epoch 4, Batch 0, train loss:0.9314046502113342, Elapsed time for epoch : 0.011699104309082031
Epoch 4, Batch 10, train loss:0.876451313495636, Elapsed time for epoch : 0.12774272759755453
Epoch 4, Batch 20, train loss:0.9200505614280701, Elapsed time for epoch : 0.24359151124954223
Epoch 4, Batch 30, train loss:0.8954121470451355, Elapsed time for epoch : 0.3593292236328125
Epoch 4, Batch 40, train loss:0.9507360458374023, Elapsed time for epoch : 0.475293242931366
Epoch 4, Batch 50, train loss:1.038448452949524, Elapsed time for epoch : 0.5910732229550679
Epoch 4, Batch 60, train loss:0.8899649381637573, Elapsed time for epoch : 0.7067645112673442
Epoch 4, Batch 70, train loss:1.034987449645996, Elapsed time for epoch : 0.822698708375295
Epoch 4, Batch 80, train loss:0.9090471267700195, Elapsed time for epoch : 0.9386273543039958
Epoch 4, Batch 90, train loss:0.9144729971885681, Elapsed time for epoch : 1.0542418241500855
Epoch 4, Batch 100, train loss:0.9577097296714783, Elapsed time for epoch : 1.1698928236961366
Epoch 4, Batch 110, train loss:1.0568903684616089, Elapsed time for epoch : 1.2860961397488913
Batch 0, val loss:1.772247314453125
Batch 10, val loss:2.011570692062378
Batch 20, val loss:3.2935595512390137
Batch 30, val loss:3.4775474071502686
Epoch 4, Train Loss:0.9362496391586635, Val loss:5.378852246536149
wandb: - 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93625
wandb:         Val Loss 5.37885
wandb:      train_batch 110
wandb: train_batch_loss 1.05689
wandb:        val_batch 30
wandb:   val_batch_loss 3.47755
wandb: 
wandb: üöÄ View run glamorous-shape-359 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/acpwdun2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_032402-acpwdun2/logs
Seed completed execution! 1 0.7_1
------------------------------------------------------------------
Running for seed 42 of experiment 0.7_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_033154-xaknj3gx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-star-361
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xaknj3gx
Epoch 0, Batch 0, train loss:7.862167835235596, Elapsed time for epoch : 0.013157395521799724
Epoch 0, Batch 10, train loss:3.255730152130127, Elapsed time for epoch : 0.1284913937250773
Epoch 0, Batch 20, train loss:2.651141405105591, Elapsed time for epoch : 0.24405355453491212
Epoch 0, Batch 30, train loss:2.353551149368286, Elapsed time for epoch : 0.3594967365264893
Epoch 0, Batch 40, train loss:2.1881794929504395, Elapsed time for epoch : 0.4749578277269999
Epoch 0, Batch 50, train loss:1.9866702556610107, Elapsed time for epoch : 0.5908277789751689
Epoch 0, Batch 60, train loss:1.9703415632247925, Elapsed time for epoch : 0.7069905837376912
Epoch 0, Batch 70, train loss:1.9410392045974731, Elapsed time for epoch : 0.8227049469947815
Epoch 0, Batch 80, train loss:1.9307727813720703, Elapsed time for epoch : 0.938762636979421
Epoch 0, Batch 90, train loss:1.6565821170806885, Elapsed time for epoch : 1.0539817372957865
Epoch 0, Batch 100, train loss:1.6503229141235352, Elapsed time for epoch : 1.1697591304779054
Epoch 0, Batch 110, train loss:1.5085197687149048, Elapsed time for epoch : 1.285620661576589
Batch 0, val loss:7.020512104034424
Batch 10, val loss:4.581690311431885
Batch 20, val loss:3.577476739883423
Batch 30, val loss:16.800762176513672
Epoch 0, Train Loss:3.3330196100732556, Val loss:5.127083036634657
Epoch 1, Batch 0, train loss:1.6317623853683472, Elapsed time for epoch : 0.011701122919718424
Epoch 1, Batch 10, train loss:1.4623563289642334, Elapsed time for epoch : 0.12712963422139487
Epoch 1, Batch 20, train loss:1.5255845785140991, Elapsed time for epoch : 0.2433333118756612
Epoch 1, Batch 30, train loss:1.4484397172927856, Elapsed time for epoch : 0.3592661460240682
Epoch 1, Batch 40, train loss:1.4189138412475586, Elapsed time for epoch : 0.47530747652053834
Epoch 1, Batch 50, train loss:1.3809001445770264, Elapsed time for epoch : 0.5910968661308289
Epoch 1, Batch 60, train loss:1.3070915937423706, Elapsed time for epoch : 0.7071972807248433
Epoch 1, Batch 70, train loss:1.390528917312622, Elapsed time for epoch : 0.8238748391469319
Epoch 1, Batch 80, train loss:1.4257593154907227, Elapsed time for epoch : 0.9402203798294068
Epoch 1, Batch 90, train loss:1.287817358970642, Elapsed time for epoch : 1.0567227999369304
Epoch 1, Batch 100, train loss:1.1563799381256104, Elapsed time for epoch : 1.172657036781311
Epoch 1, Batch 110, train loss:1.2431243658065796, Elapsed time for epoch : 1.288843321800232
Batch 0, val loss:2.039937973022461
Batch 10, val loss:9.388875007629395
Batch 20, val loss:2.1418869495391846
Batch 30, val loss:2.2809221744537354
Epoch 1, Train Loss:1.4126696959785794, Val loss:6.556566493378745
Epoch 2, Batch 0, train loss:1.245603322982788, Elapsed time for epoch : 0.011633833249409994
Epoch 2, Batch 10, train loss:1.262674331665039, Elapsed time for epoch : 0.12733465830485027
Epoch 2, Batch 20, train loss:1.1894943714141846, Elapsed time for epoch : 0.2435421586036682
Epoch 2, Batch 30, train loss:1.152649998664856, Elapsed time for epoch : 0.35951363245646156
Epoch 2, Batch 40, train loss:1.1326062679290771, Elapsed time for epoch : 0.4760875701904297
Epoch 2, Batch 50, train loss:1.170149803161621, Elapsed time for epoch : 0.5920178492863973
Epoch 2, Batch 60, train loss:1.1085522174835205, Elapsed time for epoch : 0.708164099852244
Epoch 2, Batch 70, train loss:1.0400867462158203, Elapsed time for epoch : 0.8243098537127177
Epoch 2, Batch 80, train loss:1.1706706285476685, Elapsed time for epoch : 0.9401652216911316
Epoch 2, Batch 90, train loss:1.1201276779174805, Elapsed time for epoch : 1.0563308437665304
Epoch 2, Batch 100, train loss:1.1251094341278076, Elapsed time for epoch : 1.17244238058726
Epoch 2, Batch 110, train loss:1.0961710214614868, Elapsed time for epoch : 1.2889290690422057
Batch 0, val loss:9.513059616088867
Batch 10, val loss:2.0773961544036865
Batch 20, val loss:5.383362293243408
Batch 30, val loss:7.7884440422058105
Epoch 2, Train Loss:1.132350142105766, Val loss:9.055281397369173
Epoch 3, Batch 0, train loss:1.09443199634552, Elapsed time for epoch : 0.011651547749837239
Epoch 3, Batch 10, train loss:1.049615740776062, Elapsed time for epoch : 0.1276641845703125
Epoch 3, Batch 20, train loss:1.0715888738632202, Elapsed time for epoch : 0.24387516975402831
Epoch 3, Batch 30, train loss:1.0999393463134766, Elapsed time for epoch : 0.3605131864547729
Epoch 3, Batch 40, train loss:1.1221307516098022, Elapsed time for epoch : 0.4763502995173136
Epoch 3, Batch 50, train loss:0.9366193413734436, Elapsed time for epoch : 0.5927283287048339
Epoch 3, Batch 60, train loss:1.050418496131897, Elapsed time for epoch : 0.7094920595486959
Epoch 3, Batch 70, train loss:1.0660995244979858, Elapsed time for epoch : 0.8253875176111857
Epoch 3, Batch 80, train loss:1.0528372526168823, Elapsed time for epoch : 0.9411415417989095
Epoch 3, Batch 90, train loss:1.0260642766952515, Elapsed time for epoch : 1.0571494221687316
Epoch 3, Batch 100, train loss:0.7672774195671082, Elapsed time for epoch : 1.1734076738357544
Epoch 3, Batch 110, train loss:0.95247882604599, Elapsed time for epoch : 1.290183929602305
Batch 0, val loss:4.790701389312744
Batch 10, val loss:16.238605499267578
Batch 20, val loss:15.224665641784668
Batch 30, val loss:3.409703493118286
Epoch 3, Train Loss:1.0092262345811596, Val loss:8.355670279926724
Epoch 4, Batch 0, train loss:0.9314046502113342, Elapsed time for epoch : 0.011659685770670574
Epoch 4, Batch 10, train loss:0.876451313495636, Elapsed time for epoch : 0.1281557599703471
Epoch 4, Batch 20, train loss:0.9200505614280701, Elapsed time for epoch : 0.2441249410311381
Epoch 4, Batch 30, train loss:0.8954121470451355, Elapsed time for epoch : 0.3602039933204651
Epoch 4, Batch 40, train loss:0.9507360458374023, Elapsed time for epoch : 0.4762836972872416
Epoch 4, Batch 50, train loss:1.038448452949524, Elapsed time for epoch : 0.592249886194865
Epoch 4, Batch 60, train loss:0.8899649381637573, Elapsed time for epoch : 0.7087599714597066
Epoch 4, Batch 70, train loss:1.034987449645996, Elapsed time for epoch : 0.8249905983606974
Epoch 4, Batch 80, train loss:0.9090471267700195, Elapsed time for epoch : 0.9413151264190673
Epoch 4, Batch 90, train loss:0.9144729971885681, Elapsed time for epoch : 1.0574562629063924
Epoch 4, Batch 100, train loss:0.9577097296714783, Elapsed time for epoch : 1.1734745542208354
Epoch 4, Batch 110, train loss:1.0568903684616089, Elapsed time for epoch : 1.2898091554641724
Batch 0, val loss:1.772247314453125
Batch 10, val loss:2.011570692062378
Batch 20, val loss:3.2935595512390137
Batch 30, val loss:3.4775474071502686
Epoch 4, Train Loss:0.9362496391586635, Val loss:5.378852246536149
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93625
wandb:         Val Loss 5.37885
wandb:      train_batch 110
wandb: train_batch_loss 1.05689
wandb:        val_batch 30
wandb:   val_batch_loss 3.47755
wandb: 
wandb: üöÄ View run polar-star-361 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/xaknj3gx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_033154-xaknj3gx/logs
Seed completed execution! 42 0.7_1
------------------------------------------------------------------
Running for seed 89 of experiment 0.7_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_033940-b4z0wup6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-forest-363
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/b4z0wup6
Epoch 0, Batch 0, train loss:7.862167835235596, Elapsed time for epoch : 0.01342492898305257
Epoch 0, Batch 10, train loss:3.255730152130127, Elapsed time for epoch : 0.12871599594751995
Epoch 0, Batch 20, train loss:2.651141405105591, Elapsed time for epoch : 0.24429678122202556
Epoch 0, Batch 30, train loss:2.353551149368286, Elapsed time for epoch : 0.36017791827519735
Epoch 0, Batch 40, train loss:2.1881794929504395, Elapsed time for epoch : 0.4761770685513814
Epoch 0, Batch 50, train loss:1.9866702556610107, Elapsed time for epoch : 0.5920260628064473
Epoch 0, Batch 60, train loss:1.9703415632247925, Elapsed time for epoch : 0.7081274429957072
Epoch 0, Batch 70, train loss:1.9410392045974731, Elapsed time for epoch : 0.8241336425145467
Epoch 0, Batch 80, train loss:1.9307727813720703, Elapsed time for epoch : 0.9399877746899923
Epoch 0, Batch 90, train loss:1.6565821170806885, Elapsed time for epoch : 1.0561070442199707
Epoch 0, Batch 100, train loss:1.6503229141235352, Elapsed time for epoch : 1.1721406817436217
Epoch 0, Batch 110, train loss:1.5085197687149048, Elapsed time for epoch : 1.2879920283953348
Batch 0, val loss:7.020512104034424
Batch 10, val loss:4.581690311431885
Batch 20, val loss:3.577476739883423
Batch 30, val loss:16.800762176513672
Epoch 0, Train Loss:3.3330196100732556, Val loss:5.127083036634657
Epoch 1, Batch 0, train loss:1.6317623853683472, Elapsed time for epoch : 0.01162799596786499
Epoch 1, Batch 10, train loss:1.4623563289642334, Elapsed time for epoch : 0.12755394379297894
Epoch 1, Batch 20, train loss:1.5255845785140991, Elapsed time for epoch : 0.24400195280710857
Epoch 1, Batch 30, train loss:1.4484397172927856, Elapsed time for epoch : 0.35981410344441733
Epoch 1, Batch 40, train loss:1.4189138412475586, Elapsed time for epoch : 0.475788406531016
Epoch 1, Batch 50, train loss:1.3809001445770264, Elapsed time for epoch : 0.5916600584983825
Epoch 1, Batch 60, train loss:1.3070915937423706, Elapsed time for epoch : 0.7076838970184326
Epoch 1, Batch 70, train loss:1.390528917312622, Elapsed time for epoch : 0.8238523403803507
Epoch 1, Batch 80, train loss:1.4257593154907227, Elapsed time for epoch : 0.9401493509610493
Epoch 1, Batch 90, train loss:1.287817358970642, Elapsed time for epoch : 1.0563782890637716
Epoch 1, Batch 100, train loss:1.1563799381256104, Elapsed time for epoch : 1.1722986896832783
Epoch 1, Batch 110, train loss:1.2431243658065796, Elapsed time for epoch : 1.2887133916219076
Batch 0, val loss:2.039937973022461
Batch 10, val loss:9.388875007629395
Batch 20, val loss:2.1418869495391846
Batch 30, val loss:2.2809221744537354
Epoch 1, Train Loss:1.4126696959785794, Val loss:6.556566493378745
Epoch 2, Batch 0, train loss:1.245603322982788, Elapsed time for epoch : 0.011597534020741781
Epoch 2, Batch 10, train loss:1.262674331665039, Elapsed time for epoch : 0.12758711576461793
Epoch 2, Batch 20, train loss:1.1894943714141846, Elapsed time for epoch : 0.24389381408691407
Epoch 2, Batch 30, train loss:1.152649998664856, Elapsed time for epoch : 0.3600703279177348
Epoch 2, Batch 40, train loss:1.1326062679290771, Elapsed time for epoch : 0.47611497243245443
Epoch 2, Batch 50, train loss:1.170149803161621, Elapsed time for epoch : 0.5925645748774211
Epoch 2, Batch 60, train loss:1.1085522174835205, Elapsed time for epoch : 0.7089028080304464
Epoch 2, Batch 70, train loss:1.0400867462158203, Elapsed time for epoch : 0.8249957998593648
Epoch 2, Batch 80, train loss:1.1706706285476685, Elapsed time for epoch : 0.9414060552914937
Epoch 2, Batch 90, train loss:1.1201276779174805, Elapsed time for epoch : 1.0575512329737344
Epoch 2, Batch 100, train loss:1.1251094341278076, Elapsed time for epoch : 1.17356538772583
Epoch 2, Batch 110, train loss:1.0961710214614868, Elapsed time for epoch : 1.2895707368850708
Batch 0, val loss:9.513059616088867
Batch 10, val loss:2.0773961544036865
Batch 20, val loss:5.383362293243408
Batch 30, val loss:7.7884440422058105
Epoch 2, Train Loss:1.132350142105766, Val loss:9.055281397369173
Epoch 3, Batch 0, train loss:1.09443199634552, Elapsed time for epoch : 0.011679474512736003
Epoch 3, Batch 10, train loss:1.049615740776062, Elapsed time for epoch : 0.1278775930404663
Epoch 3, Batch 20, train loss:1.0715888738632202, Elapsed time for epoch : 0.24379334847132364
Epoch 3, Batch 30, train loss:1.0999393463134766, Elapsed time for epoch : 0.3597386439641317
Epoch 3, Batch 40, train loss:1.1221307516098022, Elapsed time for epoch : 0.4764160474141439
Epoch 3, Batch 50, train loss:0.9366193413734436, Elapsed time for epoch : 0.5922587235768636
Epoch 3, Batch 60, train loss:1.050418496131897, Elapsed time for epoch : 0.7085468173027039
Epoch 3, Batch 70, train loss:1.0660995244979858, Elapsed time for epoch : 0.824408479531606
Epoch 3, Batch 80, train loss:1.0528372526168823, Elapsed time for epoch : 0.9405763069788615
Epoch 3, Batch 90, train loss:1.0260642766952515, Elapsed time for epoch : 1.057065188884735
Epoch 3, Batch 100, train loss:0.7672774195671082, Elapsed time for epoch : 1.1728504101435344
Epoch 3, Batch 110, train loss:0.95247882604599, Elapsed time for epoch : 1.2890799244244893
Batch 0, val loss:4.790701389312744
Batch 10, val loss:16.238605499267578
Batch 20, val loss:15.224665641784668
Batch 30, val loss:3.409703493118286
Epoch 3, Train Loss:1.0092262345811596, Val loss:8.355670279926724
Epoch 4, Batch 0, train loss:0.9314046502113342, Elapsed time for epoch : 0.01161041259765625
Epoch 4, Batch 10, train loss:0.876451313495636, Elapsed time for epoch : 0.12815850973129272
Epoch 4, Batch 20, train loss:0.9200505614280701, Elapsed time for epoch : 0.24441198507944742
Epoch 4, Batch 30, train loss:0.8954121470451355, Elapsed time for epoch : 0.36070682605107623
Epoch 4, Batch 40, train loss:0.9507360458374023, Elapsed time for epoch : 0.4774661938349406
Epoch 4, Batch 50, train loss:1.038448452949524, Elapsed time for epoch : 0.5937925855318705
Epoch 4, Batch 60, train loss:0.8899649381637573, Elapsed time for epoch : 0.7096911787986755
Epoch 4, Batch 70, train loss:1.034987449645996, Elapsed time for epoch : 0.8260470310846965
Epoch 4, Batch 80, train loss:0.9090471267700195, Elapsed time for epoch : 0.9425623615582784
Epoch 4, Batch 90, train loss:0.9144729971885681, Elapsed time for epoch : 1.0584341486295064
Epoch 4, Batch 100, train loss:0.9577097296714783, Elapsed time for epoch : 1.1749954581260682
Epoch 4, Batch 110, train loss:1.0568903684616089, Elapsed time for epoch : 1.2911190787951152
Batch 0, val loss:1.772247314453125
Batch 10, val loss:2.011570692062378
Batch 20, val loss:3.2935595512390137
Batch 30, val loss:3.4775474071502686
Epoch 4, Train Loss:0.9362496391586635, Val loss:5.378852246536149
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93625
wandb:         Val Loss 5.37885
wandb:      train_batch 110
wandb: train_batch_loss 1.05689
wandb:        val_batch 30
wandb:   val_batch_loss 3.47755
wandb: 
wandb: üöÄ View run fresh-forest-363 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/b4z0wup6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_033940-b4z0wup6/logs
Seed completed execution! 89 0.7_1
------------------------------------------------------------------
Running for seed 23 of experiment 0.7_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_034726-yd0g9zti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-puddle-365
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/yd0g9zti
Epoch 0, Batch 0, train loss:7.862167835235596, Elapsed time for epoch : 0.013379851977030436
Epoch 0, Batch 10, train loss:3.255730152130127, Elapsed time for epoch : 0.12925045490264891
Epoch 0, Batch 20, train loss:2.651141405105591, Elapsed time for epoch : 0.24505674441655476
Epoch 0, Batch 30, train loss:2.353551149368286, Elapsed time for epoch : 0.3610102891921997
Epoch 0, Batch 40, train loss:2.1881794929504395, Elapsed time for epoch : 0.4767664273579915
Epoch 0, Batch 50, train loss:1.9866702556610107, Elapsed time for epoch : 0.5931389292081197
Epoch 0, Batch 60, train loss:1.9703415632247925, Elapsed time for epoch : 0.7090039292971293
Epoch 0, Batch 70, train loss:1.9410392045974731, Elapsed time for epoch : 0.825007696946462
Epoch 0, Batch 80, train loss:1.9307727813720703, Elapsed time for epoch : 0.9410050789515177
Epoch 0, Batch 90, train loss:1.6565821170806885, Elapsed time for epoch : 1.0571083347002665
Epoch 0, Batch 100, train loss:1.6503229141235352, Elapsed time for epoch : 1.173124607404073
Epoch 0, Batch 110, train loss:1.5085197687149048, Elapsed time for epoch : 1.2894120295842488
Batch 0, val loss:7.020512104034424
Batch 10, val loss:4.581690311431885
Batch 20, val loss:3.577476739883423
Batch 30, val loss:16.800762176513672
Epoch 0, Train Loss:3.3330196100732556, Val loss:5.127083036634657
Epoch 1, Batch 0, train loss:1.6317623853683472, Elapsed time for epoch : 0.011628997325897217
Epoch 1, Batch 10, train loss:1.4623563289642334, Elapsed time for epoch : 0.12760507265726725
Epoch 1, Batch 20, train loss:1.5255845785140991, Elapsed time for epoch : 0.2437801718711853
Epoch 1, Batch 30, train loss:1.4484397172927856, Elapsed time for epoch : 0.3599320848782857
Epoch 1, Batch 40, train loss:1.4189138412475586, Elapsed time for epoch : 0.4762534976005554
Epoch 1, Batch 50, train loss:1.3809001445770264, Elapsed time for epoch : 0.5926672259966532
Epoch 1, Batch 60, train loss:1.3070915937423706, Elapsed time for epoch : 0.7087507804234823
Epoch 1, Batch 70, train loss:1.390528917312622, Elapsed time for epoch : 0.8248971899350485
Epoch 1, Batch 80, train loss:1.4257593154907227, Elapsed time for epoch : 0.941255259513855
Epoch 1, Batch 90, train loss:1.287817358970642, Elapsed time for epoch : 1.0571804126103719
Epoch 1, Batch 100, train loss:1.1563799381256104, Elapsed time for epoch : 1.1732919255892436
Epoch 1, Batch 110, train loss:1.2431243658065796, Elapsed time for epoch : 1.289259382088979
Batch 0, val loss:2.039937973022461
Batch 10, val loss:9.388875007629395
Batch 20, val loss:2.1418869495391846
Batch 30, val loss:2.2809221744537354
Epoch 1, Train Loss:1.4126696959785794, Val loss:6.556566493378745
Epoch 2, Batch 0, train loss:1.245603322982788, Elapsed time for epoch : 0.011705509821573893
Epoch 2, Batch 10, train loss:1.262674331665039, Elapsed time for epoch : 0.12822611729303995
Epoch 2, Batch 20, train loss:1.1894943714141846, Elapsed time for epoch : 0.24452253580093383
Epoch 2, Batch 30, train loss:1.152649998664856, Elapsed time for epoch : 0.36046897172927855
Epoch 2, Batch 40, train loss:1.1326062679290771, Elapsed time for epoch : 0.4765941619873047
Epoch 2, Batch 50, train loss:1.170149803161621, Elapsed time for epoch : 0.5931661685307821
Epoch 2, Batch 60, train loss:1.1085522174835205, Elapsed time for epoch : 0.7093226830164592
Epoch 2, Batch 70, train loss:1.0400867462158203, Elapsed time for epoch : 0.8251737157503763
Epoch 2, Batch 80, train loss:1.1706706285476685, Elapsed time for epoch : 0.9414685209592183
Epoch 2, Batch 90, train loss:1.1201276779174805, Elapsed time for epoch : 1.0575867931048075
Epoch 2, Batch 100, train loss:1.1251094341278076, Elapsed time for epoch : 1.1739913781483968
Epoch 2, Batch 110, train loss:1.0961710214614868, Elapsed time for epoch : 1.2901920994122824
Batch 0, val loss:9.513059616088867
Batch 10, val loss:2.0773961544036865
Batch 20, val loss:5.383362293243408
Batch 30, val loss:7.7884440422058105
Epoch 2, Train Loss:1.132350142105766, Val loss:9.055281397369173
Epoch 3, Batch 0, train loss:1.09443199634552, Elapsed time for epoch : 0.011954724788665771
Epoch 3, Batch 10, train loss:1.049615740776062, Elapsed time for epoch : 0.12819888989130657
Epoch 3, Batch 20, train loss:1.0715888738632202, Elapsed time for epoch : 0.244329833984375
Epoch 3, Batch 30, train loss:1.0999393463134766, Elapsed time for epoch : 0.36050648291905724
Epoch 3, Batch 40, train loss:1.1221307516098022, Elapsed time for epoch : 0.4765561858812968
Epoch 3, Batch 50, train loss:0.9366193413734436, Elapsed time for epoch : 0.5925512353579203
Epoch 3, Batch 60, train loss:1.050418496131897, Elapsed time for epoch : 0.7089850306510925
Epoch 3, Batch 70, train loss:1.0660995244979858, Elapsed time for epoch : 0.8254196961720784
Epoch 3, Batch 80, train loss:1.0528372526168823, Elapsed time for epoch : 0.9411937236785889
Epoch 3, Batch 90, train loss:1.0260642766952515, Elapsed time for epoch : 1.0572100361188252
Epoch 3, Batch 100, train loss:0.7672774195671082, Elapsed time for epoch : 1.1735793312390645
Epoch 3, Batch 110, train loss:0.95247882604599, Elapsed time for epoch : 1.2897379477818807
Batch 0, val loss:4.790701389312744
Batch 10, val loss:16.238605499267578
Batch 20, val loss:15.224665641784668
Batch 30, val loss:3.409703493118286
Epoch 3, Train Loss:1.0092262345811596, Val loss:8.355670279926724
Epoch 4, Batch 0, train loss:0.9314046502113342, Elapsed time for epoch : 0.011660949389139811
Epoch 4, Batch 10, train loss:0.876451313495636, Elapsed time for epoch : 0.1286825696627299
Epoch 4, Batch 20, train loss:0.9200505614280701, Elapsed time for epoch : 0.24524956544240314
Epoch 4, Batch 30, train loss:0.8954121470451355, Elapsed time for epoch : 0.3610836625099182
Epoch 4, Batch 40, train loss:0.9507360458374023, Elapsed time for epoch : 0.47807865142822265
Epoch 4, Batch 50, train loss:1.038448452949524, Elapsed time for epoch : 0.5943021217981974
Epoch 4, Batch 60, train loss:0.8899649381637573, Elapsed time for epoch : 0.7108764171600341
Epoch 4, Batch 70, train loss:1.034987449645996, Elapsed time for epoch : 0.8272803862889607
Epoch 4, Batch 80, train loss:0.9090471267700195, Elapsed time for epoch : 0.9432944297790528
Epoch 4, Batch 90, train loss:0.9144729971885681, Elapsed time for epoch : 1.0593259970347086
Epoch 4, Batch 100, train loss:0.9577097296714783, Elapsed time for epoch : 1.175106453895569
Epoch 4, Batch 110, train loss:1.0568903684616089, Elapsed time for epoch : 1.2912752469380697
Batch 0, val loss:1.772247314453125
Batch 10, val loss:2.011570692062378
Batch 20, val loss:3.2935595512390137
Batch 30, val loss:3.4775474071502686
Epoch 4, Train Loss:0.9362496391586635, Val loss:5.378852246536149
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93625
wandb:         Val Loss 5.37885
wandb:      train_batch 110
wandb: train_batch_loss 1.05689
wandb:        val_batch 30
wandb:   val_batch_loss 3.47755
wandb: 
wandb: üöÄ View run stoic-puddle-365 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/yd0g9zti
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_034726-yd0g9zti/logs
Seed completed execution! 23 0.7_1
------------------------------------------------------------------
Running for seed 113 of experiment 0.7_1
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_035512-98cuxwye
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-elevator-367
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/98cuxwye
Epoch 0, Batch 0, train loss:7.862167835235596, Elapsed time for epoch : 0.012170469760894776
Epoch 0, Batch 10, train loss:3.255730152130127, Elapsed time for epoch : 0.1295776128768921
Epoch 0, Batch 20, train loss:2.651141405105591, Elapsed time for epoch : 0.24587871233622233
Epoch 0, Batch 30, train loss:2.353551149368286, Elapsed time for epoch : 0.3623557130495707
Epoch 0, Batch 40, train loss:2.1881794929504395, Elapsed time for epoch : 0.47935465574264524
Epoch 0, Batch 50, train loss:1.9866702556610107, Elapsed time for epoch : 0.5965472340583802
Epoch 0, Batch 60, train loss:1.9703415632247925, Elapsed time for epoch : 0.7132770220438639
Epoch 0, Batch 70, train loss:1.9410392045974731, Elapsed time for epoch : 0.8298132697741191
Epoch 0, Batch 80, train loss:1.9307727813720703, Elapsed time for epoch : 0.9466508030891418
Epoch 0, Batch 90, train loss:1.6565821170806885, Elapsed time for epoch : 1.0631646116574605
Epoch 0, Batch 100, train loss:1.6503229141235352, Elapsed time for epoch : 1.1797929724057516
Epoch 0, Batch 110, train loss:1.5085197687149048, Elapsed time for epoch : 1.297294290860494
Batch 0, val loss:7.020512104034424
Batch 10, val loss:4.581690311431885
Batch 20, val loss:3.577476739883423
Batch 30, val loss:16.800762176513672
Epoch 0, Train Loss:3.3330196100732556, Val loss:5.127083036634657
Epoch 1, Batch 0, train loss:1.6317623853683472, Elapsed time for epoch : 0.011656065781911215
Epoch 1, Batch 10, train loss:1.4623563289642334, Elapsed time for epoch : 0.12807159026463827
Epoch 1, Batch 20, train loss:1.5255845785140991, Elapsed time for epoch : 0.24417597850163777
Epoch 1, Batch 30, train loss:1.4484397172927856, Elapsed time for epoch : 0.3601239760716756
Epoch 1, Batch 40, train loss:1.4189138412475586, Elapsed time for epoch : 0.47625606060028075
Epoch 1, Batch 50, train loss:1.3809001445770264, Elapsed time for epoch : 0.592233125368754
Epoch 1, Batch 60, train loss:1.3070915937423706, Elapsed time for epoch : 0.7082611997922261
Epoch 1, Batch 70, train loss:1.390528917312622, Elapsed time for epoch : 0.8239473740259806
Epoch 1, Batch 80, train loss:1.4257593154907227, Elapsed time for epoch : 0.9401305516560873
Epoch 1, Batch 90, train loss:1.287817358970642, Elapsed time for epoch : 1.056289517879486
Epoch 1, Batch 100, train loss:1.1563799381256104, Elapsed time for epoch : 1.1722659150759378
Epoch 1, Batch 110, train loss:1.2431243658065796, Elapsed time for epoch : 1.2885512391726175
Batch 0, val loss:2.039937973022461
Batch 10, val loss:9.388875007629395
Batch 20, val loss:2.1418869495391846
Batch 30, val loss:2.2809221744537354
Epoch 1, Train Loss:1.4126696959785794, Val loss:6.556566493378745
Epoch 2, Batch 0, train loss:1.245603322982788, Elapsed time for epoch : 0.011668280760447184
Epoch 2, Batch 10, train loss:1.262674331665039, Elapsed time for epoch : 0.12766586542129515
Epoch 2, Batch 20, train loss:1.1894943714141846, Elapsed time for epoch : 0.24411099751790363
Epoch 2, Batch 30, train loss:1.152649998664856, Elapsed time for epoch : 0.35987333456675213
Epoch 2, Batch 40, train loss:1.1326062679290771, Elapsed time for epoch : 0.4758736888567607
Epoch 2, Batch 50, train loss:1.170149803161621, Elapsed time for epoch : 0.5920737187067667
Epoch 2, Batch 60, train loss:1.1085522174835205, Elapsed time for epoch : 0.7081023097038269
Epoch 2, Batch 70, train loss:1.0400867462158203, Elapsed time for epoch : 0.8243997097015381
Epoch 2, Batch 80, train loss:1.1706706285476685, Elapsed time for epoch : 0.9404077450434367
Epoch 2, Batch 90, train loss:1.1201276779174805, Elapsed time for epoch : 1.0561217188835144
Epoch 2, Batch 100, train loss:1.1251094341278076, Elapsed time for epoch : 1.1723729610443114
Epoch 2, Batch 110, train loss:1.0961710214614868, Elapsed time for epoch : 1.2882777810096742
Batch 0, val loss:9.513059616088867
Batch 10, val loss:2.0773961544036865
Batch 20, val loss:5.383362293243408
Batch 30, val loss:7.7884440422058105
Epoch 2, Train Loss:1.132350142105766, Val loss:9.055281397369173
Epoch 3, Batch 0, train loss:1.09443199634552, Elapsed time for epoch : 0.011558945973714192
Epoch 3, Batch 10, train loss:1.049615740776062, Elapsed time for epoch : 0.12810817162195842
Epoch 3, Batch 20, train loss:1.0715888738632202, Elapsed time for epoch : 0.2443694273630778
Epoch 3, Batch 30, train loss:1.0999393463134766, Elapsed time for epoch : 0.3604477246602376
Epoch 3, Batch 40, train loss:1.1221307516098022, Elapsed time for epoch : 0.47635343472162883
Epoch 3, Batch 50, train loss:0.9366193413734436, Elapsed time for epoch : 0.5921608924865722
Epoch 3, Batch 60, train loss:1.050418496131897, Elapsed time for epoch : 0.70854305823644
Epoch 3, Batch 70, train loss:1.0660995244979858, Elapsed time for epoch : 0.8249069134394328
Epoch 3, Batch 80, train loss:1.0528372526168823, Elapsed time for epoch : 0.9413830796877544
Epoch 3, Batch 90, train loss:1.0260642766952515, Elapsed time for epoch : 1.0568100810050964
Epoch 3, Batch 100, train loss:0.7672774195671082, Elapsed time for epoch : 1.1729129076004028
Epoch 3, Batch 110, train loss:0.95247882604599, Elapsed time for epoch : 1.2892943024635315
Batch 0, val loss:4.790701389312744
Batch 10, val loss:16.238605499267578
Batch 20, val loss:15.224665641784668
Batch 30, val loss:3.409703493118286
Epoch 3, Train Loss:1.0092262345811596, Val loss:8.355670279926724
Epoch 4, Batch 0, train loss:0.9314046502113342, Elapsed time for epoch : 0.011698397000630696
Epoch 4, Batch 10, train loss:0.876451313495636, Elapsed time for epoch : 0.12808660666147867
Epoch 4, Batch 20, train loss:0.9200505614280701, Elapsed time for epoch : 0.24413546323776245
Epoch 4, Batch 30, train loss:0.8954121470451355, Elapsed time for epoch : 0.3613417387008667
Epoch 4, Batch 40, train loss:0.9507360458374023, Elapsed time for epoch : 0.4780747612317403
Epoch 4, Batch 50, train loss:1.038448452949524, Elapsed time for epoch : 0.5946595549583436
Epoch 4, Batch 60, train loss:0.8899649381637573, Elapsed time for epoch : 0.7107706189155578
Epoch 4, Batch 70, train loss:1.034987449645996, Elapsed time for epoch : 0.827100682258606
Epoch 4, Batch 80, train loss:0.9090471267700195, Elapsed time for epoch : 0.9429404616355896
Epoch 4, Batch 90, train loss:0.9144729971885681, Elapsed time for epoch : 1.0592554966608683
Epoch 4, Batch 100, train loss:0.9577097296714783, Elapsed time for epoch : 1.175301206111908
Epoch 4, Batch 110, train loss:1.0568903684616089, Elapsed time for epoch : 1.2911813775698344
Batch 0, val loss:1.772247314453125
Batch 10, val loss:2.011570692062378
Batch 20, val loss:3.2935595512390137
Batch 30, val loss:3.4775474071502686
Epoch 4, Train Loss:0.9362496391586635, Val loss:5.378852246536149
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÑ‚ñà‚ñá‚ñÅ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.93625
wandb:         Val Loss 5.37885
wandb:      train_batch 110
wandb: train_batch_loss 1.05689
wandb:        val_batch 30
wandb:   val_batch_loss 3.47755
wandb: 
wandb: üöÄ View run celestial-elevator-367 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/98cuxwye
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_035512-98cuxwye/logs
Seed completed execution! 113 0.7_1
------------------------------------------------------------------
Experiment complete 0.7_1
==========================================================================
Running experiment for setting 0.7_2
==========================================================================
Running for seed 1 of experiment 0.7_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_040258-e10d0rae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-totem-369
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e10d0rae
Epoch 0, Batch 0, train loss:7.524460792541504, Elapsed time for epoch : 0.013454171021779378
Epoch 0, Batch 10, train loss:3.994114398956299, Elapsed time for epoch : 0.13818369706471761
Epoch 0, Batch 20, train loss:2.5958220958709717, Elapsed time for epoch : 0.26209473609924316
Epoch 0, Batch 30, train loss:2.4670469760894775, Elapsed time for epoch : 0.3861963629722595
Epoch 0, Batch 40, train loss:2.1660773754119873, Elapsed time for epoch : 0.510299535592397
Epoch 0, Batch 50, train loss:2.1282804012298584, Elapsed time for epoch : 0.6352521379788717
Epoch 0, Batch 60, train loss:1.79402756690979, Elapsed time for epoch : 0.7601104140281677
Epoch 0, Batch 70, train loss:1.9038660526275635, Elapsed time for epoch : 0.8852675716082256
Epoch 0, Batch 80, train loss:1.9154592752456665, Elapsed time for epoch : 1.0105488419532775
Epoch 0, Batch 90, train loss:1.556204915046692, Elapsed time for epoch : 1.1362054864565532
Epoch 0, Batch 100, train loss:1.5877162218093872, Elapsed time for epoch : 1.261291797955831
Epoch 0, Batch 110, train loss:1.4271976947784424, Elapsed time for epoch : 1.386485517024994
Batch 0, val loss:3.685748338699341
Batch 10, val loss:6.096495628356934
Batch 20, val loss:3.5271031856536865
Batch 30, val loss:5.060764312744141
Epoch 0, Train Loss:2.8872333485147226, Val loss:3.1920104093021817
Epoch 1, Batch 0, train loss:1.5020750761032104, Elapsed time for epoch : 0.011603999137878417
Epoch 1, Batch 10, train loss:1.396649956703186, Elapsed time for epoch : 0.1276891589164734
Epoch 1, Batch 20, train loss:1.3024944067001343, Elapsed time for epoch : 0.24374688069025677
Epoch 1, Batch 30, train loss:1.4660276174545288, Elapsed time for epoch : 0.3600832978884379
Epoch 1, Batch 40, train loss:1.308589220046997, Elapsed time for epoch : 0.4761609673500061
Epoch 1, Batch 50, train loss:1.3007009029388428, Elapsed time for epoch : 0.5922899127006531
Epoch 1, Batch 60, train loss:1.328384518623352, Elapsed time for epoch : 0.708915380636851
Epoch 1, Batch 70, train loss:1.1971256732940674, Elapsed time for epoch : 0.8254640142122904
Epoch 1, Batch 80, train loss:1.1400244235992432, Elapsed time for epoch : 0.9415862917900085
Epoch 1, Batch 90, train loss:1.2270210981369019, Elapsed time for epoch : 1.0580343325932822
Epoch 1, Batch 100, train loss:1.0267614126205444, Elapsed time for epoch : 1.1744688550631206
Epoch 1, Batch 110, train loss:1.0085093975067139, Elapsed time for epoch : 1.291049559911092
Batch 0, val loss:2.18146014213562
Batch 10, val loss:4.5032639503479
Batch 20, val loss:2.5654921531677246
Batch 30, val loss:2.3982980251312256
Epoch 1, Train Loss:1.2647468136704487, Val loss:6.293603791130914
Epoch 2, Batch 0, train loss:1.0662493705749512, Elapsed time for epoch : 0.011545336246490479
Epoch 2, Batch 10, train loss:1.052196741104126, Elapsed time for epoch : 0.1275837540626526
Epoch 2, Batch 20, train loss:1.1099567413330078, Elapsed time for epoch : 0.2447166363398234
Epoch 2, Batch 30, train loss:1.0627894401550293, Elapsed time for epoch : 0.361055862903595
Epoch 2, Batch 40, train loss:0.9988035559654236, Elapsed time for epoch : 0.477427860101064
Epoch 2, Batch 50, train loss:0.9644098281860352, Elapsed time for epoch : 0.593861190478007
Epoch 2, Batch 60, train loss:0.9823341965675354, Elapsed time for epoch : 0.7100577354431152
Epoch 2, Batch 70, train loss:0.7594824433326721, Elapsed time for epoch : 0.8264865040779114
Epoch 2, Batch 80, train loss:0.9600093960762024, Elapsed time for epoch : 0.9430766383806864
Epoch 2, Batch 90, train loss:0.9570473432540894, Elapsed time for epoch : 1.0593734979629517
Epoch 2, Batch 100, train loss:0.9648347496986389, Elapsed time for epoch : 1.1754303574562073
Epoch 2, Batch 110, train loss:0.8587432503700256, Elapsed time for epoch : 1.2919155637423196
Batch 0, val loss:8.383828163146973
Batch 10, val loss:1.770790457725525
Batch 20, val loss:1.921714186668396
Batch 30, val loss:17.910846710205078
Epoch 2, Train Loss:0.9738578894863958, Val loss:6.372041606240803
Epoch 3, Batch 0, train loss:0.9120631217956543, Elapsed time for epoch : 0.011703848838806152
Epoch 3, Batch 10, train loss:0.9519385099411011, Elapsed time for epoch : 0.12771490414937336
Epoch 3, Batch 20, train loss:0.955115795135498, Elapsed time for epoch : 0.24389756520589193
Epoch 3, Batch 30, train loss:0.8407045006752014, Elapsed time for epoch : 0.3601597428321838
Epoch 3, Batch 40, train loss:0.9371300935745239, Elapsed time for epoch : 0.4763686498006185
Epoch 3, Batch 50, train loss:0.5565146803855896, Elapsed time for epoch : 0.5929665327072143
Epoch 3, Batch 60, train loss:0.8375134468078613, Elapsed time for epoch : 0.7092991471290588
Epoch 3, Batch 70, train loss:0.928778350353241, Elapsed time for epoch : 0.8264855186144511
Epoch 3, Batch 80, train loss:0.8294297456741333, Elapsed time for epoch : 0.9429479122161866
Epoch 3, Batch 90, train loss:0.7704241871833801, Elapsed time for epoch : 1.0591565251350403
Epoch 3, Batch 100, train loss:0.5092821717262268, Elapsed time for epoch : 1.1755576133728027
Epoch 3, Batch 110, train loss:0.6362019777297974, Elapsed time for epoch : 1.2927303830782573
Batch 0, val loss:8.724692344665527
Batch 10, val loss:10.300885200500488
Batch 20, val loss:8.40817928314209
Batch 30, val loss:3.2478251457214355
Epoch 3, Train Loss:0.7956112615440203, Val loss:8.064994457695219
Epoch 4, Batch 0, train loss:0.7027440071105957, Elapsed time for epoch : 0.011588096618652344
Epoch 4, Batch 10, train loss:0.4292391836643219, Elapsed time for epoch : 0.12782569328943888
Epoch 4, Batch 20, train loss:0.47542840242385864, Elapsed time for epoch : 0.24427535136540732
Epoch 4, Batch 30, train loss:0.588658332824707, Elapsed time for epoch : 0.36078389883041384
Epoch 4, Batch 40, train loss:0.6915445923805237, Elapsed time for epoch : 0.4772254983584086
Epoch 4, Batch 50, train loss:0.6139757037162781, Elapsed time for epoch : 0.593616267045339
Epoch 4, Batch 60, train loss:0.6490853428840637, Elapsed time for epoch : 0.7097784678141276
Epoch 4, Batch 70, train loss:0.6970778703689575, Elapsed time for epoch : 0.826763141155243
Epoch 4, Batch 80, train loss:0.4695417881011963, Elapsed time for epoch : 0.9428173939387003
Epoch 4, Batch 90, train loss:0.5817186236381531, Elapsed time for epoch : 1.0589191476504007
Epoch 4, Batch 100, train loss:0.5298834443092346, Elapsed time for epoch : 1.1753524978955587
Epoch 4, Batch 110, train loss:0.5863550305366516, Elapsed time for epoch : 1.2915427168210347
Batch 0, val loss:1.9618512392044067
Batch 10, val loss:2.0727803707122803
Batch 20, val loss:2.1869587898254395
Batch 30, val loss:1.9915767908096313
Epoch 4, Train Loss:0.6002631920835246, Val loss:6.125668047202958
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.60026
wandb:         Val Loss 6.12567
wandb:      train_batch 110
wandb: train_batch_loss 0.58636
wandb:        val_batch 30
wandb:   val_batch_loss 1.99158
wandb: 
wandb: üöÄ View run wild-totem-369 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/e10d0rae
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_040258-e10d0rae/logs
Seed completed execution! 1 0.7_2
------------------------------------------------------------------
Running for seed 42 of experiment 0.7_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_041052-8v5lbmrb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-sound-371
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8v5lbmrb
Epoch 0, Batch 0, train loss:7.524460792541504, Elapsed time for epoch : 0.012882677714029948
Epoch 0, Batch 10, train loss:3.994114398956299, Elapsed time for epoch : 0.12885608275731406
Epoch 0, Batch 20, train loss:2.5958220958709717, Elapsed time for epoch : 0.24491459925969442
Epoch 0, Batch 30, train loss:2.4670469760894775, Elapsed time for epoch : 0.36089659929275514
Epoch 0, Batch 40, train loss:2.1660773754119873, Elapsed time for epoch : 0.47690776983896893
Epoch 0, Batch 50, train loss:2.1282804012298584, Elapsed time for epoch : 0.5928724050521851
Epoch 0, Batch 60, train loss:1.79402756690979, Elapsed time for epoch : 0.7091913501421611
Epoch 0, Batch 70, train loss:1.9038660526275635, Elapsed time for epoch : 0.825194255510966
Epoch 0, Batch 80, train loss:1.9154592752456665, Elapsed time for epoch : 0.9412160873413086
Epoch 0, Batch 90, train loss:1.556204915046692, Elapsed time for epoch : 1.0580556432406107
Epoch 0, Batch 100, train loss:1.5877162218093872, Elapsed time for epoch : 1.1746488372484842
Epoch 0, Batch 110, train loss:1.4271976947784424, Elapsed time for epoch : 1.2906087001164754
Batch 0, val loss:3.685748338699341
Batch 10, val loss:6.096495628356934
Batch 20, val loss:3.5271031856536865
Batch 30, val loss:5.060764312744141
Epoch 0, Train Loss:2.8872333485147226, Val loss:3.1920104093021817
Epoch 1, Batch 0, train loss:1.5020750761032104, Elapsed time for epoch : 0.011602826913197835
Epoch 1, Batch 10, train loss:1.396649956703186, Elapsed time for epoch : 0.12796305020650228
Epoch 1, Batch 20, train loss:1.3024944067001343, Elapsed time for epoch : 0.2443782091140747
Epoch 1, Batch 30, train loss:1.4660276174545288, Elapsed time for epoch : 0.3605089982350667
Epoch 1, Batch 40, train loss:1.308589220046997, Elapsed time for epoch : 0.4765085736910502
Epoch 1, Batch 50, train loss:1.3007009029388428, Elapsed time for epoch : 0.5921977361043295
Epoch 1, Batch 60, train loss:1.328384518623352, Elapsed time for epoch : 0.7079451322555542
Epoch 1, Batch 70, train loss:1.1971256732940674, Elapsed time for epoch : 0.8243279218673706
Epoch 1, Batch 80, train loss:1.1400244235992432, Elapsed time for epoch : 0.9403958559036255
Epoch 1, Batch 90, train loss:1.2270210981369019, Elapsed time for epoch : 1.0563383221626281
Epoch 1, Batch 100, train loss:1.0267614126205444, Elapsed time for epoch : 1.1724311629931132
Epoch 1, Batch 110, train loss:1.0085093975067139, Elapsed time for epoch : 1.2882734696070353
Batch 0, val loss:2.18146014213562
Batch 10, val loss:4.5032639503479
Batch 20, val loss:2.5654921531677246
Batch 30, val loss:2.3982980251312256
Epoch 1, Train Loss:1.2647468136704487, Val loss:6.293603791130914
Epoch 2, Batch 0, train loss:1.0662493705749512, Elapsed time for epoch : 0.011607976754506429
Epoch 2, Batch 10, train loss:1.052196741104126, Elapsed time for epoch : 0.1281007409095764
Epoch 2, Batch 20, train loss:1.1099567413330078, Elapsed time for epoch : 0.24418879350026448
Epoch 2, Batch 30, train loss:1.0627894401550293, Elapsed time for epoch : 0.3604590614636739
Epoch 2, Batch 40, train loss:0.9988035559654236, Elapsed time for epoch : 0.47717071771621705
Epoch 2, Batch 50, train loss:0.9644098281860352, Elapsed time for epoch : 0.5933430790901184
Epoch 2, Batch 60, train loss:0.9823341965675354, Elapsed time for epoch : 0.7099844018618265
Epoch 2, Batch 70, train loss:0.7594824433326721, Elapsed time for epoch : 0.8262130379676819
Epoch 2, Batch 80, train loss:0.9600093960762024, Elapsed time for epoch : 0.9431454976399739
Epoch 2, Batch 90, train loss:0.9570473432540894, Elapsed time for epoch : 1.0591389298439027
Epoch 2, Batch 100, train loss:0.9648347496986389, Elapsed time for epoch : 1.1757043759028116
Epoch 2, Batch 110, train loss:0.8587432503700256, Elapsed time for epoch : 1.2920371095339458
Batch 0, val loss:8.383828163146973
Batch 10, val loss:1.770790457725525
Batch 20, val loss:1.921714186668396
Batch 30, val loss:17.910846710205078
Epoch 2, Train Loss:0.9738578894863958, Val loss:6.372041606240803
Epoch 3, Batch 0, train loss:0.9120631217956543, Elapsed time for epoch : 0.011693040529886881
Epoch 3, Batch 10, train loss:0.9519385099411011, Elapsed time for epoch : 0.1280014395713806
Epoch 3, Batch 20, train loss:0.955115795135498, Elapsed time for epoch : 0.24424749215443928
Epoch 3, Batch 30, train loss:0.8407045006752014, Elapsed time for epoch : 0.3600967248280843
Epoch 3, Batch 40, train loss:0.9371300935745239, Elapsed time for epoch : 0.4771507183710734
Epoch 3, Batch 50, train loss:0.5565146803855896, Elapsed time for epoch : 0.593255341053009
Epoch 3, Batch 60, train loss:0.8375134468078613, Elapsed time for epoch : 0.7099520564079285
Epoch 3, Batch 70, train loss:0.928778350353241, Elapsed time for epoch : 0.82673233350118
Epoch 3, Batch 80, train loss:0.8294297456741333, Elapsed time for epoch : 0.9434553384780884
Epoch 3, Batch 90, train loss:0.7704241871833801, Elapsed time for epoch : 1.059858512878418
Epoch 3, Batch 100, train loss:0.5092821717262268, Elapsed time for epoch : 1.1760634263356526
Epoch 3, Batch 110, train loss:0.6362019777297974, Elapsed time for epoch : 1.2925474087397257
Batch 0, val loss:8.724692344665527
Batch 10, val loss:10.300885200500488
Batch 20, val loss:8.40817928314209
Batch 30, val loss:3.2478251457214355
Epoch 3, Train Loss:0.7956112615440203, Val loss:8.064994457695219
Epoch 4, Batch 0, train loss:0.7027440071105957, Elapsed time for epoch : 0.01180594762166341
Epoch 4, Batch 10, train loss:0.4292391836643219, Elapsed time for epoch : 0.12817378838857016
Epoch 4, Batch 20, train loss:0.47542840242385864, Elapsed time for epoch : 0.24461297591527303
Epoch 4, Batch 30, train loss:0.588658332824707, Elapsed time for epoch : 0.36095032294591267
Epoch 4, Batch 40, train loss:0.6915445923805237, Elapsed time for epoch : 0.4775981426239014
Epoch 4, Batch 50, train loss:0.6139757037162781, Elapsed time for epoch : 0.5938609560330709
Epoch 4, Batch 60, train loss:0.6490853428840637, Elapsed time for epoch : 0.7100080092748006
Epoch 4, Batch 70, train loss:0.6970778703689575, Elapsed time for epoch : 0.8264328519503276
Epoch 4, Batch 80, train loss:0.4695417881011963, Elapsed time for epoch : 0.9428588072458903
Epoch 4, Batch 90, train loss:0.5817186236381531, Elapsed time for epoch : 1.0593031247456868
Epoch 4, Batch 100, train loss:0.5298834443092346, Elapsed time for epoch : 1.1752485553423564
Epoch 4, Batch 110, train loss:0.5863550305366516, Elapsed time for epoch : 1.291924206415812
Batch 0, val loss:1.9618512392044067
Batch 10, val loss:2.0727803707122803
Batch 20, val loss:2.1869587898254395
Batch 30, val loss:1.9915767908096313
Epoch 4, Train Loss:0.6002631920835246, Val loss:6.125668047202958
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.60026
wandb:         Val Loss 6.12567
wandb:      train_batch 110
wandb: train_batch_loss 0.58636
wandb:        val_batch 30
wandb:   val_batch_loss 1.99158
wandb: 
wandb: üöÄ View run sage-sound-371 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8v5lbmrb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_041052-8v5lbmrb/logs
Seed completed execution! 42 0.7_2
------------------------------------------------------------------
Running for seed 89 of experiment 0.7_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_041838-wu1lmwjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eternal-dragon-373
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wu1lmwjb
Epoch 0, Batch 0, train loss:7.524460792541504, Elapsed time for epoch : 0.013382172584533692
Epoch 0, Batch 10, train loss:3.994114398956299, Elapsed time for epoch : 0.1295885403951009
Epoch 0, Batch 20, train loss:2.5958220958709717, Elapsed time for epoch : 0.24561890761057537
Epoch 0, Batch 30, train loss:2.4670469760894775, Elapsed time for epoch : 0.3619491418202718
Epoch 0, Batch 40, train loss:2.1660773754119873, Elapsed time for epoch : 0.4780434489250183
Epoch 0, Batch 50, train loss:2.1282804012298584, Elapsed time for epoch : 0.5944145083427429
Epoch 0, Batch 60, train loss:1.79402756690979, Elapsed time for epoch : 0.7106730222702027
Epoch 0, Batch 70, train loss:1.9038660526275635, Elapsed time for epoch : 0.8271130164464314
Epoch 0, Batch 80, train loss:1.9154592752456665, Elapsed time for epoch : 0.9436745246251425
Epoch 0, Batch 90, train loss:1.556204915046692, Elapsed time for epoch : 1.0601629694302876
Epoch 0, Batch 100, train loss:1.5877162218093872, Elapsed time for epoch : 1.1763904134432475
Epoch 0, Batch 110, train loss:1.4271976947784424, Elapsed time for epoch : 1.2929235458374024
Batch 0, val loss:3.685748338699341
Batch 10, val loss:6.096495628356934
Batch 20, val loss:3.5271031856536865
Batch 30, val loss:5.060764312744141
Epoch 0, Train Loss:2.8872333485147226, Val loss:3.1920104093021817
Epoch 1, Batch 0, train loss:1.5020750761032104, Elapsed time for epoch : 0.011660484472910564
Epoch 1, Batch 10, train loss:1.396649956703186, Elapsed time for epoch : 0.12772895495096842
Epoch 1, Batch 20, train loss:1.3024944067001343, Elapsed time for epoch : 0.24479057788848876
Epoch 1, Batch 30, train loss:1.4660276174545288, Elapsed time for epoch : 0.3614261666933695
Epoch 1, Batch 40, train loss:1.308589220046997, Elapsed time for epoch : 0.4783869465192159
Epoch 1, Batch 50, train loss:1.3007009029388428, Elapsed time for epoch : 0.5953394293785095
Epoch 1, Batch 60, train loss:1.328384518623352, Elapsed time for epoch : 0.7116730292638143
Epoch 1, Batch 70, train loss:1.1971256732940674, Elapsed time for epoch : 0.8280749758084615
Epoch 1, Batch 80, train loss:1.1400244235992432, Elapsed time for epoch : 0.9448320031166076
Epoch 1, Batch 90, train loss:1.2270210981369019, Elapsed time for epoch : 1.0611004749933879
Epoch 1, Batch 100, train loss:1.0267614126205444, Elapsed time for epoch : 1.1777695536613464
Epoch 1, Batch 110, train loss:1.0085093975067139, Elapsed time for epoch : 1.2941664536794026
Batch 0, val loss:2.18146014213562
Batch 10, val loss:4.5032639503479
Batch 20, val loss:2.5654921531677246
Batch 30, val loss:2.3982980251312256
Epoch 1, Train Loss:1.2647468136704487, Val loss:6.293603791130914
Epoch 2, Batch 0, train loss:1.0662493705749512, Elapsed time for epoch : 0.011658056577046712
Epoch 2, Batch 10, train loss:1.052196741104126, Elapsed time for epoch : 0.12810126145680745
Epoch 2, Batch 20, train loss:1.1099567413330078, Elapsed time for epoch : 0.24452288945515951
Epoch 2, Batch 30, train loss:1.0627894401550293, Elapsed time for epoch : 0.36090888977050783
Epoch 2, Batch 40, train loss:0.9988035559654236, Elapsed time for epoch : 0.47768733501434324
Epoch 2, Batch 50, train loss:0.9644098281860352, Elapsed time for epoch : 0.5942175110181173
Epoch 2, Batch 60, train loss:0.9823341965675354, Elapsed time for epoch : 0.7106142560640971
Epoch 2, Batch 70, train loss:0.7594824433326721, Elapsed time for epoch : 0.8272428313891093
Epoch 2, Batch 80, train loss:0.9600093960762024, Elapsed time for epoch : 0.9443170547485351
Epoch 2, Batch 90, train loss:0.9570473432540894, Elapsed time for epoch : 1.0605607310930887
Epoch 2, Batch 100, train loss:0.9648347496986389, Elapsed time for epoch : 1.177410646279653
Epoch 2, Batch 110, train loss:0.8587432503700256, Elapsed time for epoch : 1.293919595082601
Batch 0, val loss:8.383828163146973
Batch 10, val loss:1.770790457725525
Batch 20, val loss:1.921714186668396
Batch 30, val loss:17.910846710205078
Epoch 2, Train Loss:0.9738578894863958, Val loss:6.372041606240803
Epoch 3, Batch 0, train loss:0.9120631217956543, Elapsed time for epoch : 0.011657941341400146
Epoch 3, Batch 10, train loss:0.9519385099411011, Elapsed time for epoch : 0.12790517012278238
Epoch 3, Batch 20, train loss:0.955115795135498, Elapsed time for epoch : 0.24440588156382242
Epoch 3, Batch 30, train loss:0.8407045006752014, Elapsed time for epoch : 0.3608408808708191
Epoch 3, Batch 40, train loss:0.9371300935745239, Elapsed time for epoch : 0.47801872889200847
Epoch 3, Batch 50, train loss:0.5565146803855896, Elapsed time for epoch : 0.594290550549825
Epoch 3, Batch 60, train loss:0.8375134468078613, Elapsed time for epoch : 0.7107282797495524
Epoch 3, Batch 70, train loss:0.928778350353241, Elapsed time for epoch : 0.8272588570912679
Epoch 3, Batch 80, train loss:0.8294297456741333, Elapsed time for epoch : 0.9436806956926982
Epoch 3, Batch 90, train loss:0.7704241871833801, Elapsed time for epoch : 1.0602880835533142
Epoch 3, Batch 100, train loss:0.5092821717262268, Elapsed time for epoch : 1.1765424092610677
Epoch 3, Batch 110, train loss:0.6362019777297974, Elapsed time for epoch : 1.2930776039759317
Batch 0, val loss:8.724692344665527
Batch 10, val loss:10.300885200500488
Batch 20, val loss:8.40817928314209
Batch 30, val loss:3.2478251457214355
Epoch 3, Train Loss:0.7956112615440203, Val loss:8.064994457695219
Epoch 4, Batch 0, train loss:0.7027440071105957, Elapsed time for epoch : 0.01159308354059855
Epoch 4, Batch 10, train loss:0.4292391836643219, Elapsed time for epoch : 0.1281095822652181
Epoch 4, Batch 20, train loss:0.47542840242385864, Elapsed time for epoch : 0.2444474736849467
Epoch 4, Batch 30, train loss:0.588658332824707, Elapsed time for epoch : 0.3605997562408447
Epoch 4, Batch 40, train loss:0.6915445923805237, Elapsed time for epoch : 0.4769794185956319
Epoch 4, Batch 50, train loss:0.6139757037162781, Elapsed time for epoch : 0.5934129039446513
Epoch 4, Batch 60, train loss:0.6490853428840637, Elapsed time for epoch : 0.7098840157190959
Epoch 4, Batch 70, train loss:0.6970778703689575, Elapsed time for epoch : 0.8263582944869995
Epoch 4, Batch 80, train loss:0.4695417881011963, Elapsed time for epoch : 0.9426565607388814
Epoch 4, Batch 90, train loss:0.5817186236381531, Elapsed time for epoch : 1.0590533415476482
Epoch 4, Batch 100, train loss:0.5298834443092346, Elapsed time for epoch : 1.1753528038660686
Epoch 4, Batch 110, train loss:0.5863550305366516, Elapsed time for epoch : 1.2916124224662782
Batch 0, val loss:1.9618512392044067
Batch 10, val loss:2.0727803707122803
Batch 20, val loss:2.1869587898254395
Batch 30, val loss:1.9915767908096313
Epoch 4, Train Loss:0.6002631920835246, Val loss:6.125668047202958
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.60026
wandb:         Val Loss 6.12567
wandb:      train_batch 110
wandb: train_batch_loss 0.58636
wandb:        val_batch 30
wandb:   val_batch_loss 1.99158
wandb: 
wandb: üöÄ View run eternal-dragon-373 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/wu1lmwjb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_041838-wu1lmwjb/logs
Seed completed execution! 89 0.7_2
------------------------------------------------------------------
Running for seed 23 of experiment 0.7_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_042626-nczedteo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-fog-375
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/nczedteo
Epoch 0, Batch 0, train loss:7.524460792541504, Elapsed time for epoch : 0.012451577186584472
Epoch 0, Batch 10, train loss:3.994114398956299, Elapsed time for epoch : 0.12832464774449667
Epoch 0, Batch 20, train loss:2.5958220958709717, Elapsed time for epoch : 0.24404415686925252
Epoch 0, Batch 30, train loss:2.4670469760894775, Elapsed time for epoch : 0.36003932158152263
Epoch 0, Batch 40, train loss:2.1660773754119873, Elapsed time for epoch : 0.4768050789833069
Epoch 0, Batch 50, train loss:2.1282804012298584, Elapsed time for epoch : 0.5928152322769165
Epoch 0, Batch 60, train loss:1.79402756690979, Elapsed time for epoch : 0.7089825868606567
Epoch 0, Batch 70, train loss:1.9038660526275635, Elapsed time for epoch : 0.8253206531206767
Epoch 0, Batch 80, train loss:1.9154592752456665, Elapsed time for epoch : 0.9418469270070394
Epoch 0, Batch 90, train loss:1.556204915046692, Elapsed time for epoch : 1.0583400050799052
Epoch 0, Batch 100, train loss:1.5877162218093872, Elapsed time for epoch : 1.1750278353691102
Epoch 0, Batch 110, train loss:1.4271976947784424, Elapsed time for epoch : 1.2913154244422913
Batch 0, val loss:3.685748338699341
Batch 10, val loss:6.096495628356934
Batch 20, val loss:3.5271031856536865
Batch 30, val loss:5.060764312744141
Epoch 0, Train Loss:2.8872333485147226, Val loss:3.1920104093021817
Epoch 1, Batch 0, train loss:1.5020750761032104, Elapsed time for epoch : 0.011609379450480144
Epoch 1, Batch 10, train loss:1.396649956703186, Elapsed time for epoch : 0.12794007062911988
Epoch 1, Batch 20, train loss:1.3024944067001343, Elapsed time for epoch : 0.24461520910263063
Epoch 1, Batch 30, train loss:1.4660276174545288, Elapsed time for epoch : 0.36101870934168495
Epoch 1, Batch 40, train loss:1.308589220046997, Elapsed time for epoch : 0.4775779883066813
Epoch 1, Batch 50, train loss:1.3007009029388428, Elapsed time for epoch : 0.5940832138061524
Epoch 1, Batch 60, train loss:1.328384518623352, Elapsed time for epoch : 0.7103057066599529
Epoch 1, Batch 70, train loss:1.1971256732940674, Elapsed time for epoch : 0.8264458775520325
Epoch 1, Batch 80, train loss:1.1400244235992432, Elapsed time for epoch : 0.9431472023328146
Epoch 1, Batch 90, train loss:1.2270210981369019, Elapsed time for epoch : 1.0596647063891094
Epoch 1, Batch 100, train loss:1.0267614126205444, Elapsed time for epoch : 1.1761096318562825
Epoch 1, Batch 110, train loss:1.0085093975067139, Elapsed time for epoch : 1.292644206682841
Batch 0, val loss:2.18146014213562
Batch 10, val loss:4.5032639503479
Batch 20, val loss:2.5654921531677246
Batch 30, val loss:2.3982980251312256
Epoch 1, Train Loss:1.2647468136704487, Val loss:6.293603791130914
Epoch 2, Batch 0, train loss:1.0662493705749512, Elapsed time for epoch : 0.011625230312347412
Epoch 2, Batch 10, train loss:1.052196741104126, Elapsed time for epoch : 0.1285646120707194
Epoch 2, Batch 20, train loss:1.1099567413330078, Elapsed time for epoch : 0.24498643477757773
Epoch 2, Batch 30, train loss:1.0627894401550293, Elapsed time for epoch : 0.3615398128827413
Epoch 2, Batch 40, train loss:0.9988035559654236, Elapsed time for epoch : 0.47822014888127645
Epoch 2, Batch 50, train loss:0.9644098281860352, Elapsed time for epoch : 0.5948862393697103
Epoch 2, Batch 60, train loss:0.9823341965675354, Elapsed time for epoch : 0.711337931950887
Epoch 2, Batch 70, train loss:0.7594824433326721, Elapsed time for epoch : 0.8278485457102458
Epoch 2, Batch 80, train loss:0.9600093960762024, Elapsed time for epoch : 0.944899034500122
Epoch 2, Batch 90, train loss:0.9570473432540894, Elapsed time for epoch : 1.0616279045740764
Epoch 2, Batch 100, train loss:0.9648347496986389, Elapsed time for epoch : 1.178045912583669
Epoch 2, Batch 110, train loss:0.8587432503700256, Elapsed time for epoch : 1.2945510466893515
Batch 0, val loss:8.383828163146973
Batch 10, val loss:1.770790457725525
Batch 20, val loss:1.921714186668396
Batch 30, val loss:17.910846710205078
Epoch 2, Train Loss:0.9738578894863958, Val loss:6.372041606240803
Epoch 3, Batch 0, train loss:0.9120631217956543, Elapsed time for epoch : 0.011585779984792073
Epoch 3, Batch 10, train loss:0.9519385099411011, Elapsed time for epoch : 0.12800404230753581
Epoch 3, Batch 20, train loss:0.955115795135498, Elapsed time for epoch : 0.24427224000295003
Epoch 3, Batch 30, train loss:0.8407045006752014, Elapsed time for epoch : 0.36087841987609864
Epoch 3, Batch 40, train loss:0.9371300935745239, Elapsed time for epoch : 0.47739075819651283
Epoch 3, Batch 50, train loss:0.5565146803855896, Elapsed time for epoch : 0.5936514973640442
Epoch 3, Batch 60, train loss:0.8375134468078613, Elapsed time for epoch : 0.7100772221883138
Epoch 3, Batch 70, train loss:0.928778350353241, Elapsed time for epoch : 0.8263770739237467
Epoch 3, Batch 80, train loss:0.8294297456741333, Elapsed time for epoch : 0.943175717194875
Epoch 3, Batch 90, train loss:0.7704241871833801, Elapsed time for epoch : 1.0596840302149455
Epoch 3, Batch 100, train loss:0.5092821717262268, Elapsed time for epoch : 1.1757687131563823
Epoch 3, Batch 110, train loss:0.6362019777297974, Elapsed time for epoch : 1.2921165466308593
Batch 0, val loss:8.724692344665527
Batch 10, val loss:10.300885200500488
Batch 20, val loss:8.40817928314209
Batch 30, val loss:3.2478251457214355
Epoch 3, Train Loss:0.7956112615440203, Val loss:8.064994457695219
Epoch 4, Batch 0, train loss:0.7027440071105957, Elapsed time for epoch : 0.01159508228302002
Epoch 4, Batch 10, train loss:0.4292391836643219, Elapsed time for epoch : 0.12786842981974283
Epoch 4, Batch 20, train loss:0.47542840242385864, Elapsed time for epoch : 0.24440994660059612
Epoch 4, Batch 30, train loss:0.588658332824707, Elapsed time for epoch : 0.3607613444328308
Epoch 4, Batch 40, train loss:0.6915445923805237, Elapsed time for epoch : 0.4770052711168925
Epoch 4, Batch 50, train loss:0.6139757037162781, Elapsed time for epoch : 0.5939931154251099
Epoch 4, Batch 60, train loss:0.6490853428840637, Elapsed time for epoch : 0.7104016741116842
Epoch 4, Batch 70, train loss:0.6970778703689575, Elapsed time for epoch : 0.8272048274676005
Epoch 4, Batch 80, train loss:0.4695417881011963, Elapsed time for epoch : 0.944649354616801
Epoch 4, Batch 90, train loss:0.5817186236381531, Elapsed time for epoch : 1.0612443606058757
Epoch 4, Batch 100, train loss:0.5298834443092346, Elapsed time for epoch : 1.1777070124944051
Epoch 4, Batch 110, train loss:0.5863550305366516, Elapsed time for epoch : 1.2944351275761923
Batch 0, val loss:1.9618512392044067
Batch 10, val loss:2.0727803707122803
Batch 20, val loss:2.1869587898254395
Batch 30, val loss:1.9915767908096313
Epoch 4, Train Loss:0.6002631920835246, Val loss:6.125668047202958
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.60026
wandb:         Val Loss 6.12567
wandb:      train_batch 110
wandb: train_batch_loss 0.58636
wandb:        val_batch 30
wandb:   val_batch_loss 1.99158
wandb: 
wandb: üöÄ View run generous-fog-375 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/nczedteo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_042626-nczedteo/logs
Seed completed execution! 23 0.7_2
------------------------------------------------------------------
Running for seed 113 of experiment 0.7_2
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_043414-m8xhuoe1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-dew-377
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m8xhuoe1
Epoch 0, Batch 0, train loss:7.524460792541504, Elapsed time for epoch : 0.013404766718546549
Epoch 0, Batch 10, train loss:3.994114398956299, Elapsed time for epoch : 0.12986872593561807
Epoch 0, Batch 20, train loss:2.5958220958709717, Elapsed time for epoch : 0.2471130688985189
Epoch 0, Batch 30, train loss:2.4670469760894775, Elapsed time for epoch : 0.36423571904500324
Epoch 0, Batch 40, train loss:2.1660773754119873, Elapsed time for epoch : 0.4808460116386414
Epoch 0, Batch 50, train loss:2.1282804012298584, Elapsed time for epoch : 0.5972916483879089
Epoch 0, Batch 60, train loss:1.79402756690979, Elapsed time for epoch : 0.714722998936971
Epoch 0, Batch 70, train loss:1.9038660526275635, Elapsed time for epoch : 0.8321557442347208
Epoch 0, Batch 80, train loss:1.9154592752456665, Elapsed time for epoch : 0.9487552603085836
Epoch 0, Batch 90, train loss:1.556204915046692, Elapsed time for epoch : 1.0659747203191121
Epoch 0, Batch 100, train loss:1.5877162218093872, Elapsed time for epoch : 1.1836594700813294
Epoch 0, Batch 110, train loss:1.4271976947784424, Elapsed time for epoch : 1.300527528921763
Batch 0, val loss:3.685748338699341
Batch 10, val loss:6.096495628356934
Batch 20, val loss:3.5271031856536865
Batch 30, val loss:5.060764312744141
Epoch 0, Train Loss:2.8872333485147226, Val loss:3.1920104093021817
Epoch 1, Batch 0, train loss:1.5020750761032104, Elapsed time for epoch : 0.011682788530985514
Epoch 1, Batch 10, train loss:1.396649956703186, Elapsed time for epoch : 0.12772385676701864
Epoch 1, Batch 20, train loss:1.3024944067001343, Elapsed time for epoch : 0.24428682724634807
Epoch 1, Batch 30, train loss:1.4660276174545288, Elapsed time for epoch : 0.36038823127746583
Epoch 1, Batch 40, train loss:1.308589220046997, Elapsed time for epoch : 0.4766427477200826
Epoch 1, Batch 50, train loss:1.3007009029388428, Elapsed time for epoch : 0.5929997642834981
Epoch 1, Batch 60, train loss:1.328384518623352, Elapsed time for epoch : 0.7093273480733235
Epoch 1, Batch 70, train loss:1.1971256732940674, Elapsed time for epoch : 0.8255119760831197
Epoch 1, Batch 80, train loss:1.1400244235992432, Elapsed time for epoch : 0.9422774354616801
Epoch 1, Batch 90, train loss:1.2270210981369019, Elapsed time for epoch : 1.058755926291148
Epoch 1, Batch 100, train loss:1.0267614126205444, Elapsed time for epoch : 1.175218673547109
Epoch 1, Batch 110, train loss:1.0085093975067139, Elapsed time for epoch : 1.292217743396759
Batch 0, val loss:2.18146014213562
Batch 10, val loss:4.5032639503479
Batch 20, val loss:2.5654921531677246
Batch 30, val loss:2.3982980251312256
Epoch 1, Train Loss:1.2647468136704487, Val loss:6.293603791130914
Epoch 2, Batch 0, train loss:1.0662493705749512, Elapsed time for epoch : 0.011691455046335857
Epoch 2, Batch 10, train loss:1.052196741104126, Elapsed time for epoch : 0.12843697071075438
Epoch 2, Batch 20, train loss:1.1099567413330078, Elapsed time for epoch : 0.24524257183074952
Epoch 2, Batch 30, train loss:1.0627894401550293, Elapsed time for epoch : 0.36151092052459716
Epoch 2, Batch 40, train loss:0.9988035559654236, Elapsed time for epoch : 0.4780079126358032
Epoch 2, Batch 50, train loss:0.9644098281860352, Elapsed time for epoch : 0.5943732817967733
Epoch 2, Batch 60, train loss:0.9823341965675354, Elapsed time for epoch : 0.7109782934188843
Epoch 2, Batch 70, train loss:0.7594824433326721, Elapsed time for epoch : 0.8273606260617574
Epoch 2, Batch 80, train loss:0.9600093960762024, Elapsed time for epoch : 0.9440837184588114
Epoch 2, Batch 90, train loss:0.9570473432540894, Elapsed time for epoch : 1.0603522260983784
Epoch 2, Batch 100, train loss:0.9648347496986389, Elapsed time for epoch : 1.1763943195343018
Epoch 2, Batch 110, train loss:0.8587432503700256, Elapsed time for epoch : 1.2928052226702371
Batch 0, val loss:8.383828163146973
Batch 10, val loss:1.770790457725525
Batch 20, val loss:1.921714186668396
Batch 30, val loss:17.910846710205078
Epoch 2, Train Loss:0.9738578894863958, Val loss:6.372041606240803
Epoch 3, Batch 0, train loss:0.9120631217956543, Elapsed time for epoch : 0.0116096297899882
Epoch 3, Batch 10, train loss:0.9519385099411011, Elapsed time for epoch : 0.12746860980987548
Epoch 3, Batch 20, train loss:0.955115795135498, Elapsed time for epoch : 0.24393198092778523
Epoch 3, Batch 30, train loss:0.8407045006752014, Elapsed time for epoch : 0.3605008403460185
Epoch 3, Batch 40, train loss:0.9371300935745239, Elapsed time for epoch : 0.47628661394119265
Epoch 3, Batch 50, train loss:0.5565146803855896, Elapsed time for epoch : 0.5918580849965414
Epoch 3, Batch 60, train loss:0.8375134468078613, Elapsed time for epoch : 0.7077564477920533
Epoch 3, Batch 70, train loss:0.928778350353241, Elapsed time for epoch : 0.8238719185193379
Epoch 3, Batch 80, train loss:0.8294297456741333, Elapsed time for epoch : 0.9397130370140075
Epoch 3, Batch 90, train loss:0.7704241871833801, Elapsed time for epoch : 1.055698831876119
Epoch 3, Batch 100, train loss:0.5092821717262268, Elapsed time for epoch : 1.1716621160507201
Epoch 3, Batch 110, train loss:0.6362019777297974, Elapsed time for epoch : 1.2877014756202698
Batch 0, val loss:8.724692344665527
Batch 10, val loss:10.300885200500488
Batch 20, val loss:8.40817928314209
Batch 30, val loss:3.2478251457214355
Epoch 3, Train Loss:0.7956112615440203, Val loss:8.064994457695219
Epoch 4, Batch 0, train loss:0.7027440071105957, Elapsed time for epoch : 0.01161892811457316
Epoch 4, Batch 10, train loss:0.4292391836643219, Elapsed time for epoch : 0.1276563286781311
Epoch 4, Batch 20, train loss:0.47542840242385864, Elapsed time for epoch : 0.2442631681760152
Epoch 4, Batch 30, train loss:0.588658332824707, Elapsed time for epoch : 0.3610307494799296
Epoch 4, Batch 40, train loss:0.6915445923805237, Elapsed time for epoch : 0.47739676634470624
Epoch 4, Batch 50, train loss:0.6139757037162781, Elapsed time for epoch : 0.5939430395762125
Epoch 4, Batch 60, train loss:0.6490853428840637, Elapsed time for epoch : 0.710083270072937
Epoch 4, Batch 70, train loss:0.6970778703689575, Elapsed time for epoch : 0.8261871337890625
Epoch 4, Batch 80, train loss:0.4695417881011963, Elapsed time for epoch : 0.9427908261617025
Epoch 4, Batch 90, train loss:0.5817186236381531, Elapsed time for epoch : 1.0588246266047159
Epoch 4, Batch 100, train loss:0.5298834443092346, Elapsed time for epoch : 1.1754672567049662
Epoch 4, Batch 110, train loss:0.5863550305366516, Elapsed time for epoch : 1.2917125225067139
Batch 0, val loss:1.9618512392044067
Batch 10, val loss:2.0727803707122803
Batch 20, val loss:2.1869587898254395
Batch 30, val loss:1.9915767908096313
Epoch 4, Train Loss:0.6002631920835246, Val loss:6.125668047202958
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÖ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.60026
wandb:         Val Loss 6.12567
wandb:      train_batch 110
wandb: train_batch_loss 0.58636
wandb:        val_batch 30
wandb:   val_batch_loss 1.99158
wandb: 
wandb: üöÄ View run genial-dew-377 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/m8xhuoe1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_043414-m8xhuoe1/logs
Seed completed execution! 113 0.7_2
------------------------------------------------------------------
Experiment complete 0.7_2
==========================================================================
Running experiment for setting 0.7_3
==========================================================================
Running for seed 1 of experiment 0.7_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_044202-gq5q2epz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-cloud-379
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gq5q2epz
Epoch 0, Batch 0, train loss:7.8952436447143555, Elapsed time for epoch : 0.013843274116516114
Epoch 0, Batch 10, train loss:3.9263250827789307, Elapsed time for epoch : 0.13779260714848837
Epoch 0, Batch 20, train loss:3.6967179775238037, Elapsed time for epoch : 0.2602879087130229
Epoch 0, Batch 30, train loss:3.4420559406280518, Elapsed time for epoch : 0.3828664938608805
Epoch 0, Batch 40, train loss:2.9840335845947266, Elapsed time for epoch : 0.505729095141093
Epoch 0, Batch 50, train loss:2.622028350830078, Elapsed time for epoch : 0.6292817632357279
Epoch 0, Batch 60, train loss:2.4967143535614014, Elapsed time for epoch : 0.7535811026891073
Epoch 0, Batch 70, train loss:2.694042205810547, Elapsed time for epoch : 0.8780496120452881
Epoch 0, Batch 80, train loss:2.198442220687866, Elapsed time for epoch : 1.0037134369214376
Epoch 0, Batch 90, train loss:2.48689603805542, Elapsed time for epoch : 1.1268487373987834
Epoch 0, Batch 100, train loss:2.046370267868042, Elapsed time for epoch : 1.251507051785787
Epoch 0, Batch 110, train loss:1.73784339427948, Elapsed time for epoch : 1.3753325899442037
Batch 0, val loss:4.156585216522217
Batch 10, val loss:5.654478549957275
Batch 20, val loss:3.6912589073181152
Batch 30, val loss:4.685665607452393
Epoch 0, Train Loss:3.067605363804361, Val loss:3.893806152873569
Epoch 1, Batch 0, train loss:1.783852458000183, Elapsed time for epoch : 0.0115881085395813
Epoch 1, Batch 10, train loss:1.7523176670074463, Elapsed time for epoch : 0.1277894337972005
Epoch 1, Batch 20, train loss:1.3961492776870728, Elapsed time for epoch : 0.2448001980781555
Epoch 1, Batch 30, train loss:1.373896837234497, Elapsed time for epoch : 0.36111770470937093
Epoch 1, Batch 40, train loss:1.4318839311599731, Elapsed time for epoch : 0.4780501206715902
Epoch 1, Batch 50, train loss:1.455545425415039, Elapsed time for epoch : 0.5945809046427409
Epoch 1, Batch 60, train loss:1.3017473220825195, Elapsed time for epoch : 0.7107246955235799
Epoch 1, Batch 70, train loss:1.311164140701294, Elapsed time for epoch : 0.8271775881449381
Epoch 1, Batch 80, train loss:1.2466152906417847, Elapsed time for epoch : 0.9438355088233947
Epoch 1, Batch 90, train loss:1.0925954580307007, Elapsed time for epoch : 1.0601839502652486
Epoch 1, Batch 100, train loss:0.965423583984375, Elapsed time for epoch : 1.1769089063008626
Epoch 1, Batch 110, train loss:1.0176233053207397, Elapsed time for epoch : 1.2936306715011596
Batch 0, val loss:2.1919867992401123
Batch 10, val loss:3.119342565536499
Batch 20, val loss:3.675529956817627
Batch 30, val loss:2.8238563537597656
Epoch 1, Train Loss:1.3463587771291319, Val loss:6.777335716618432
Epoch 2, Batch 0, train loss:0.9026598930358887, Elapsed time for epoch : 0.011574339866638184
Epoch 2, Batch 10, train loss:0.973571240901947, Elapsed time for epoch : 0.1276612083117167
Epoch 2, Batch 20, train loss:0.9193786382675171, Elapsed time for epoch : 0.24370580514272053
Epoch 2, Batch 30, train loss:0.8352727890014648, Elapsed time for epoch : 0.3597438931465149
Epoch 2, Batch 40, train loss:0.8714846968650818, Elapsed time for epoch : 0.4757941881815592
Epoch 2, Batch 50, train loss:0.9008646011352539, Elapsed time for epoch : 0.5921048998832703
Epoch 2, Batch 60, train loss:0.8725008368492126, Elapsed time for epoch : 0.7082300424575806
Epoch 2, Batch 70, train loss:0.6894030570983887, Elapsed time for epoch : 0.8244551380475362
Epoch 2, Batch 80, train loss:0.8782601952552795, Elapsed time for epoch : 0.9410855730374654
Epoch 2, Batch 90, train loss:0.8825315237045288, Elapsed time for epoch : 1.0569906036059062
Epoch 2, Batch 100, train loss:0.8338719606399536, Elapsed time for epoch : 1.1733614126841228
Epoch 2, Batch 110, train loss:0.7598673701286316, Elapsed time for epoch : 1.2897080540657044
Batch 0, val loss:10.376310348510742
Batch 10, val loss:2.924535036087036
Batch 20, val loss:2.742116689682007
Batch 30, val loss:18.492023468017578
Epoch 2, Train Loss:0.8685092501018359, Val loss:7.7309339344501495
Epoch 3, Batch 0, train loss:0.7920657396316528, Elapsed time for epoch : 0.011603875954945882
Epoch 3, Batch 10, train loss:0.7808602452278137, Elapsed time for epoch : 0.12792288462320964
Epoch 3, Batch 20, train loss:0.7656532526016235, Elapsed time for epoch : 0.24414961735407512
Epoch 3, Batch 30, train loss:0.737415611743927, Elapsed time for epoch : 0.36068058808644615
Epoch 3, Batch 40, train loss:0.7167195081710815, Elapsed time for epoch : 0.4767475605010986
Epoch 3, Batch 50, train loss:0.34332939982414246, Elapsed time for epoch : 0.5929097215334574
Epoch 3, Batch 60, train loss:0.6796959638595581, Elapsed time for epoch : 0.7093901872634888
Epoch 3, Batch 70, train loss:0.6449976563453674, Elapsed time for epoch : 0.8259057998657227
Epoch 3, Batch 80, train loss:0.7229905724525452, Elapsed time for epoch : 0.942551334698995
Epoch 3, Batch 90, train loss:0.5492269992828369, Elapsed time for epoch : 1.0590675433476766
Epoch 3, Batch 100, train loss:0.20707380771636963, Elapsed time for epoch : 1.1754327654838561
Epoch 3, Batch 110, train loss:0.5109580159187317, Elapsed time for epoch : 1.2929425676663717
Batch 0, val loss:21.94540786743164
Batch 10, val loss:12.617593765258789
Batch 20, val loss:14.59040355682373
Batch 30, val loss:2.9185240268707275
Epoch 3, Train Loss:0.6102842342594396, Val loss:13.656599352757135
Epoch 4, Batch 0, train loss:0.7086611390113831, Elapsed time for epoch : 0.011793025334676107
Epoch 4, Batch 10, train loss:0.22196707129478455, Elapsed time for epoch : 0.1280018210411072
Epoch 4, Batch 20, train loss:0.1790456771850586, Elapsed time for epoch : 0.24459793170293173
Epoch 4, Batch 30, train loss:0.43903034925460815, Elapsed time for epoch : 0.3610143264134725
Epoch 4, Batch 40, train loss:0.46741339564323425, Elapsed time for epoch : 0.47714316844940186
Epoch 4, Batch 50, train loss:0.4534810483455658, Elapsed time for epoch : 0.5939027309417725
Epoch 4, Batch 60, train loss:0.38337111473083496, Elapsed time for epoch : 0.7106029709180196
Epoch 4, Batch 70, train loss:0.4598627984523773, Elapsed time for epoch : 0.8272540887196859
Epoch 4, Batch 80, train loss:0.20836691558361053, Elapsed time for epoch : 0.9437182863553365
Epoch 4, Batch 90, train loss:0.38467445969581604, Elapsed time for epoch : 1.0604900161425272
Epoch 4, Batch 100, train loss:0.3512543737888336, Elapsed time for epoch : 1.176682714621226
Epoch 4, Batch 110, train loss:0.42817753553390503, Elapsed time for epoch : 1.2930567781130473
Batch 0, val loss:12.434854507446289
Batch 10, val loss:3.769225835800171
Batch 20, val loss:3.569978952407837
Batch 30, val loss:3.2288289070129395
Epoch 4, Train Loss:0.3797388527704322, Val loss:8.541857731011179
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.37974
wandb:         Val Loss 8.54186
wandb:      train_batch 110
wandb: train_batch_loss 0.42818
wandb:        val_batch 30
wandb:   val_batch_loss 3.22883
wandb: 
wandb: üöÄ View run stoic-cloud-379 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/gq5q2epz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_044202-gq5q2epz/logs
Seed completed execution! 1 0.7_3
------------------------------------------------------------------
Running for seed 42 of experiment 0.7_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_044957-42iiu42m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-gorge-381
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/42iiu42m
Epoch 0, Batch 0, train loss:7.8952436447143555, Elapsed time for epoch : 0.01325457493464152
Epoch 0, Batch 10, train loss:3.9263250827789307, Elapsed time for epoch : 0.12905303239822388
Epoch 0, Batch 20, train loss:3.6967179775238037, Elapsed time for epoch : 0.2451777458190918
Epoch 0, Batch 30, train loss:3.4420559406280518, Elapsed time for epoch : 0.3613968014717102
Epoch 0, Batch 40, train loss:2.9840335845947266, Elapsed time for epoch : 0.47852106889088947
Epoch 0, Batch 50, train loss:2.622028350830078, Elapsed time for epoch : 0.5953988552093505
Epoch 0, Batch 60, train loss:2.4967143535614014, Elapsed time for epoch : 0.7117516915003459
Epoch 0, Batch 70, train loss:2.694042205810547, Elapsed time for epoch : 0.8283305207888285
Epoch 0, Batch 80, train loss:2.198442220687866, Elapsed time for epoch : 0.944827659924825
Epoch 0, Batch 90, train loss:2.48689603805542, Elapsed time for epoch : 1.0611036578814188
Epoch 0, Batch 100, train loss:2.046370267868042, Elapsed time for epoch : 1.1774041414260865
Epoch 0, Batch 110, train loss:1.73784339427948, Elapsed time for epoch : 1.2941648960113525
Batch 0, val loss:4.156585216522217
Batch 10, val loss:5.654478549957275
Batch 20, val loss:3.6912589073181152
Batch 30, val loss:4.685665607452393
Epoch 0, Train Loss:3.067605363804361, Val loss:3.893806152873569
Epoch 1, Batch 0, train loss:1.783852458000183, Elapsed time for epoch : 0.011604380607604981
Epoch 1, Batch 10, train loss:1.7523176670074463, Elapsed time for epoch : 0.1277798811594645
Epoch 1, Batch 20, train loss:1.3961492776870728, Elapsed time for epoch : 0.24455802043279012
Epoch 1, Batch 30, train loss:1.373896837234497, Elapsed time for epoch : 0.36147921085357665
Epoch 1, Batch 40, train loss:1.4318839311599731, Elapsed time for epoch : 0.4779937068621318
Epoch 1, Batch 50, train loss:1.455545425415039, Elapsed time for epoch : 0.5946100513140361
Epoch 1, Batch 60, train loss:1.3017473220825195, Elapsed time for epoch : 0.7115119179089864
Epoch 1, Batch 70, train loss:1.311164140701294, Elapsed time for epoch : 0.8279848456382751
Epoch 1, Batch 80, train loss:1.2466152906417847, Elapsed time for epoch : 0.9451545794804891
Epoch 1, Batch 90, train loss:1.0925954580307007, Elapsed time for epoch : 1.062340760231018
Epoch 1, Batch 100, train loss:0.965423583984375, Elapsed time for epoch : 1.1788838426272075
Epoch 1, Batch 110, train loss:1.0176233053207397, Elapsed time for epoch : 1.2950981338818868
Batch 0, val loss:2.1919867992401123
Batch 10, val loss:3.119342565536499
Batch 20, val loss:3.675529956817627
Batch 30, val loss:2.8238563537597656
Epoch 1, Train Loss:1.3463587771291319, Val loss:6.777335716618432
Epoch 2, Batch 0, train loss:0.9026598930358887, Elapsed time for epoch : 0.011632100741068522
Epoch 2, Batch 10, train loss:0.973571240901947, Elapsed time for epoch : 0.128145170211792
Epoch 2, Batch 20, train loss:0.9193786382675171, Elapsed time for epoch : 0.24444190661112467
Epoch 2, Batch 30, train loss:0.8352727890014648, Elapsed time for epoch : 0.36108212868372597
Epoch 2, Batch 40, train loss:0.8714846968650818, Elapsed time for epoch : 0.4774535497029622
Epoch 2, Batch 50, train loss:0.9008646011352539, Elapsed time for epoch : 0.5937480807304383
Epoch 2, Batch 60, train loss:0.8725008368492126, Elapsed time for epoch : 0.7102066953976949
Epoch 2, Batch 70, train loss:0.6894030570983887, Elapsed time for epoch : 0.8259531537691752
Epoch 2, Batch 80, train loss:0.8782601952552795, Elapsed time for epoch : 0.942547098795573
Epoch 2, Batch 90, train loss:0.8825315237045288, Elapsed time for epoch : 1.0591973821322123
Epoch 2, Batch 100, train loss:0.8338719606399536, Elapsed time for epoch : 1.1761047840118408
Epoch 2, Batch 110, train loss:0.7598673701286316, Elapsed time for epoch : 1.2927047491073609
Batch 0, val loss:10.376310348510742
Batch 10, val loss:2.924535036087036
Batch 20, val loss:2.742116689682007
Batch 30, val loss:18.492023468017578
Epoch 2, Train Loss:0.8685092501018359, Val loss:7.7309339344501495
Epoch 3, Batch 0, train loss:0.7920657396316528, Elapsed time for epoch : 0.01179255247116089
Epoch 3, Batch 10, train loss:0.7808602452278137, Elapsed time for epoch : 0.12804994583129883
Epoch 3, Batch 20, train loss:0.7656532526016235, Elapsed time for epoch : 0.24499640067418416
Epoch 3, Batch 30, train loss:0.737415611743927, Elapsed time for epoch : 0.3611594001452128
Epoch 3, Batch 40, train loss:0.7167195081710815, Elapsed time for epoch : 0.4776349385579427
Epoch 3, Batch 50, train loss:0.34332939982414246, Elapsed time for epoch : 0.5947957237561544
Epoch 3, Batch 60, train loss:0.6796959638595581, Elapsed time for epoch : 0.7115860859553019
Epoch 3, Batch 70, train loss:0.6449976563453674, Elapsed time for epoch : 0.8280036687850952
Epoch 3, Batch 80, train loss:0.7229905724525452, Elapsed time for epoch : 0.9451099475224812
Epoch 3, Batch 90, train loss:0.5492269992828369, Elapsed time for epoch : 1.0615488688151042
Epoch 3, Batch 100, train loss:0.20707380771636963, Elapsed time for epoch : 1.178167732556661
Epoch 3, Batch 110, train loss:0.5109580159187317, Elapsed time for epoch : 1.2946015079816182
Batch 0, val loss:21.94540786743164
Batch 10, val loss:12.617593765258789
Batch 20, val loss:14.59040355682373
Batch 30, val loss:2.9185240268707275
Epoch 3, Train Loss:0.6102842342594396, Val loss:13.656599352757135
Epoch 4, Batch 0, train loss:0.7086611390113831, Elapsed time for epoch : 0.011658847332000732
Epoch 4, Batch 10, train loss:0.22196707129478455, Elapsed time for epoch : 0.12811739047368367
Epoch 4, Batch 20, train loss:0.1790456771850586, Elapsed time for epoch : 0.24476975202560425
Epoch 4, Batch 30, train loss:0.43903034925460815, Elapsed time for epoch : 0.3616130709648132
Epoch 4, Batch 40, train loss:0.46741339564323425, Elapsed time for epoch : 0.47840121189753215
Epoch 4, Batch 50, train loss:0.4534810483455658, Elapsed time for epoch : 0.5948110024134318
Epoch 4, Batch 60, train loss:0.38337111473083496, Elapsed time for epoch : 0.7113038380940755
Epoch 4, Batch 70, train loss:0.4598627984523773, Elapsed time for epoch : 0.8279136697451274
Epoch 4, Batch 80, train loss:0.20836691558361053, Elapsed time for epoch : 0.9443218827247619
Epoch 4, Batch 90, train loss:0.38467445969581604, Elapsed time for epoch : 1.0613643685976664
Epoch 4, Batch 100, train loss:0.3512543737888336, Elapsed time for epoch : 1.1775843620300293
Epoch 4, Batch 110, train loss:0.42817753553390503, Elapsed time for epoch : 1.2942505041758219
Batch 0, val loss:12.434854507446289
Batch 10, val loss:3.769225835800171
Batch 20, val loss:3.569978952407837
Batch 30, val loss:3.2288289070129395
Epoch 4, Train Loss:0.3797388527704322, Val loss:8.541857731011179
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.157 MB of 0.157 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.37974
wandb:         Val Loss 8.54186
wandb:      train_batch 110
wandb: train_batch_loss 0.42818
wandb:        val_batch 30
wandb:   val_batch_loss 3.22883
wandb: 
wandb: üöÄ View run fluent-gorge-381 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/42iiu42m
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_044957-42iiu42m/logs
Seed completed execution! 42 0.7_3
------------------------------------------------------------------
Running for seed 89 of experiment 0.7_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_045745-64lynck1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-dream-383
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/64lynck1
Epoch 0, Batch 0, train loss:7.8952436447143555, Elapsed time for epoch : 0.013688302040100098
Epoch 0, Batch 10, train loss:3.9263250827789307, Elapsed time for epoch : 0.12991160949071248
Epoch 0, Batch 20, train loss:3.6967179775238037, Elapsed time for epoch : 0.2457895000775655
Epoch 0, Batch 30, train loss:3.4420559406280518, Elapsed time for epoch : 0.3623956044514974
Epoch 0, Batch 40, train loss:2.9840335845947266, Elapsed time for epoch : 0.4787340879440308
Epoch 0, Batch 50, train loss:2.622028350830078, Elapsed time for epoch : 0.5951852202415466
Epoch 0, Batch 60, train loss:2.4967143535614014, Elapsed time for epoch : 0.7118129293123882
Epoch 0, Batch 70, train loss:2.694042205810547, Elapsed time for epoch : 0.8278443733851115
Epoch 0, Batch 80, train loss:2.198442220687866, Elapsed time for epoch : 0.9447339733441671
Epoch 0, Batch 90, train loss:2.48689603805542, Elapsed time for epoch : 1.061168666680654
Epoch 0, Batch 100, train loss:2.046370267868042, Elapsed time for epoch : 1.1776699900627137
Epoch 0, Batch 110, train loss:1.73784339427948, Elapsed time for epoch : 1.294284737110138
Batch 0, val loss:4.156585216522217
Batch 10, val loss:5.654478549957275
Batch 20, val loss:3.6912589073181152
Batch 30, val loss:4.685665607452393
Epoch 0, Train Loss:3.067605363804361, Val loss:3.893806152873569
Epoch 1, Batch 0, train loss:1.783852458000183, Elapsed time for epoch : 0.011619369188944498
Epoch 1, Batch 10, train loss:1.7523176670074463, Elapsed time for epoch : 0.12791345914204916
Epoch 1, Batch 20, train loss:1.3961492776870728, Elapsed time for epoch : 0.2448666532834371
Epoch 1, Batch 30, train loss:1.373896837234497, Elapsed time for epoch : 0.3613060712814331
Epoch 1, Batch 40, train loss:1.4318839311599731, Elapsed time for epoch : 0.477951443195343
Epoch 1, Batch 50, train loss:1.455545425415039, Elapsed time for epoch : 0.5945903897285462
Epoch 1, Batch 60, train loss:1.3017473220825195, Elapsed time for epoch : 0.7110728661219279
Epoch 1, Batch 70, train loss:1.311164140701294, Elapsed time for epoch : 0.8275413552920023
Epoch 1, Batch 80, train loss:1.2466152906417847, Elapsed time for epoch : 0.9446430643399556
Epoch 1, Batch 90, train loss:1.0925954580307007, Elapsed time for epoch : 1.0614068349202475
Epoch 1, Batch 100, train loss:0.965423583984375, Elapsed time for epoch : 1.1781627178192138
Epoch 1, Batch 110, train loss:1.0176233053207397, Elapsed time for epoch : 1.2948889931042988
Batch 0, val loss:2.1919867992401123
Batch 10, val loss:3.119342565536499
Batch 20, val loss:3.675529956817627
Batch 30, val loss:2.8238563537597656
Epoch 1, Train Loss:1.3463587771291319, Val loss:6.777335716618432
Epoch 2, Batch 0, train loss:0.9026598930358887, Elapsed time for epoch : 0.01164017915725708
Epoch 2, Batch 10, train loss:0.973571240901947, Elapsed time for epoch : 0.12807146310806275
Epoch 2, Batch 20, train loss:0.9193786382675171, Elapsed time for epoch : 0.24451423486073812
Epoch 2, Batch 30, train loss:0.8352727890014648, Elapsed time for epoch : 0.3611095507939657
Epoch 2, Batch 40, train loss:0.8714846968650818, Elapsed time for epoch : 0.4778708497683207
Epoch 2, Batch 50, train loss:0.9008646011352539, Elapsed time for epoch : 0.5947760224342347
Epoch 2, Batch 60, train loss:0.8725008368492126, Elapsed time for epoch : 0.711958400408427
Epoch 2, Batch 70, train loss:0.6894030570983887, Elapsed time for epoch : 0.8284960786501566
Epoch 2, Batch 80, train loss:0.8782601952552795, Elapsed time for epoch : 0.9451643625895182
Epoch 2, Batch 90, train loss:0.8825315237045288, Elapsed time for epoch : 1.0620458324750264
Epoch 2, Batch 100, train loss:0.8338719606399536, Elapsed time for epoch : 1.1789050380388895
Epoch 2, Batch 110, train loss:0.7598673701286316, Elapsed time for epoch : 1.2957985281944275
Batch 0, val loss:10.376310348510742
Batch 10, val loss:2.924535036087036
Batch 20, val loss:2.742116689682007
Batch 30, val loss:18.492023468017578
Epoch 2, Train Loss:0.8685092501018359, Val loss:7.7309339344501495
Epoch 3, Batch 0, train loss:0.7920657396316528, Elapsed time for epoch : 0.011651957035064697
Epoch 3, Batch 10, train loss:0.7808602452278137, Elapsed time for epoch : 0.1281401475270589
Epoch 3, Batch 20, train loss:0.7656532526016235, Elapsed time for epoch : 0.24480241934458416
Epoch 3, Batch 30, train loss:0.737415611743927, Elapsed time for epoch : 0.3610707998275757
Epoch 3, Batch 40, train loss:0.7167195081710815, Elapsed time for epoch : 0.47762266000111897
Epoch 3, Batch 50, train loss:0.34332939982414246, Elapsed time for epoch : 0.5942861557006835
Epoch 3, Batch 60, train loss:0.6796959638595581, Elapsed time for epoch : 0.7107154568036397
Epoch 3, Batch 70, train loss:0.6449976563453674, Elapsed time for epoch : 0.8272708853085836
Epoch 3, Batch 80, train loss:0.7229905724525452, Elapsed time for epoch : 0.9437200029691061
Epoch 3, Batch 90, train loss:0.5492269992828369, Elapsed time for epoch : 1.0601284861564637
Epoch 3, Batch 100, train loss:0.20707380771636963, Elapsed time for epoch : 1.1769278605779012
Epoch 3, Batch 110, train loss:0.5109580159187317, Elapsed time for epoch : 1.2938647786776225
Batch 0, val loss:21.94540786743164
Batch 10, val loss:12.617593765258789
Batch 20, val loss:14.59040355682373
Batch 30, val loss:2.9185240268707275
Epoch 3, Train Loss:0.6102842342594396, Val loss:13.656599352757135
Epoch 4, Batch 0, train loss:0.7086611390113831, Elapsed time for epoch : 0.011581949392954509
Epoch 4, Batch 10, train loss:0.22196707129478455, Elapsed time for epoch : 0.12815709114074708
Epoch 4, Batch 20, train loss:0.1790456771850586, Elapsed time for epoch : 0.24480509757995605
Epoch 4, Batch 30, train loss:0.43903034925460815, Elapsed time for epoch : 0.36143949429194133
Epoch 4, Batch 40, train loss:0.46741339564323425, Elapsed time for epoch : 0.4780229131380717
Epoch 4, Batch 50, train loss:0.4534810483455658, Elapsed time for epoch : 0.5945629835128784
Epoch 4, Batch 60, train loss:0.38337111473083496, Elapsed time for epoch : 0.7110704978307089
Epoch 4, Batch 70, train loss:0.4598627984523773, Elapsed time for epoch : 0.827521546681722
Epoch 4, Batch 80, train loss:0.20836691558361053, Elapsed time for epoch : 0.944555922349294
Epoch 4, Batch 90, train loss:0.38467445969581604, Elapsed time for epoch : 1.0616930643717448
Epoch 4, Batch 100, train loss:0.3512543737888336, Elapsed time for epoch : 1.1786621888478597
Epoch 4, Batch 110, train loss:0.42817753553390503, Elapsed time for epoch : 1.2952732563018798
Batch 0, val loss:12.434854507446289
Batch 10, val loss:3.769225835800171
Batch 20, val loss:3.569978952407837
Batch 30, val loss:3.2288289070129395
Epoch 4, Train Loss:0.3797388527704322, Val loss:8.541857731011179
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.37974
wandb:         Val Loss 8.54186
wandb:      train_batch 110
wandb: train_batch_loss 0.42818
wandb:        val_batch 30
wandb:   val_batch_loss 3.22883
wandb: 
wandb: üöÄ View run lilac-dream-383 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/64lynck1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_045745-64lynck1/logs
Seed completed execution! 89 0.7_3
------------------------------------------------------------------
Running for seed 23 of experiment 0.7_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_050533-zbs5p8zy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run efficient-cherry-385
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/zbs5p8zy
Epoch 0, Batch 0, train loss:7.8952436447143555, Elapsed time for epoch : 0.013148335615793864
Epoch 0, Batch 10, train loss:3.9263250827789307, Elapsed time for epoch : 0.1292492389678955
Epoch 0, Batch 20, train loss:3.6967179775238037, Elapsed time for epoch : 0.24534235000610352
Epoch 0, Batch 30, train loss:3.4420559406280518, Elapsed time for epoch : 0.3617976665496826
Epoch 0, Batch 40, train loss:2.9840335845947266, Elapsed time for epoch : 0.4788801193237305
Epoch 0, Batch 50, train loss:2.622028350830078, Elapsed time for epoch : 0.5953574339548747
Epoch 0, Batch 60, train loss:2.4967143535614014, Elapsed time for epoch : 0.7117017070452373
Epoch 0, Batch 70, train loss:2.694042205810547, Elapsed time for epoch : 0.8282984534899394
Epoch 0, Batch 80, train loss:2.198442220687866, Elapsed time for epoch : 0.9447537978490194
Epoch 0, Batch 90, train loss:2.48689603805542, Elapsed time for epoch : 1.060817539691925
Epoch 0, Batch 100, train loss:2.046370267868042, Elapsed time for epoch : 1.1773877104123434
Epoch 0, Batch 110, train loss:1.73784339427948, Elapsed time for epoch : 1.2938965717951456
Batch 0, val loss:4.156585216522217
Batch 10, val loss:5.654478549957275
Batch 20, val loss:3.6912589073181152
Batch 30, val loss:4.685665607452393
Epoch 0, Train Loss:3.067605363804361, Val loss:3.893806152873569
Epoch 1, Batch 0, train loss:1.783852458000183, Elapsed time for epoch : 0.01168979008992513
Epoch 1, Batch 10, train loss:1.7523176670074463, Elapsed time for epoch : 0.12824278275171916
Epoch 1, Batch 20, train loss:1.3961492776870728, Elapsed time for epoch : 0.24479453961054484
Epoch 1, Batch 30, train loss:1.373896837234497, Elapsed time for epoch : 0.3613893230756124
Epoch 1, Batch 40, train loss:1.4318839311599731, Elapsed time for epoch : 0.47777820030848184
Epoch 1, Batch 50, train loss:1.455545425415039, Elapsed time for epoch : 0.5943277676900228
Epoch 1, Batch 60, train loss:1.3017473220825195, Elapsed time for epoch : 0.7109257141749065
Epoch 1, Batch 70, train loss:1.311164140701294, Elapsed time for epoch : 0.827964703241984
Epoch 1, Batch 80, train loss:1.2466152906417847, Elapsed time for epoch : 0.9442860802014669
Epoch 1, Batch 90, train loss:1.0925954580307007, Elapsed time for epoch : 1.0605032006899515
Epoch 1, Batch 100, train loss:0.965423583984375, Elapsed time for epoch : 1.1772156675656638
Epoch 1, Batch 110, train loss:1.0176233053207397, Elapsed time for epoch : 1.2939259608586628
Batch 0, val loss:2.1919867992401123
Batch 10, val loss:3.119342565536499
Batch 20, val loss:3.675529956817627
Batch 30, val loss:2.8238563537597656
Epoch 1, Train Loss:1.3463587771291319, Val loss:6.777335716618432
Epoch 2, Batch 0, train loss:0.9026598930358887, Elapsed time for epoch : 0.01159125566482544
Epoch 2, Batch 10, train loss:0.973571240901947, Elapsed time for epoch : 0.12841912110646567
Epoch 2, Batch 20, train loss:0.9193786382675171, Elapsed time for epoch : 0.2446193019549052
Epoch 2, Batch 30, train loss:0.8352727890014648, Elapsed time for epoch : 0.36136866013209024
Epoch 2, Batch 40, train loss:0.8714846968650818, Elapsed time for epoch : 0.47769398291905724
Epoch 2, Batch 50, train loss:0.9008646011352539, Elapsed time for epoch : 0.5942490259806316
Epoch 2, Batch 60, train loss:0.8725008368492126, Elapsed time for epoch : 0.7108683705329895
Epoch 2, Batch 70, train loss:0.6894030570983887, Elapsed time for epoch : 0.8276663621266683
Epoch 2, Batch 80, train loss:0.8782601952552795, Elapsed time for epoch : 0.9442372600237529
Epoch 2, Batch 90, train loss:0.8825315237045288, Elapsed time for epoch : 1.0607001701990764
Epoch 2, Batch 100, train loss:0.8338719606399536, Elapsed time for epoch : 1.1774151921272278
Epoch 2, Batch 110, train loss:0.7598673701286316, Elapsed time for epoch : 1.2939226428667705
Batch 0, val loss:10.376310348510742
Batch 10, val loss:2.924535036087036
Batch 20, val loss:2.742116689682007
Batch 30, val loss:18.492023468017578
Epoch 2, Train Loss:0.8685092501018359, Val loss:7.7309339344501495
Epoch 3, Batch 0, train loss:0.7920657396316528, Elapsed time for epoch : 0.011624717712402343
Epoch 3, Batch 10, train loss:0.7808602452278137, Elapsed time for epoch : 0.12820911407470703
Epoch 3, Batch 20, train loss:0.7656532526016235, Elapsed time for epoch : 0.2448201338450114
Epoch 3, Batch 30, train loss:0.737415611743927, Elapsed time for epoch : 0.3610479394594828
Epoch 3, Batch 40, train loss:0.7167195081710815, Elapsed time for epoch : 0.477342414855957
Epoch 3, Batch 50, train loss:0.34332939982414246, Elapsed time for epoch : 0.5939032793045044
Epoch 3, Batch 60, train loss:0.6796959638595581, Elapsed time for epoch : 0.7106510480244954
Epoch 3, Batch 70, train loss:0.6449976563453674, Elapsed time for epoch : 0.8271084427833557
Epoch 3, Batch 80, train loss:0.7229905724525452, Elapsed time for epoch : 0.9435213724772136
Epoch 3, Batch 90, train loss:0.5492269992828369, Elapsed time for epoch : 1.0601643363634745
Epoch 3, Batch 100, train loss:0.20707380771636963, Elapsed time for epoch : 1.1767961740493775
Epoch 3, Batch 110, train loss:0.5109580159187317, Elapsed time for epoch : 1.2934695601463317
Batch 0, val loss:21.94540786743164
Batch 10, val loss:12.617593765258789
Batch 20, val loss:14.59040355682373
Batch 30, val loss:2.9185240268707275
Epoch 3, Train Loss:0.6102842342594396, Val loss:13.656599352757135
Epoch 4, Batch 0, train loss:0.7086611390113831, Elapsed time for epoch : 0.011605823040008545
Epoch 4, Batch 10, train loss:0.22196707129478455, Elapsed time for epoch : 0.12807396650314332
Epoch 4, Batch 20, train loss:0.1790456771850586, Elapsed time for epoch : 0.2447308897972107
Epoch 4, Batch 30, train loss:0.43903034925460815, Elapsed time for epoch : 0.3610144337018331
Epoch 4, Batch 40, train loss:0.46741339564323425, Elapsed time for epoch : 0.4779973785082499
Epoch 4, Batch 50, train loss:0.4534810483455658, Elapsed time for epoch : 0.5946490089098613
Epoch 4, Batch 60, train loss:0.38337111473083496, Elapsed time for epoch : 0.7110692262649536
Epoch 4, Batch 70, train loss:0.4598627984523773, Elapsed time for epoch : 0.8279648264249165
Epoch 4, Batch 80, train loss:0.20836691558361053, Elapsed time for epoch : 0.9451286474863688
Epoch 4, Batch 90, train loss:0.38467445969581604, Elapsed time for epoch : 1.0615761995315551
Epoch 4, Batch 100, train loss:0.3512543737888336, Elapsed time for epoch : 1.1782329638799032
Epoch 4, Batch 110, train loss:0.42817753553390503, Elapsed time for epoch : 1.2950655142466228
Batch 0, val loss:12.434854507446289
Batch 10, val loss:3.769225835800171
Batch 20, val loss:3.569978952407837
Batch 30, val loss:3.2288289070129395
Epoch 4, Train Loss:0.3797388527704322, Val loss:8.541857731011179
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.37974
wandb:         Val Loss 8.54186
wandb:      train_batch 110
wandb: train_batch_loss 0.42818
wandb:        val_batch 30
wandb:   val_batch_loss 3.22883
wandb: 
wandb: üöÄ View run efficient-cherry-385 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/zbs5p8zy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_050533-zbs5p8zy/logs
Seed completed execution! 23 0.7_3
------------------------------------------------------------------
Running for seed 113 of experiment 0.7_3
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_051320-9bf8k0vj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run splendid-thunder-387
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9bf8k0vj
Epoch 0, Batch 0, train loss:7.8952436447143555, Elapsed time for epoch : 0.013031395276387532
Epoch 0, Batch 10, train loss:3.9263250827789307, Elapsed time for epoch : 0.1301406423250834
Epoch 0, Batch 20, train loss:3.6967179775238037, Elapsed time for epoch : 0.24733890295028688
Epoch 0, Batch 30, train loss:3.4420559406280518, Elapsed time for epoch : 0.36520347992579144
Epoch 0, Batch 40, train loss:2.9840335845947266, Elapsed time for epoch : 0.4819321632385254
Epoch 0, Batch 50, train loss:2.622028350830078, Elapsed time for epoch : 0.5996645092964172
Epoch 0, Batch 60, train loss:2.4967143535614014, Elapsed time for epoch : 0.7171937108039856
Epoch 0, Batch 70, train loss:2.694042205810547, Elapsed time for epoch : 0.8351362705230713
Epoch 0, Batch 80, train loss:2.198442220687866, Elapsed time for epoch : 0.9527058959007263
Epoch 0, Batch 90, train loss:2.48689603805542, Elapsed time for epoch : 1.0700944622357687
Epoch 0, Batch 100, train loss:2.046370267868042, Elapsed time for epoch : 1.1875454743703207
Epoch 0, Batch 110, train loss:1.73784339427948, Elapsed time for epoch : 1.30554309686025
Batch 0, val loss:4.156585216522217
Batch 10, val loss:5.654478549957275
Batch 20, val loss:3.6912589073181152
Batch 30, val loss:4.685665607452393
Epoch 0, Train Loss:3.067605363804361, Val loss:3.893806152873569
Epoch 1, Batch 0, train loss:1.783852458000183, Elapsed time for epoch : 0.01162955363591512
Epoch 1, Batch 10, train loss:1.7523176670074463, Elapsed time for epoch : 0.1281587322552999
Epoch 1, Batch 20, train loss:1.3961492776870728, Elapsed time for epoch : 0.24471838474273683
Epoch 1, Batch 30, train loss:1.373896837234497, Elapsed time for epoch : 0.3610832929611206
Epoch 1, Batch 40, train loss:1.4318839311599731, Elapsed time for epoch : 0.4773708740870158
Epoch 1, Batch 50, train loss:1.455545425415039, Elapsed time for epoch : 0.5940197825431823
Epoch 1, Batch 60, train loss:1.3017473220825195, Elapsed time for epoch : 0.710338314374288
Epoch 1, Batch 70, train loss:1.311164140701294, Elapsed time for epoch : 0.826839296023051
Epoch 1, Batch 80, train loss:1.2466152906417847, Elapsed time for epoch : 0.9433129310607911
Epoch 1, Batch 90, train loss:1.0925954580307007, Elapsed time for epoch : 1.0596559643745422
Epoch 1, Batch 100, train loss:0.965423583984375, Elapsed time for epoch : 1.1764623840649924
Epoch 1, Batch 110, train loss:1.0176233053207397, Elapsed time for epoch : 1.2929321567217509
Batch 0, val loss:2.1919867992401123
Batch 10, val loss:3.119342565536499
Batch 20, val loss:3.675529956817627
Batch 30, val loss:2.8238563537597656
Epoch 1, Train Loss:1.3463587771291319, Val loss:6.777335716618432
Epoch 2, Batch 0, train loss:0.9026598930358887, Elapsed time for epoch : 0.01213152805964152
Epoch 2, Batch 10, train loss:0.973571240901947, Elapsed time for epoch : 0.12836331129074097
Epoch 2, Batch 20, train loss:0.9193786382675171, Elapsed time for epoch : 0.24466476837793985
Epoch 2, Batch 30, train loss:0.8352727890014648, Elapsed time for epoch : 0.3609008510907491
Epoch 2, Batch 40, train loss:0.8714846968650818, Elapsed time for epoch : 0.4769258777300517
Epoch 2, Batch 50, train loss:0.9008646011352539, Elapsed time for epoch : 0.5930021484692891
Epoch 2, Batch 60, train loss:0.8725008368492126, Elapsed time for epoch : 0.7093382239341736
Epoch 2, Batch 70, train loss:0.6894030570983887, Elapsed time for epoch : 0.8255582849184672
Epoch 2, Batch 80, train loss:0.8782601952552795, Elapsed time for epoch : 0.9417355219523112
Epoch 2, Batch 90, train loss:0.8825315237045288, Elapsed time for epoch : 1.0580466310183207
Epoch 2, Batch 100, train loss:0.8338719606399536, Elapsed time for epoch : 1.1746063709259034
Epoch 2, Batch 110, train loss:0.7598673701286316, Elapsed time for epoch : 1.2907817085584006
Batch 0, val loss:10.376310348510742
Batch 10, val loss:2.924535036087036
Batch 20, val loss:2.742116689682007
Batch 30, val loss:18.492023468017578
Epoch 2, Train Loss:0.8685092501018359, Val loss:7.7309339344501495
Epoch 3, Batch 0, train loss:0.7920657396316528, Elapsed time for epoch : 0.011659038066864014
Epoch 3, Batch 10, train loss:0.7808602452278137, Elapsed time for epoch : 0.128156578540802
Epoch 3, Batch 20, train loss:0.7656532526016235, Elapsed time for epoch : 0.24461201429367066
Epoch 3, Batch 30, train loss:0.737415611743927, Elapsed time for epoch : 0.36076842149098715
Epoch 3, Batch 40, train loss:0.7167195081710815, Elapsed time for epoch : 0.47706759770711266
Epoch 3, Batch 50, train loss:0.34332939982414246, Elapsed time for epoch : 0.593391482035319
Epoch 3, Batch 60, train loss:0.6796959638595581, Elapsed time for epoch : 0.7100716193517049
Epoch 3, Batch 70, train loss:0.6449976563453674, Elapsed time for epoch : 0.8264453490575154
Epoch 3, Batch 80, train loss:0.7229905724525452, Elapsed time for epoch : 0.942678701877594
Epoch 3, Batch 90, train loss:0.5492269992828369, Elapsed time for epoch : 1.059023912747701
Epoch 3, Batch 100, train loss:0.20707380771636963, Elapsed time for epoch : 1.175686534245809
Epoch 3, Batch 110, train loss:0.5109580159187317, Elapsed time for epoch : 1.2919004321098329
Batch 0, val loss:21.94540786743164
Batch 10, val loss:12.617593765258789
Batch 20, val loss:14.59040355682373
Batch 30, val loss:2.9185240268707275
Epoch 3, Train Loss:0.6102842342594396, Val loss:13.656599352757135
Epoch 4, Batch 0, train loss:0.7086611390113831, Elapsed time for epoch : 0.011662729581197103
Epoch 4, Batch 10, train loss:0.22196707129478455, Elapsed time for epoch : 0.12793914874394735
Epoch 4, Batch 20, train loss:0.1790456771850586, Elapsed time for epoch : 0.24434614181518555
Epoch 4, Batch 30, train loss:0.43903034925460815, Elapsed time for epoch : 0.3604578892389933
Epoch 4, Batch 40, train loss:0.46741339564323425, Elapsed time for epoch : 0.47689584096272786
Epoch 4, Batch 50, train loss:0.4534810483455658, Elapsed time for epoch : 0.5931560238202412
Epoch 4, Batch 60, train loss:0.38337111473083496, Elapsed time for epoch : 0.7098831137021383
Epoch 4, Batch 70, train loss:0.4598627984523773, Elapsed time for epoch : 0.8264613072077434
Epoch 4, Batch 80, train loss:0.20836691558361053, Elapsed time for epoch : 0.9431693434715271
Epoch 4, Batch 90, train loss:0.38467445969581604, Elapsed time for epoch : 1.0592935721079508
Epoch 4, Batch 100, train loss:0.3512543737888336, Elapsed time for epoch : 1.1758068839708964
Epoch 4, Batch 110, train loss:0.42817753553390503, Elapsed time for epoch : 1.2921716531117757
Batch 0, val loss:12.434854507446289
Batch 10, val loss:3.769225835800171
Batch 20, val loss:3.569978952407837
Batch 30, val loss:3.2288289070129395
Epoch 4, Train Loss:0.3797388527704322, Val loss:8.541857731011179
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñÑ
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.37974
wandb:         Val Loss 8.54186
wandb:      train_batch 110
wandb: train_batch_loss 0.42818
wandb:        val_batch 30
wandb:   val_batch_loss 3.22883
wandb: 
wandb: üöÄ View run splendid-thunder-387 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9bf8k0vj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_051320-9bf8k0vj/logs
Seed completed execution! 113 0.7_3
------------------------------------------------------------------
Experiment complete 0.7_3
==========================================================================
Running experiment for setting 0.7_4
==========================================================================
Running for seed 1 of experiment 0.7_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_052108-k1i7x02u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-yogurt-389
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/k1i7x02u
Epoch 0, Batch 0, train loss:8.67747688293457, Elapsed time for epoch : 0.013589060306549073
Epoch 0, Batch 10, train loss:4.928752899169922, Elapsed time for epoch : 0.13738627831141154
Epoch 0, Batch 20, train loss:3.738034725189209, Elapsed time for epoch : 0.2608946800231934
Epoch 0, Batch 30, train loss:4.731654167175293, Elapsed time for epoch : 0.38593727350234985
Epoch 0, Batch 40, train loss:2.9866909980773926, Elapsed time for epoch : 0.5097461581230164
Epoch 0, Batch 50, train loss:2.7353641986846924, Elapsed time for epoch : 0.634305993715922
Epoch 0, Batch 60, train loss:2.6798408031463623, Elapsed time for epoch : 0.7590494155883789
Epoch 0, Batch 70, train loss:2.5314579010009766, Elapsed time for epoch : 0.8832546949386597
Epoch 0, Batch 80, train loss:2.7569782733917236, Elapsed time for epoch : 1.0087587753931682
Epoch 0, Batch 90, train loss:2.214308977127075, Elapsed time for epoch : 1.1327038645744323
Epoch 0, Batch 100, train loss:2.1548101902008057, Elapsed time for epoch : 1.2583317359288533
Epoch 0, Batch 110, train loss:1.8309334516525269, Elapsed time for epoch : 1.3828612327575684
Batch 0, val loss:3.410646438598633
Batch 10, val loss:3.098804235458374
Batch 20, val loss:2.6606194972991943
Batch 30, val loss:5.244523525238037
Epoch 0, Train Loss:3.2332028109094373, Val loss:3.5390967461797924
Epoch 1, Batch 0, train loss:2.0031578540802, Elapsed time for epoch : 0.01158223549524943
Epoch 1, Batch 10, train loss:1.7330533266067505, Elapsed time for epoch : 0.1274125099182129
Epoch 1, Batch 20, train loss:1.4836429357528687, Elapsed time for epoch : 0.24347811142603557
Epoch 1, Batch 30, train loss:1.6068178415298462, Elapsed time for epoch : 0.3596209247907003
Epoch 1, Batch 40, train loss:1.6742351055145264, Elapsed time for epoch : 0.4756780703862508
Epoch 1, Batch 50, train loss:1.5329152345657349, Elapsed time for epoch : 0.591781210899353
Epoch 1, Batch 60, train loss:1.4499298334121704, Elapsed time for epoch : 0.7081304272015889
Epoch 1, Batch 70, train loss:1.4503127336502075, Elapsed time for epoch : 0.8241053859392802
Epoch 1, Batch 80, train loss:1.3496475219726562, Elapsed time for epoch : 0.940051523844401
Epoch 1, Batch 90, train loss:1.3171006441116333, Elapsed time for epoch : 1.0558993101119996
Epoch 1, Batch 100, train loss:0.8923994302749634, Elapsed time for epoch : 1.1722095211346943
Epoch 1, Batch 110, train loss:1.0864521265029907, Elapsed time for epoch : 1.2888171950976053
Batch 0, val loss:3.6375012397766113
Batch 10, val loss:2.378173351287842
Batch 20, val loss:2.7924532890319824
Batch 30, val loss:3.334510326385498
Epoch 1, Train Loss:1.4323618116586105, Val loss:3.42646387219429
Epoch 2, Batch 0, train loss:1.1040394306182861, Elapsed time for epoch : 0.011679351329803467
Epoch 2, Batch 10, train loss:1.0977438688278198, Elapsed time for epoch : 0.1275789459546407
Epoch 2, Batch 20, train loss:1.2056865692138672, Elapsed time for epoch : 0.24401519298553467
Epoch 2, Batch 30, train loss:1.173994779586792, Elapsed time for epoch : 0.3604056477546692
Epoch 2, Batch 40, train loss:1.1646149158477783, Elapsed time for epoch : 0.4765697518984477
Epoch 2, Batch 50, train loss:1.1621010303497314, Elapsed time for epoch : 0.5928767720858256
Epoch 2, Batch 60, train loss:1.0853074789047241, Elapsed time for epoch : 0.7087644259134929
Epoch 2, Batch 70, train loss:0.7260655164718628, Elapsed time for epoch : 0.825201666355133
Epoch 2, Batch 80, train loss:1.0289241075515747, Elapsed time for epoch : 0.9413318157196044
Epoch 2, Batch 90, train loss:1.053415060043335, Elapsed time for epoch : 1.0576967438062033
Epoch 2, Batch 100, train loss:1.1402710676193237, Elapsed time for epoch : 1.1737722277641296
Epoch 2, Batch 110, train loss:0.9120470881462097, Elapsed time for epoch : 1.2899335741996765
Batch 0, val loss:1.1233161687850952
Batch 10, val loss:1.8665696382522583
Batch 20, val loss:1.8084481954574585
Batch 30, val loss:2.067765951156616
Epoch 2, Train Loss:1.0123360462810682, Val loss:3.087476503517893
Epoch 3, Batch 0, train loss:0.9317677617073059, Elapsed time for epoch : 0.011670362949371339
Epoch 3, Batch 10, train loss:0.9150304198265076, Elapsed time for epoch : 0.12793840567270914
Epoch 3, Batch 20, train loss:0.9677649736404419, Elapsed time for epoch : 0.2443477153778076
Epoch 3, Batch 30, train loss:1.0052030086517334, Elapsed time for epoch : 0.3605620662371318
Epoch 3, Batch 40, train loss:0.8069934844970703, Elapsed time for epoch : 0.4766820232073466
Epoch 3, Batch 50, train loss:0.5139209032058716, Elapsed time for epoch : 0.5936128616333007
Epoch 3, Batch 60, train loss:0.9955669045448303, Elapsed time for epoch : 0.7100438078244528
Epoch 3, Batch 70, train loss:0.8725718855857849, Elapsed time for epoch : 0.8266461253166199
Epoch 3, Batch 80, train loss:0.9368018507957458, Elapsed time for epoch : 0.9433765689531962
Epoch 3, Batch 90, train loss:0.8436188101768494, Elapsed time for epoch : 1.0598909298578898
Epoch 3, Batch 100, train loss:0.41895976662635803, Elapsed time for epoch : 1.175829287370046
Epoch 3, Batch 110, train loss:0.7997506856918335, Elapsed time for epoch : 1.29229679107666
Batch 0, val loss:1.7350126504898071
Batch 10, val loss:7.270478248596191
Batch 20, val loss:6.655613422393799
Batch 30, val loss:2.2582225799560547
Epoch 3, Train Loss:0.8196506116701209, Val loss:4.253905296325684
Epoch 4, Batch 0, train loss:0.7640039324760437, Elapsed time for epoch : 0.011595634619394939
Epoch 4, Batch 10, train loss:0.3799583911895752, Elapsed time for epoch : 0.1279035488764445
Epoch 4, Batch 20, train loss:0.3628239035606384, Elapsed time for epoch : 0.24425985813140869
Epoch 4, Batch 30, train loss:0.7596164345741272, Elapsed time for epoch : 0.3602958718935649
Epoch 4, Batch 40, train loss:0.7156862020492554, Elapsed time for epoch : 0.4764617125193278
Epoch 4, Batch 50, train loss:0.6554054021835327, Elapsed time for epoch : 0.5933100740114848
Epoch 4, Batch 60, train loss:0.6517500877380371, Elapsed time for epoch : 0.7094303369522095
Epoch 4, Batch 70, train loss:0.6720961928367615, Elapsed time for epoch : 0.8257912079493205
Epoch 4, Batch 80, train loss:0.32507023215293884, Elapsed time for epoch : 0.9420739889144898
Epoch 4, Batch 90, train loss:0.620062530040741, Elapsed time for epoch : 1.0579258640607199
Epoch 4, Batch 100, train loss:0.6726428270339966, Elapsed time for epoch : 1.174106192588806
Epoch 4, Batch 110, train loss:0.6601153016090393, Elapsed time for epoch : 1.2903164823849995
Batch 0, val loss:3.3127071857452393
Batch 10, val loss:1.850555419921875
Batch 20, val loss:4.510145664215088
Batch 30, val loss:2.576169967651367
Epoch 4, Train Loss:0.6113759023987728, Val loss:4.664507216877407
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61138
wandb:         Val Loss 4.66451
wandb:      train_batch 110
wandb: train_batch_loss 0.66012
wandb:        val_batch 30
wandb:   val_batch_loss 2.57617
wandb: 
wandb: üöÄ View run expert-yogurt-389 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/k1i7x02u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_052108-k1i7x02u/logs
Seed completed execution! 1 0.7_4
------------------------------------------------------------------
Running for seed 42 of experiment 0.7_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_052902-9ficopyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-field-391
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9ficopyv
Epoch 0, Batch 0, train loss:8.67747688293457, Elapsed time for epoch : 0.013664865493774414
Epoch 0, Batch 10, train loss:4.928752899169922, Elapsed time for epoch : 0.12934763431549073
Epoch 0, Batch 20, train loss:3.738034725189209, Elapsed time for epoch : 0.24550007581710814
Epoch 0, Batch 30, train loss:4.731654167175293, Elapsed time for epoch : 0.36144638458887735
Epoch 0, Batch 40, train loss:2.9866909980773926, Elapsed time for epoch : 0.47771454652150475
Epoch 0, Batch 50, train loss:2.7353641986846924, Elapsed time for epoch : 0.5934465845425924
Epoch 0, Batch 60, train loss:2.6798408031463623, Elapsed time for epoch : 0.7103321552276611
Epoch 0, Batch 70, train loss:2.5314579010009766, Elapsed time for epoch : 0.8266279101371765
Epoch 0, Batch 80, train loss:2.7569782733917236, Elapsed time for epoch : 0.9429324706395467
Epoch 0, Batch 90, train loss:2.214308977127075, Elapsed time for epoch : 1.0599755883216857
Epoch 0, Batch 100, train loss:2.1548101902008057, Elapsed time for epoch : 1.1770360708236693
Epoch 0, Batch 110, train loss:1.8309334516525269, Elapsed time for epoch : 1.293361759185791
Batch 0, val loss:3.410646438598633
Batch 10, val loss:3.098804235458374
Batch 20, val loss:2.6606194972991943
Batch 30, val loss:5.244523525238037
Epoch 0, Train Loss:3.2332028109094373, Val loss:3.5390967461797924
Epoch 1, Batch 0, train loss:2.0031578540802, Elapsed time for epoch : 0.011631107330322266
Epoch 1, Batch 10, train loss:1.7330533266067505, Elapsed time for epoch : 0.12799644470214844
Epoch 1, Batch 20, train loss:1.4836429357528687, Elapsed time for epoch : 0.24400306940078736
Epoch 1, Batch 30, train loss:1.6068178415298462, Elapsed time for epoch : 0.36045111417770387
Epoch 1, Batch 40, train loss:1.6742351055145264, Elapsed time for epoch : 0.477046791712443
Epoch 1, Batch 50, train loss:1.5329152345657349, Elapsed time for epoch : 0.5932687520980835
Epoch 1, Batch 60, train loss:1.4499298334121704, Elapsed time for epoch : 0.7099769194920857
Epoch 1, Batch 70, train loss:1.4503127336502075, Elapsed time for epoch : 0.8263815641403198
Epoch 1, Batch 80, train loss:1.3496475219726562, Elapsed time for epoch : 0.9432114760080973
Epoch 1, Batch 90, train loss:1.3171006441116333, Elapsed time for epoch : 1.0594557801882425
Epoch 1, Batch 100, train loss:0.8923994302749634, Elapsed time for epoch : 1.1756544351577758
Epoch 1, Batch 110, train loss:1.0864521265029907, Elapsed time for epoch : 1.2924243132273356
Batch 0, val loss:3.6375012397766113
Batch 10, val loss:2.378173351287842
Batch 20, val loss:2.7924532890319824
Batch 30, val loss:3.334510326385498
Epoch 1, Train Loss:1.4323618116586105, Val loss:3.42646387219429
Epoch 2, Batch 0, train loss:1.1040394306182861, Elapsed time for epoch : 0.01163638432820638
Epoch 2, Batch 10, train loss:1.0977438688278198, Elapsed time for epoch : 0.12788015206654865
Epoch 2, Batch 20, train loss:1.2056865692138672, Elapsed time for epoch : 0.244299058119456
Epoch 2, Batch 30, train loss:1.173994779586792, Elapsed time for epoch : 0.36019434134165446
Epoch 2, Batch 40, train loss:1.1646149158477783, Elapsed time for epoch : 0.4767006039619446
Epoch 2, Batch 50, train loss:1.1621010303497314, Elapsed time for epoch : 0.5931752721468607
Epoch 2, Batch 60, train loss:1.0853074789047241, Elapsed time for epoch : 0.7094786127408346
Epoch 2, Batch 70, train loss:0.7260655164718628, Elapsed time for epoch : 0.8261232972145081
Epoch 2, Batch 80, train loss:1.0289241075515747, Elapsed time for epoch : 0.9433557192484537
Epoch 2, Batch 90, train loss:1.053415060043335, Elapsed time for epoch : 1.0595103979110718
Epoch 2, Batch 100, train loss:1.1402710676193237, Elapsed time for epoch : 1.1760528286298115
Epoch 2, Batch 110, train loss:0.9120470881462097, Elapsed time for epoch : 1.292703358332316
Batch 0, val loss:1.1233161687850952
Batch 10, val loss:1.8665696382522583
Batch 20, val loss:1.8084481954574585
Batch 30, val loss:2.067765951156616
Epoch 2, Train Loss:1.0123360462810682, Val loss:3.087476503517893
Epoch 3, Batch 0, train loss:0.9317677617073059, Elapsed time for epoch : 0.011652008692423502
Epoch 3, Batch 10, train loss:0.9150304198265076, Elapsed time for epoch : 0.1280849854151408
Epoch 3, Batch 20, train loss:0.9677649736404419, Elapsed time for epoch : 0.24412171840667723
Epoch 3, Batch 30, train loss:1.0052030086517334, Elapsed time for epoch : 0.3603944460550944
Epoch 3, Batch 40, train loss:0.8069934844970703, Elapsed time for epoch : 0.47686716318130495
Epoch 3, Batch 50, train loss:0.5139209032058716, Elapsed time for epoch : 0.5929112712542216
Epoch 3, Batch 60, train loss:0.9955669045448303, Elapsed time for epoch : 0.7094820340474447
Epoch 3, Batch 70, train loss:0.8725718855857849, Elapsed time for epoch : 0.826224935054779
Epoch 3, Batch 80, train loss:0.9368018507957458, Elapsed time for epoch : 0.9425527612368266
Epoch 3, Batch 90, train loss:0.8436188101768494, Elapsed time for epoch : 1.0588015913963318
Epoch 3, Batch 100, train loss:0.41895976662635803, Elapsed time for epoch : 1.1748252113660176
Epoch 3, Batch 110, train loss:0.7997506856918335, Elapsed time for epoch : 1.291614826520284
Batch 0, val loss:1.7350126504898071
Batch 10, val loss:7.270478248596191
Batch 20, val loss:6.655613422393799
Batch 30, val loss:2.2582225799560547
Epoch 3, Train Loss:0.8196506116701209, Val loss:4.253905296325684
Epoch 4, Batch 0, train loss:0.7640039324760437, Elapsed time for epoch : 0.011906683444976807
Epoch 4, Batch 10, train loss:0.3799583911895752, Elapsed time for epoch : 0.12849425077438353
Epoch 4, Batch 20, train loss:0.3628239035606384, Elapsed time for epoch : 0.24528123140335084
Epoch 4, Batch 30, train loss:0.7596164345741272, Elapsed time for epoch : 0.3616124431292216
Epoch 4, Batch 40, train loss:0.7156862020492554, Elapsed time for epoch : 0.4777395804723104
Epoch 4, Batch 50, train loss:0.6554054021835327, Elapsed time for epoch : 0.5945481618245443
Epoch 4, Batch 60, train loss:0.6517500877380371, Elapsed time for epoch : 0.7106564203898112
Epoch 4, Batch 70, train loss:0.6720961928367615, Elapsed time for epoch : 0.8268408258756001
Epoch 4, Batch 80, train loss:0.32507023215293884, Elapsed time for epoch : 0.9434848586718242
Epoch 4, Batch 90, train loss:0.620062530040741, Elapsed time for epoch : 1.0599199811617532
Epoch 4, Batch 100, train loss:0.6726428270339966, Elapsed time for epoch : 1.1765857179959616
Epoch 4, Batch 110, train loss:0.6601153016090393, Elapsed time for epoch : 1.2927845478057862
Batch 0, val loss:3.3127071857452393
Batch 10, val loss:1.850555419921875
Batch 20, val loss:4.510145664215088
Batch 30, val loss:2.576169967651367
Epoch 4, Train Loss:0.6113759023987728, Val loss:4.664507216877407
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61138
wandb:         Val Loss 4.66451
wandb:      train_batch 110
wandb: train_batch_loss 0.66012
wandb:        val_batch 30
wandb:   val_batch_loss 2.57617
wandb: 
wandb: üöÄ View run balmy-field-391 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/9ficopyv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_052902-9ficopyv/logs
Seed completed execution! 42 0.7_4
------------------------------------------------------------------
Running for seed 89 of experiment 0.7_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_053650-8q5gi96z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-paper-393
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8q5gi96z
Epoch 0, Batch 0, train loss:8.67747688293457, Elapsed time for epoch : 0.012095451354980469
Epoch 0, Batch 10, train loss:4.928752899169922, Elapsed time for epoch : 0.12867043415705362
Epoch 0, Batch 20, train loss:3.738034725189209, Elapsed time for epoch : 0.24485029379526774
Epoch 0, Batch 30, train loss:4.731654167175293, Elapsed time for epoch : 0.3605655312538147
Epoch 0, Batch 40, train loss:2.9866909980773926, Elapsed time for epoch : 0.47709681987762453
Epoch 0, Batch 50, train loss:2.7353641986846924, Elapsed time for epoch : 0.5933241963386535
Epoch 0, Batch 60, train loss:2.6798408031463623, Elapsed time for epoch : 0.7093318025271098
Epoch 0, Batch 70, train loss:2.5314579010009766, Elapsed time for epoch : 0.8257570664087931
Epoch 0, Batch 80, train loss:2.7569782733917236, Elapsed time for epoch : 0.9418093880017598
Epoch 0, Batch 90, train loss:2.214308977127075, Elapsed time for epoch : 1.0579538822174073
Epoch 0, Batch 100, train loss:2.1548101902008057, Elapsed time for epoch : 1.1740609367688497
Epoch 0, Batch 110, train loss:1.8309334516525269, Elapsed time for epoch : 1.2903622388839722
Batch 0, val loss:3.410646438598633
Batch 10, val loss:3.098804235458374
Batch 20, val loss:2.6606194972991943
Batch 30, val loss:5.244523525238037
Epoch 0, Train Loss:3.2332028109094373, Val loss:3.5390967461797924
Epoch 1, Batch 0, train loss:2.0031578540802, Elapsed time for epoch : 0.01164399782816569
Epoch 1, Batch 10, train loss:1.7330533266067505, Elapsed time for epoch : 0.1275411367416382
Epoch 1, Batch 20, train loss:1.4836429357528687, Elapsed time for epoch : 0.24389439423878986
Epoch 1, Batch 30, train loss:1.6068178415298462, Elapsed time for epoch : 0.3604159355163574
Epoch 1, Batch 40, train loss:1.6742351055145264, Elapsed time for epoch : 0.4772761424382528
Epoch 1, Batch 50, train loss:1.5329152345657349, Elapsed time for epoch : 0.5937888542811076
Epoch 1, Batch 60, train loss:1.4499298334121704, Elapsed time for epoch : 0.7100934664408366
Epoch 1, Batch 70, train loss:1.4503127336502075, Elapsed time for epoch : 0.8264585892359416
Epoch 1, Batch 80, train loss:1.3496475219726562, Elapsed time for epoch : 0.9428781072298685
Epoch 1, Batch 90, train loss:1.3171006441116333, Elapsed time for epoch : 1.0593272964159648
Epoch 1, Batch 100, train loss:0.8923994302749634, Elapsed time for epoch : 1.1756900747617085
Epoch 1, Batch 110, train loss:1.0864521265029907, Elapsed time for epoch : 1.292033302783966
Batch 0, val loss:3.6375012397766113
Batch 10, val loss:2.378173351287842
Batch 20, val loss:2.7924532890319824
Batch 30, val loss:3.334510326385498
Epoch 1, Train Loss:1.4323618116586105, Val loss:3.42646387219429
Epoch 2, Batch 0, train loss:1.1040394306182861, Elapsed time for epoch : 0.011694363753000895
Epoch 2, Batch 10, train loss:1.0977438688278198, Elapsed time for epoch : 0.1278576135635376
Epoch 2, Batch 20, train loss:1.2056865692138672, Elapsed time for epoch : 0.24423228502273558
Epoch 2, Batch 30, train loss:1.173994779586792, Elapsed time for epoch : 0.3608467737833659
Epoch 2, Batch 40, train loss:1.1646149158477783, Elapsed time for epoch : 0.4771158258120219
Epoch 2, Batch 50, train loss:1.1621010303497314, Elapsed time for epoch : 0.5935739199320476
Epoch 2, Batch 60, train loss:1.0853074789047241, Elapsed time for epoch : 0.7100545525550842
Epoch 2, Batch 70, train loss:0.7260655164718628, Elapsed time for epoch : 0.8264576872189839
Epoch 2, Batch 80, train loss:1.0289241075515747, Elapsed time for epoch : 0.9433686176935832
Epoch 2, Batch 90, train loss:1.053415060043335, Elapsed time for epoch : 1.059481426080068
Epoch 2, Batch 100, train loss:1.1402710676193237, Elapsed time for epoch : 1.1759737769762675
Epoch 2, Batch 110, train loss:0.9120470881462097, Elapsed time for epoch : 1.2922685305277506
Batch 0, val loss:1.1233161687850952
Batch 10, val loss:1.8665696382522583
Batch 20, val loss:1.8084481954574585
Batch 30, val loss:2.067765951156616
Epoch 2, Train Loss:1.0123360462810682, Val loss:3.087476503517893
Epoch 3, Batch 0, train loss:0.9317677617073059, Elapsed time for epoch : 0.011686110496520996
Epoch 3, Batch 10, train loss:0.9150304198265076, Elapsed time for epoch : 0.12772867282231648
Epoch 3, Batch 20, train loss:0.9677649736404419, Elapsed time for epoch : 0.24401952823003134
Epoch 3, Batch 30, train loss:1.0052030086517334, Elapsed time for epoch : 0.3604151844978333
Epoch 3, Batch 40, train loss:0.8069934844970703, Elapsed time for epoch : 0.4767078638076782
Epoch 3, Batch 50, train loss:0.5139209032058716, Elapsed time for epoch : 0.5930310606956481
Epoch 3, Batch 60, train loss:0.9955669045448303, Elapsed time for epoch : 0.709620984395345
Epoch 3, Batch 70, train loss:0.8725718855857849, Elapsed time for epoch : 0.825882355372111
Epoch 3, Batch 80, train loss:0.9368018507957458, Elapsed time for epoch : 0.941949478785197
Epoch 3, Batch 90, train loss:0.8436188101768494, Elapsed time for epoch : 1.0583762804667154
Epoch 3, Batch 100, train loss:0.41895976662635803, Elapsed time for epoch : 1.1743691126505533
Epoch 3, Batch 110, train loss:0.7997506856918335, Elapsed time for epoch : 1.291023023923238
Batch 0, val loss:1.7350126504898071
Batch 10, val loss:7.270478248596191
Batch 20, val loss:6.655613422393799
Batch 30, val loss:2.2582225799560547
Epoch 3, Train Loss:0.8196506116701209, Val loss:4.253905296325684
Epoch 4, Batch 0, train loss:0.7640039324760437, Elapsed time for epoch : 0.011657877763112386
Epoch 4, Batch 10, train loss:0.3799583911895752, Elapsed time for epoch : 0.12811567385991415
Epoch 4, Batch 20, train loss:0.3628239035606384, Elapsed time for epoch : 0.24477426608403524
Epoch 4, Batch 30, train loss:0.7596164345741272, Elapsed time for epoch : 0.3610242009162903
Epoch 4, Batch 40, train loss:0.7156862020492554, Elapsed time for epoch : 0.477102522055308
Epoch 4, Batch 50, train loss:0.6554054021835327, Elapsed time for epoch : 0.5936914920806885
Epoch 4, Batch 60, train loss:0.6517500877380371, Elapsed time for epoch : 0.7100488464037578
Epoch 4, Batch 70, train loss:0.6720961928367615, Elapsed time for epoch : 0.8267935554186503
Epoch 4, Batch 80, train loss:0.32507023215293884, Elapsed time for epoch : 0.9437039693196615
Epoch 4, Batch 90, train loss:0.620062530040741, Elapsed time for epoch : 1.0599597454071046
Epoch 4, Batch 100, train loss:0.6726428270339966, Elapsed time for epoch : 1.1766319354375203
Epoch 4, Batch 110, train loss:0.6601153016090393, Elapsed time for epoch : 1.2929350813229878
Batch 0, val loss:3.3127071857452393
Batch 10, val loss:1.850555419921875
Batch 20, val loss:4.510145664215088
Batch 30, val loss:2.576169967651367
Epoch 4, Train Loss:0.6113759023987728, Val loss:4.664507216877407
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61138
wandb:         Val Loss 4.66451
wandb:      train_batch 110
wandb: train_batch_loss 0.66012
wandb:        val_batch 30
wandb:   val_batch_loss 2.57617
wandb: 
wandb: üöÄ View run leafy-paper-393 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/8q5gi96z
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_053650-8q5gi96z/logs
Seed completed execution! 89 0.7_4
------------------------------------------------------------------
Running for seed 23 of experiment 0.7_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_054438-rqi9vjub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run likely-terrain-395
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rqi9vjub
Epoch 0, Batch 0, train loss:8.67747688293457, Elapsed time for epoch : 0.0134177565574646
Epoch 0, Batch 10, train loss:4.928752899169922, Elapsed time for epoch : 0.12914968331654866
Epoch 0, Batch 20, train loss:3.738034725189209, Elapsed time for epoch : 0.2453662395477295
Epoch 0, Batch 30, train loss:4.731654167175293, Elapsed time for epoch : 0.3610667705535889
Epoch 0, Batch 40, train loss:2.9866909980773926, Elapsed time for epoch : 0.4769158919652303
Epoch 0, Batch 50, train loss:2.7353641986846924, Elapsed time for epoch : 0.5932295719782511
Epoch 0, Batch 60, train loss:2.6798408031463623, Elapsed time for epoch : 0.7090364257494609
Epoch 0, Batch 70, train loss:2.5314579010009766, Elapsed time for epoch : 0.8254016955693563
Epoch 0, Batch 80, train loss:2.7569782733917236, Elapsed time for epoch : 0.9414762417475383
Epoch 0, Batch 90, train loss:2.214308977127075, Elapsed time for epoch : 1.0572821497917175
Epoch 0, Batch 100, train loss:2.1548101902008057, Elapsed time for epoch : 1.173369288444519
Epoch 0, Batch 110, train loss:1.8309334516525269, Elapsed time for epoch : 1.2895637591679892
Batch 0, val loss:3.410646438598633
Batch 10, val loss:3.098804235458374
Batch 20, val loss:2.6606194972991943
Batch 30, val loss:5.244523525238037
Epoch 0, Train Loss:3.2332028109094373, Val loss:3.5390967461797924
Epoch 1, Batch 0, train loss:2.0031578540802, Elapsed time for epoch : 0.011608393987019856
Epoch 1, Batch 10, train loss:1.7330533266067505, Elapsed time for epoch : 0.12773958444595337
Epoch 1, Batch 20, train loss:1.4836429357528687, Elapsed time for epoch : 0.2440411329269409
Epoch 1, Batch 30, train loss:1.6068178415298462, Elapsed time for epoch : 0.36034842729568484
Epoch 1, Batch 40, train loss:1.6742351055145264, Elapsed time for epoch : 0.47659556070963544
Epoch 1, Batch 50, train loss:1.5329152345657349, Elapsed time for epoch : 0.5929685314496358
Epoch 1, Batch 60, train loss:1.4499298334121704, Elapsed time for epoch : 0.7093671242396037
Epoch 1, Batch 70, train loss:1.4503127336502075, Elapsed time for epoch : 0.8259208122889201
Epoch 1, Batch 80, train loss:1.3496475219726562, Elapsed time for epoch : 0.9421905636787414
Epoch 1, Batch 90, train loss:1.3171006441116333, Elapsed time for epoch : 1.0580461144447326
Epoch 1, Batch 100, train loss:0.8923994302749634, Elapsed time for epoch : 1.174504554271698
Epoch 1, Batch 110, train loss:1.0864521265029907, Elapsed time for epoch : 1.2911691228548685
Batch 0, val loss:3.6375012397766113
Batch 10, val loss:2.378173351287842
Batch 20, val loss:2.7924532890319824
Batch 30, val loss:3.334510326385498
Epoch 1, Train Loss:1.4323618116586105, Val loss:3.42646387219429
Epoch 2, Batch 0, train loss:1.1040394306182861, Elapsed time for epoch : 0.01167922814687093
Epoch 2, Batch 10, train loss:1.0977438688278198, Elapsed time for epoch : 0.1279148062070211
Epoch 2, Batch 20, train loss:1.2056865692138672, Elapsed time for epoch : 0.24448692798614502
Epoch 2, Batch 30, train loss:1.173994779586792, Elapsed time for epoch : 0.3607461333274841
Epoch 2, Batch 40, train loss:1.1646149158477783, Elapsed time for epoch : 0.47740223407745364
Epoch 2, Batch 50, train loss:1.1621010303497314, Elapsed time for epoch : 0.5942585984865825
Epoch 2, Batch 60, train loss:1.0853074789047241, Elapsed time for epoch : 0.7107885797818502
Epoch 2, Batch 70, train loss:0.7260655164718628, Elapsed time for epoch : 0.8276658733685811
Epoch 2, Batch 80, train loss:1.0289241075515747, Elapsed time for epoch : 0.9440040667851766
Epoch 2, Batch 90, train loss:1.053415060043335, Elapsed time for epoch : 1.0601658821105957
Epoch 2, Batch 100, train loss:1.1402710676193237, Elapsed time for epoch : 1.1770049254099528
Epoch 2, Batch 110, train loss:0.9120470881462097, Elapsed time for epoch : 1.2932769576708476
Batch 0, val loss:1.1233161687850952
Batch 10, val loss:1.8665696382522583
Batch 20, val loss:1.8084481954574585
Batch 30, val loss:2.067765951156616
Epoch 2, Train Loss:1.0123360462810682, Val loss:3.087476503517893
Epoch 3, Batch 0, train loss:0.9317677617073059, Elapsed time for epoch : 0.011651281515757244
Epoch 3, Batch 10, train loss:0.9150304198265076, Elapsed time for epoch : 0.12800851265589397
Epoch 3, Batch 20, train loss:0.9677649736404419, Elapsed time for epoch : 0.24429766734441122
Epoch 3, Batch 30, train loss:1.0052030086517334, Elapsed time for epoch : 0.3607011636098226
Epoch 3, Batch 40, train loss:0.8069934844970703, Elapsed time for epoch : 0.47700818777084353
Epoch 3, Batch 50, train loss:0.5139209032058716, Elapsed time for epoch : 0.5931356231371562
Epoch 3, Batch 60, train loss:0.9955669045448303, Elapsed time for epoch : 0.709424881140391
Epoch 3, Batch 70, train loss:0.8725718855857849, Elapsed time for epoch : 0.8258075078328451
Epoch 3, Batch 80, train loss:0.9368018507957458, Elapsed time for epoch : 0.9421982407569885
Epoch 3, Batch 90, train loss:0.8436188101768494, Elapsed time for epoch : 1.0588510711987813
Epoch 3, Batch 100, train loss:0.41895976662635803, Elapsed time for epoch : 1.1751739263534546
Epoch 3, Batch 110, train loss:0.7997506856918335, Elapsed time for epoch : 1.2914716402689617
Batch 0, val loss:1.7350126504898071
Batch 10, val loss:7.270478248596191
Batch 20, val loss:6.655613422393799
Batch 30, val loss:2.2582225799560547
Epoch 3, Train Loss:0.8196506116701209, Val loss:4.253905296325684
Epoch 4, Batch 0, train loss:0.7640039324760437, Elapsed time for epoch : 0.01166298786799113
Epoch 4, Batch 10, train loss:0.3799583911895752, Elapsed time for epoch : 0.12822212378184
Epoch 4, Batch 20, train loss:0.3628239035606384, Elapsed time for epoch : 0.24453892707824706
Epoch 4, Batch 30, train loss:0.7596164345741272, Elapsed time for epoch : 0.3610530972480774
Epoch 4, Batch 40, train loss:0.7156862020492554, Elapsed time for epoch : 0.4778255025545756
Epoch 4, Batch 50, train loss:0.6554054021835327, Elapsed time for epoch : 0.5942465941111247
Epoch 4, Batch 60, train loss:0.6517500877380371, Elapsed time for epoch : 0.7110010385513306
Epoch 4, Batch 70, train loss:0.6720961928367615, Elapsed time for epoch : 0.8278511524200439
Epoch 4, Batch 80, train loss:0.32507023215293884, Elapsed time for epoch : 0.9441797574361165
Epoch 4, Batch 90, train loss:0.620062530040741, Elapsed time for epoch : 1.060563063621521
Epoch 4, Batch 100, train loss:0.6726428270339966, Elapsed time for epoch : 1.1768783847490947
Epoch 4, Batch 110, train loss:0.6601153016090393, Elapsed time for epoch : 1.2932267228762309
Batch 0, val loss:3.3127071857452393
Batch 10, val loss:1.850555419921875
Batch 20, val loss:4.510145664215088
Batch 30, val loss:2.576169967651367
Epoch 4, Train Loss:0.6113759023987728, Val loss:4.664507216877407
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61138
wandb:         Val Loss 4.66451
wandb:      train_batch 110
wandb: train_batch_loss 0.66012
wandb:        val_batch 30
wandb:   val_batch_loss 2.57617
wandb: 
wandb: üöÄ View run likely-terrain-395 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/rqi9vjub
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_054438-rqi9vjub/logs
Seed completed execution! 23 0.7_4
------------------------------------------------------------------
Running for seed 113 of experiment 0.7_4
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_055224-s50smhmt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-snow-397
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/s50smhmt
Epoch 0, Batch 0, train loss:8.67747688293457, Elapsed time for epoch : 0.013457369804382325
Epoch 0, Batch 10, train loss:4.928752899169922, Elapsed time for epoch : 0.1306009531021118
Epoch 0, Batch 20, train loss:3.738034725189209, Elapsed time for epoch : 0.24686915079752605
Epoch 0, Batch 30, train loss:4.731654167175293, Elapsed time for epoch : 0.3637998898824056
Epoch 0, Batch 40, train loss:2.9866909980773926, Elapsed time for epoch : 0.4810891032218933
Epoch 0, Batch 50, train loss:2.7353641986846924, Elapsed time for epoch : 0.5985297282536824
Epoch 0, Batch 60, train loss:2.6798408031463623, Elapsed time for epoch : 0.7158558170000712
Epoch 0, Batch 70, train loss:2.5314579010009766, Elapsed time for epoch : 0.8325785199801127
Epoch 0, Batch 80, train loss:2.7569782733917236, Elapsed time for epoch : 0.9504666765530904
Epoch 0, Batch 90, train loss:2.214308977127075, Elapsed time for epoch : 1.0677979310353598
Epoch 0, Batch 100, train loss:2.1548101902008057, Elapsed time for epoch : 1.1852217396100362
Epoch 0, Batch 110, train loss:1.8309334516525269, Elapsed time for epoch : 1.3019869804382325
Batch 0, val loss:3.410646438598633
Batch 10, val loss:3.098804235458374
Batch 20, val loss:2.6606194972991943
Batch 30, val loss:5.244523525238037
Epoch 0, Train Loss:3.2332028109094373, Val loss:3.5390967461797924
Epoch 1, Batch 0, train loss:2.0031578540802, Elapsed time for epoch : 0.011651901404062907
Epoch 1, Batch 10, train loss:1.7330533266067505, Elapsed time for epoch : 0.1281037171681722
Epoch 1, Batch 20, train loss:1.4836429357528687, Elapsed time for epoch : 0.2440810203552246
Epoch 1, Batch 30, train loss:1.6068178415298462, Elapsed time for epoch : 0.36057936350504555
Epoch 1, Batch 40, train loss:1.6742351055145264, Elapsed time for epoch : 0.4772031505902608
Epoch 1, Batch 50, train loss:1.5329152345657349, Elapsed time for epoch : 0.593127179145813
Epoch 1, Batch 60, train loss:1.4499298334121704, Elapsed time for epoch : 0.7101234714190165
Epoch 1, Batch 70, train loss:1.4503127336502075, Elapsed time for epoch : 0.8265444358189901
Epoch 1, Batch 80, train loss:1.3496475219726562, Elapsed time for epoch : 0.9426304499308268
Epoch 1, Batch 90, train loss:1.3171006441116333, Elapsed time for epoch : 1.058703899383545
Epoch 1, Batch 100, train loss:0.8923994302749634, Elapsed time for epoch : 1.1749279856681825
Epoch 1, Batch 110, train loss:1.0864521265029907, Elapsed time for epoch : 1.291141891479492
Batch 0, val loss:3.6375012397766113
Batch 10, val loss:2.378173351287842
Batch 20, val loss:2.7924532890319824
Batch 30, val loss:3.334510326385498
Epoch 1, Train Loss:1.4323618116586105, Val loss:3.42646387219429
Epoch 2, Batch 0, train loss:1.1040394306182861, Elapsed time for epoch : 0.011646246910095215
Epoch 2, Batch 10, train loss:1.0977438688278198, Elapsed time for epoch : 0.12785944143931072
Epoch 2, Batch 20, train loss:1.2056865692138672, Elapsed time for epoch : 0.24438944260279338
Epoch 2, Batch 30, train loss:1.173994779586792, Elapsed time for epoch : 0.3610800584157308
Epoch 2, Batch 40, train loss:1.1646149158477783, Elapsed time for epoch : 0.47755833466847736
Epoch 2, Batch 50, train loss:1.1621010303497314, Elapsed time for epoch : 0.5937588334083557
Epoch 2, Batch 60, train loss:1.0853074789047241, Elapsed time for epoch : 0.7103755315144856
Epoch 2, Batch 70, train loss:0.7260655164718628, Elapsed time for epoch : 0.8268755793571472
Epoch 2, Batch 80, train loss:1.0289241075515747, Elapsed time for epoch : 0.9435544729232788
Epoch 2, Batch 90, train loss:1.053415060043335, Elapsed time for epoch : 1.0600829362869262
Epoch 2, Batch 100, train loss:1.1402710676193237, Elapsed time for epoch : 1.1764354785283406
Epoch 2, Batch 110, train loss:0.9120470881462097, Elapsed time for epoch : 1.292814556757609
Batch 0, val loss:1.1233161687850952
Batch 10, val loss:1.8665696382522583
Batch 20, val loss:1.8084481954574585
Batch 30, val loss:2.067765951156616
Epoch 2, Train Loss:1.0123360462810682, Val loss:3.087476503517893
Epoch 3, Batch 0, train loss:0.9317677617073059, Elapsed time for epoch : 0.011602977911631266
Epoch 3, Batch 10, train loss:0.9150304198265076, Elapsed time for epoch : 0.12784530321756998
Epoch 3, Batch 20, train loss:0.9677649736404419, Elapsed time for epoch : 0.2438807765642802
Epoch 3, Batch 30, train loss:1.0052030086517334, Elapsed time for epoch : 0.3604164997736613
Epoch 3, Batch 40, train loss:0.8069934844970703, Elapsed time for epoch : 0.4767395734786987
Epoch 3, Batch 50, train loss:0.5139209032058716, Elapsed time for epoch : 0.5925717075665792
Epoch 3, Batch 60, train loss:0.9955669045448303, Elapsed time for epoch : 0.7091169675191243
Epoch 3, Batch 70, train loss:0.8725718855857849, Elapsed time for epoch : 0.8256131728490194
Epoch 3, Batch 80, train loss:0.9368018507957458, Elapsed time for epoch : 0.9417715311050415
Epoch 3, Batch 90, train loss:0.8436188101768494, Elapsed time for epoch : 1.0584804852803549
Epoch 3, Batch 100, train loss:0.41895976662635803, Elapsed time for epoch : 1.1746972004572551
Epoch 3, Batch 110, train loss:0.7997506856918335, Elapsed time for epoch : 1.2910709102948508
Batch 0, val loss:1.7350126504898071
Batch 10, val loss:7.270478248596191
Batch 20, val loss:6.655613422393799
Batch 30, val loss:2.2582225799560547
Epoch 3, Train Loss:0.8196506116701209, Val loss:4.253905296325684
Epoch 4, Batch 0, train loss:0.7640039324760437, Elapsed time for epoch : 0.011624181270599365
Epoch 4, Batch 10, train loss:0.3799583911895752, Elapsed time for epoch : 0.12818548281987507
Epoch 4, Batch 20, train loss:0.3628239035606384, Elapsed time for epoch : 0.24448923667271932
Epoch 4, Batch 30, train loss:0.7596164345741272, Elapsed time for epoch : 0.3609091838200887
Epoch 4, Batch 40, train loss:0.7156862020492554, Elapsed time for epoch : 0.477361532052358
Epoch 4, Batch 50, train loss:0.6554054021835327, Elapsed time for epoch : 0.5942267696062724
Epoch 4, Batch 60, train loss:0.6517500877380371, Elapsed time for epoch : 0.71055113474528
Epoch 4, Batch 70, train loss:0.6720961928367615, Elapsed time for epoch : 0.8273584842681885
Epoch 4, Batch 80, train loss:0.32507023215293884, Elapsed time for epoch : 0.9443073193232219
Epoch 4, Batch 90, train loss:0.620062530040741, Elapsed time for epoch : 1.0608786304791769
Epoch 4, Batch 100, train loss:0.6726428270339966, Elapsed time for epoch : 1.177304478486379
Epoch 4, Batch 110, train loss:0.6601153016090393, Elapsed time for epoch : 1.2942773540814718
Batch 0, val loss:3.3127071857452393
Batch 10, val loss:1.850555419921875
Batch 20, val loss:4.510145664215088
Batch 30, val loss:2.576169967651367
Epoch 4, Train Loss:0.6113759023987728, Val loss:4.664507216877407
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñà
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÉ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.61138
wandb:         Val Loss 4.66451
wandb:      train_batch 110
wandb: train_batch_loss 0.66012
wandb:        val_batch 30
wandb:   val_batch_loss 2.57617
wandb: 
wandb: üöÄ View run exalted-snow-397 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/s50smhmt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_055224-s50smhmt/logs
Seed completed execution! 113 0.7_4
------------------------------------------------------------------
Experiment complete 0.7_4
==========================================================================
Running experiment for setting 0.7_5
==========================================================================
Running for seed 1 of experiment 0.7_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_060012-cjmebe9b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-bush-399
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/cjmebe9b
Epoch 0, Batch 0, train loss:7.772118091583252, Elapsed time for epoch : 0.013589505354563396
Epoch 0, Batch 10, train loss:4.110156059265137, Elapsed time for epoch : 0.14033315579096475
Epoch 0, Batch 20, train loss:3.5861294269561768, Elapsed time for epoch : 0.265859043598175
Epoch 0, Batch 30, train loss:3.3507461547851562, Elapsed time for epoch : 0.3902249892552694
Epoch 0, Batch 40, train loss:3.1592957973480225, Elapsed time for epoch : 0.5159265160560608
Epoch 0, Batch 50, train loss:2.8426802158355713, Elapsed time for epoch : 0.6421397089958191
Epoch 0, Batch 60, train loss:2.7208786010742188, Elapsed time for epoch : 0.7674110968907674
Epoch 0, Batch 70, train loss:2.7141194343566895, Elapsed time for epoch : 0.8921724796295166
Epoch 0, Batch 80, train loss:2.5511584281921387, Elapsed time for epoch : 1.0172604640324912
Epoch 0, Batch 90, train loss:2.534867763519287, Elapsed time for epoch : 1.142147946357727
Epoch 0, Batch 100, train loss:2.195695400238037, Elapsed time for epoch : 1.2661940217018128
Epoch 0, Batch 110, train loss:1.91389000415802, Elapsed time for epoch : 1.391565708319346
Batch 0, val loss:6.808574676513672
Batch 10, val loss:7.134714603424072
Batch 20, val loss:3.8598008155822754
Batch 30, val loss:7.128749847412109
Epoch 0, Train Loss:3.2517762360365494, Val loss:4.491629593902164
Epoch 1, Batch 0, train loss:1.9461984634399414, Elapsed time for epoch : 0.011661084493001302
Epoch 1, Batch 10, train loss:1.8205089569091797, Elapsed time for epoch : 0.1276703357696533
Epoch 1, Batch 20, train loss:1.4860020875930786, Elapsed time for epoch : 0.2439678986867269
Epoch 1, Batch 30, train loss:1.609519362449646, Elapsed time for epoch : 0.36072092056274413
Epoch 1, Batch 40, train loss:1.7044317722320557, Elapsed time for epoch : 0.4767604947090149
Epoch 1, Batch 50, train loss:1.5855234861373901, Elapsed time for epoch : 0.5929410139719645
Epoch 1, Batch 60, train loss:1.5392639636993408, Elapsed time for epoch : 0.7094972928365072
Epoch 1, Batch 70, train loss:1.7378628253936768, Elapsed time for epoch : 0.8256018956502279
Epoch 1, Batch 80, train loss:1.3756721019744873, Elapsed time for epoch : 0.9419513702392578
Epoch 1, Batch 90, train loss:1.309278130531311, Elapsed time for epoch : 1.0582835714022318
Epoch 1, Batch 100, train loss:0.7931307554244995, Elapsed time for epoch : 1.1742562572161357
Epoch 1, Batch 110, train loss:1.0261014699935913, Elapsed time for epoch : 1.290581206480662
Batch 0, val loss:2.4065041542053223
Batch 10, val loss:2.594111680984497
Batch 20, val loss:3.0769762992858887
Batch 30, val loss:2.633695363998413
Epoch 1, Train Loss:1.4384395988091179, Val loss:3.8756343060069613
Epoch 2, Batch 0, train loss:0.9507532119750977, Elapsed time for epoch : 0.011592129866282145
Epoch 2, Batch 10, train loss:0.9133151173591614, Elapsed time for epoch : 0.12781707445780435
Epoch 2, Batch 20, train loss:0.9675410389900208, Elapsed time for epoch : 0.24402627150217693
Epoch 2, Batch 30, train loss:0.9982491731643677, Elapsed time for epoch : 0.36003949244817096
Epoch 2, Batch 40, train loss:0.8952164053916931, Elapsed time for epoch : 0.47650839885075885
Epoch 2, Batch 50, train loss:1.0990649461746216, Elapsed time for epoch : 0.5932101726531982
Epoch 2, Batch 60, train loss:0.8621593117713928, Elapsed time for epoch : 0.7092929959297181
Epoch 2, Batch 70, train loss:0.379912406206131, Elapsed time for epoch : 0.8258263270060221
Epoch 2, Batch 80, train loss:0.7908468842506409, Elapsed time for epoch : 0.942421559492747
Epoch 2, Batch 90, train loss:0.7981401085853577, Elapsed time for epoch : 1.0587874452273052
Epoch 2, Batch 100, train loss:0.9419729113578796, Elapsed time for epoch : 1.1754506945610046
Epoch 2, Batch 110, train loss:0.5831536650657654, Elapsed time for epoch : 1.2918618281682333
Batch 0, val loss:5.503520965576172
Batch 10, val loss:4.2605204582214355
Batch 20, val loss:1.8713349103927612
Batch 30, val loss:2.900514602661133
Epoch 2, Train Loss:0.7871142896621124, Val loss:5.1755169207851095
Epoch 3, Batch 0, train loss:0.6490575075149536, Elapsed time for epoch : 0.011730047067006429
Epoch 3, Batch 10, train loss:0.750257670879364, Elapsed time for epoch : 0.1281483769416809
Epoch 3, Batch 20, train loss:0.7601352334022522, Elapsed time for epoch : 0.2443697214126587
Epoch 3, Batch 30, train loss:0.5802761912345886, Elapsed time for epoch : 0.36086634794871014
Epoch 3, Batch 40, train loss:0.7409505844116211, Elapsed time for epoch : 0.4773856282234192
Epoch 3, Batch 50, train loss:0.20397578179836273, Elapsed time for epoch : 0.5936396400133769
Epoch 3, Batch 60, train loss:0.538579523563385, Elapsed time for epoch : 0.7101873477300008
Epoch 3, Batch 70, train loss:0.51080322265625, Elapsed time for epoch : 0.8267319957415263
Epoch 3, Batch 80, train loss:0.5771481394767761, Elapsed time for epoch : 0.9430480003356934
Epoch 3, Batch 90, train loss:0.46415239572525024, Elapsed time for epoch : 1.0593645175298054
Epoch 3, Batch 100, train loss:0.09808585792779922, Elapsed time for epoch : 1.1756961981455485
Epoch 3, Batch 110, train loss:0.3527704179286957, Elapsed time for epoch : 1.292108146349589
Batch 0, val loss:5.798018932342529
Batch 10, val loss:14.75664234161377
Batch 20, val loss:15.528691291809082
Batch 30, val loss:2.8383238315582275
Epoch 3, Train Loss:0.5010341617076294, Val loss:6.259986804591285
Epoch 4, Batch 0, train loss:0.45740172266960144, Elapsed time for epoch : 0.01161641279856364
Epoch 4, Batch 10, train loss:0.12530829012393951, Elapsed time for epoch : 0.1281261960665385
Epoch 4, Batch 20, train loss:0.07544941455125809, Elapsed time for epoch : 0.2446471373240153
Epoch 4, Batch 30, train loss:0.4435541033744812, Elapsed time for epoch : 0.3610648592313131
Epoch 4, Batch 40, train loss:0.3920346796512604, Elapsed time for epoch : 0.47729575634002686
Epoch 4, Batch 50, train loss:0.3572840392589569, Elapsed time for epoch : 0.593678883711497
Epoch 4, Batch 60, train loss:0.3137335479259491, Elapsed time for epoch : 0.7099061210950216
Epoch 4, Batch 70, train loss:0.34199070930480957, Elapsed time for epoch : 0.8264432946840922
Epoch 4, Batch 80, train loss:0.10399054735898972, Elapsed time for epoch : 0.9429030974706014
Epoch 4, Batch 90, train loss:0.33058473467826843, Elapsed time for epoch : 1.0591540137926738
Epoch 4, Batch 100, train loss:0.33447375893592834, Elapsed time for epoch : 1.175461208820343
Epoch 4, Batch 110, train loss:0.36815881729125977, Elapsed time for epoch : 1.2919895648956299
Batch 0, val loss:10.521428108215332
Batch 10, val loss:2.1417243480682373
Batch 20, val loss:6.997398376464844
Batch 30, val loss:1.9540059566497803
Epoch 4, Train Loss:0.2943423909985501, Val loss:5.943444619576137
wandb: - 0.157 MB of 0.171 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29434
wandb:         Val Loss 5.94344
wandb:      train_batch 110
wandb: train_batch_loss 0.36816
wandb:        val_batch 30
wandb:   val_batch_loss 1.95401
wandb: 
wandb: üöÄ View run ruby-bush-399 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/cjmebe9b
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_060012-cjmebe9b/logs
Seed completed execution! 1 0.7_5
------------------------------------------------------------------
Running for seed 42 of experiment 0.7_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_060806-d4k50o8e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-planet-401
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/d4k50o8e
Epoch 0, Batch 0, train loss:7.772118091583252, Elapsed time for epoch : 0.01359173059463501
Epoch 0, Batch 10, train loss:4.110156059265137, Elapsed time for epoch : 0.13020360469818115
Epoch 0, Batch 20, train loss:3.5861294269561768, Elapsed time for epoch : 0.24646246433258057
Epoch 0, Batch 30, train loss:3.3507461547851562, Elapsed time for epoch : 0.36262096961339313
Epoch 0, Batch 40, train loss:3.1592957973480225, Elapsed time for epoch : 0.47886486450831095
Epoch 0, Batch 50, train loss:2.8426802158355713, Elapsed time for epoch : 0.5952728629112244
Epoch 0, Batch 60, train loss:2.7208786010742188, Elapsed time for epoch : 0.7113446354866028
Epoch 0, Batch 70, train loss:2.7141194343566895, Elapsed time for epoch : 0.8276431957880656
Epoch 0, Batch 80, train loss:2.5511584281921387, Elapsed time for epoch : 0.9439249952634176
Epoch 0, Batch 90, train loss:2.534867763519287, Elapsed time for epoch : 1.0600261290868123
Epoch 0, Batch 100, train loss:2.195695400238037, Elapsed time for epoch : 1.176361870765686
Epoch 0, Batch 110, train loss:1.91389000415802, Elapsed time for epoch : 1.292622454961141
Batch 0, val loss:6.808574676513672
Batch 10, val loss:7.134714603424072
Batch 20, val loss:3.8598008155822754
Batch 30, val loss:7.128749847412109
Epoch 0, Train Loss:3.2517762360365494, Val loss:4.491629593902164
Epoch 1, Batch 0, train loss:1.9461984634399414, Elapsed time for epoch : 0.01167533795038859
Epoch 1, Batch 10, train loss:1.8205089569091797, Elapsed time for epoch : 0.128753395875295
Epoch 1, Batch 20, train loss:1.4860020875930786, Elapsed time for epoch : 0.24576410055160522
Epoch 1, Batch 30, train loss:1.609519362449646, Elapsed time for epoch : 0.3621579011281331
Epoch 1, Batch 40, train loss:1.7044317722320557, Elapsed time for epoch : 0.47864070733388264
Epoch 1, Batch 50, train loss:1.5855234861373901, Elapsed time for epoch : 0.595160988966624
Epoch 1, Batch 60, train loss:1.5392639636993408, Elapsed time for epoch : 0.7115386168162028
Epoch 1, Batch 70, train loss:1.7378628253936768, Elapsed time for epoch : 0.8281975110371907
Epoch 1, Batch 80, train loss:1.3756721019744873, Elapsed time for epoch : 0.944488251209259
Epoch 1, Batch 90, train loss:1.309278130531311, Elapsed time for epoch : 1.0606747229894002
Epoch 1, Batch 100, train loss:0.7931307554244995, Elapsed time for epoch : 1.1770582795143127
Epoch 1, Batch 110, train loss:1.0261014699935913, Elapsed time for epoch : 1.2933290680249532
Batch 0, val loss:2.4065041542053223
Batch 10, val loss:2.594111680984497
Batch 20, val loss:3.0769762992858887
Batch 30, val loss:2.633695363998413
Epoch 1, Train Loss:1.4384395988091179, Val loss:3.8756343060069613
Epoch 2, Batch 0, train loss:0.9507532119750977, Elapsed time for epoch : 0.011631910006205242
Epoch 2, Batch 10, train loss:0.9133151173591614, Elapsed time for epoch : 0.1280619978904724
Epoch 2, Batch 20, train loss:0.9675410389900208, Elapsed time for epoch : 0.24431039492289225
Epoch 2, Batch 30, train loss:0.9982491731643677, Elapsed time for epoch : 0.3606931805610657
Epoch 2, Batch 40, train loss:0.8952164053916931, Elapsed time for epoch : 0.47731767892837523
Epoch 2, Batch 50, train loss:1.0990649461746216, Elapsed time for epoch : 0.5938713630040486
Epoch 2, Batch 60, train loss:0.8621593117713928, Elapsed time for epoch : 0.710725708802541
Epoch 2, Batch 70, train loss:0.379912406206131, Elapsed time for epoch : 0.8273133397102356
Epoch 2, Batch 80, train loss:0.7908468842506409, Elapsed time for epoch : 0.9439399679501851
Epoch 2, Batch 90, train loss:0.7981401085853577, Elapsed time for epoch : 1.060331658522288
Epoch 2, Batch 100, train loss:0.9419729113578796, Elapsed time for epoch : 1.1764910658200582
Epoch 2, Batch 110, train loss:0.5831536650657654, Elapsed time for epoch : 1.2931570688883463
Batch 0, val loss:5.503520965576172
Batch 10, val loss:4.2605204582214355
Batch 20, val loss:1.8713349103927612
Batch 30, val loss:2.900514602661133
Epoch 2, Train Loss:0.7871142896621124, Val loss:5.1755169207851095
Epoch 3, Batch 0, train loss:0.6490575075149536, Elapsed time for epoch : 0.011640524864196778
Epoch 3, Batch 10, train loss:0.750257670879364, Elapsed time for epoch : 0.12767861684163412
Epoch 3, Batch 20, train loss:0.7601352334022522, Elapsed time for epoch : 0.24425372282663982
Epoch 3, Batch 30, train loss:0.5802761912345886, Elapsed time for epoch : 0.36103636423746743
Epoch 3, Batch 40, train loss:0.7409505844116211, Elapsed time for epoch : 0.47760976950327555
Epoch 3, Batch 50, train loss:0.20397578179836273, Elapsed time for epoch : 0.5938688317934672
Epoch 3, Batch 60, train loss:0.538579523563385, Elapsed time for epoch : 0.710067069530487
Epoch 3, Batch 70, train loss:0.51080322265625, Elapsed time for epoch : 0.8269647518793742
Epoch 3, Batch 80, train loss:0.5771481394767761, Elapsed time for epoch : 0.9442473173141479
Epoch 3, Batch 90, train loss:0.46415239572525024, Elapsed time for epoch : 1.060941425959269
Epoch 3, Batch 100, train loss:0.09808585792779922, Elapsed time for epoch : 1.177605942885081
Epoch 3, Batch 110, train loss:0.3527704179286957, Elapsed time for epoch : 1.29403954744339
Batch 0, val loss:5.798018932342529
Batch 10, val loss:14.75664234161377
Batch 20, val loss:15.528691291809082
Batch 30, val loss:2.8383238315582275
Epoch 3, Train Loss:0.5010341617076294, Val loss:6.259986804591285
Epoch 4, Batch 0, train loss:0.45740172266960144, Elapsed time for epoch : 0.011644077301025391
Epoch 4, Batch 10, train loss:0.12530829012393951, Elapsed time for epoch : 0.12807486454645792
Epoch 4, Batch 20, train loss:0.07544941455125809, Elapsed time for epoch : 0.24455798069636028
Epoch 4, Batch 30, train loss:0.4435541033744812, Elapsed time for epoch : 0.36070337692896526
Epoch 4, Batch 40, train loss:0.3920346796512604, Elapsed time for epoch : 0.4770480990409851
Epoch 4, Batch 50, train loss:0.3572840392589569, Elapsed time for epoch : 0.5933544278144837
Epoch 4, Batch 60, train loss:0.3137335479259491, Elapsed time for epoch : 0.7098529020945231
Epoch 4, Batch 70, train loss:0.34199070930480957, Elapsed time for epoch : 0.8265977342923482
Epoch 4, Batch 80, train loss:0.10399054735898972, Elapsed time for epoch : 0.9429835081100464
Epoch 4, Batch 90, train loss:0.33058473467826843, Elapsed time for epoch : 1.0594144622484842
Epoch 4, Batch 100, train loss:0.33447375893592834, Elapsed time for epoch : 1.1763216455777485
Epoch 4, Batch 110, train loss:0.36815881729125977, Elapsed time for epoch : 1.2926531394322713
Batch 0, val loss:10.521428108215332
Batch 10, val loss:2.1417243480682373
Batch 20, val loss:6.997398376464844
Batch 30, val loss:1.9540059566497803
Epoch 4, Train Loss:0.2943423909985501, Val loss:5.943444619576137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.157 MB uploadedwandb: | 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29434
wandb:         Val Loss 5.94344
wandb:      train_batch 110
wandb: train_batch_loss 0.36816
wandb:        val_batch 30
wandb:   val_batch_loss 1.95401
wandb: 
wandb: üöÄ View run still-planet-401 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/d4k50o8e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_060806-d4k50o8e/logs
Seed completed execution! 42 0.7_5
------------------------------------------------------------------
Running for seed 89 of experiment 0.7_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_061554-qetbfogo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-deluge-403
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/qetbfogo
Epoch 0, Batch 0, train loss:7.772118091583252, Elapsed time for epoch : 0.013514947891235352
Epoch 0, Batch 10, train loss:4.110156059265137, Elapsed time for epoch : 0.12979021072387695
Epoch 0, Batch 20, train loss:3.5861294269561768, Elapsed time for epoch : 0.24592761596043905
Epoch 0, Batch 30, train loss:3.3507461547851562, Elapsed time for epoch : 0.36239120960235593
Epoch 0, Batch 40, train loss:3.1592957973480225, Elapsed time for epoch : 0.47895820140838624
Epoch 0, Batch 50, train loss:2.8426802158355713, Elapsed time for epoch : 0.5952184160550436
Epoch 0, Batch 60, train loss:2.7208786010742188, Elapsed time for epoch : 0.7114770650863648
Epoch 0, Batch 70, train loss:2.7141194343566895, Elapsed time for epoch : 0.8277747790018718
Epoch 0, Batch 80, train loss:2.5511584281921387, Elapsed time for epoch : 0.9441309134165446
Epoch 0, Batch 90, train loss:2.534867763519287, Elapsed time for epoch : 1.0605435689290366
Epoch 0, Batch 100, train loss:2.195695400238037, Elapsed time for epoch : 1.1769683996836344
Epoch 0, Batch 110, train loss:1.91389000415802, Elapsed time for epoch : 1.2934573849042257
Batch 0, val loss:6.808574676513672
Batch 10, val loss:7.134714603424072
Batch 20, val loss:3.8598008155822754
Batch 30, val loss:7.128749847412109
Epoch 0, Train Loss:3.2517762360365494, Val loss:4.491629593902164
Epoch 1, Batch 0, train loss:1.9461984634399414, Elapsed time for epoch : 0.011610865592956543
Epoch 1, Batch 10, train loss:1.8205089569091797, Elapsed time for epoch : 0.12818921407063802
Epoch 1, Batch 20, train loss:1.4860020875930786, Elapsed time for epoch : 0.2445316712061564
Epoch 1, Batch 30, train loss:1.609519362449646, Elapsed time for epoch : 0.36139258941014607
Epoch 1, Batch 40, train loss:1.7044317722320557, Elapsed time for epoch : 0.4776676615079244
Epoch 1, Batch 50, train loss:1.5855234861373901, Elapsed time for epoch : 0.5940242369969686
Epoch 1, Batch 60, train loss:1.5392639636993408, Elapsed time for epoch : 0.7105579177538554
Epoch 1, Batch 70, train loss:1.7378628253936768, Elapsed time for epoch : 0.8273552179336547
Epoch 1, Batch 80, train loss:1.3756721019744873, Elapsed time for epoch : 0.9441553711891174
Epoch 1, Batch 90, train loss:1.309278130531311, Elapsed time for epoch : 1.0606615622838338
Epoch 1, Batch 100, train loss:0.7931307554244995, Elapsed time for epoch : 1.1771154483159383
Epoch 1, Batch 110, train loss:1.0261014699935913, Elapsed time for epoch : 1.2933427691459656
Batch 0, val loss:2.4065041542053223
Batch 10, val loss:2.594111680984497
Batch 20, val loss:3.0769762992858887
Batch 30, val loss:2.633695363998413
Epoch 1, Train Loss:1.4384395988091179, Val loss:3.8756343060069613
Epoch 2, Batch 0, train loss:0.9507532119750977, Elapsed time for epoch : 0.011661152044932047
Epoch 2, Batch 10, train loss:0.9133151173591614, Elapsed time for epoch : 0.12784578402837118
Epoch 2, Batch 20, train loss:0.9675410389900208, Elapsed time for epoch : 0.2441974679629008
Epoch 2, Batch 30, train loss:0.9982491731643677, Elapsed time for epoch : 0.3606142441431681
Epoch 2, Batch 40, train loss:0.8952164053916931, Elapsed time for epoch : 0.4772526383399963
Epoch 2, Batch 50, train loss:1.0990649461746216, Elapsed time for epoch : 0.5938174247741699
Epoch 2, Batch 60, train loss:0.8621593117713928, Elapsed time for epoch : 0.710523267587026
Epoch 2, Batch 70, train loss:0.379912406206131, Elapsed time for epoch : 0.8270522673924764
Epoch 2, Batch 80, train loss:0.7908468842506409, Elapsed time for epoch : 0.9433163444201151
Epoch 2, Batch 90, train loss:0.7981401085853577, Elapsed time for epoch : 1.0601125915845235
Epoch 2, Batch 100, train loss:0.9419729113578796, Elapsed time for epoch : 1.1767267783482869
Epoch 2, Batch 110, train loss:0.5831536650657654, Elapsed time for epoch : 1.293470553557078
Batch 0, val loss:5.503520965576172
Batch 10, val loss:4.2605204582214355
Batch 20, val loss:1.8713349103927612
Batch 30, val loss:2.900514602661133
Epoch 2, Train Loss:0.7871142896621124, Val loss:5.1755169207851095
Epoch 3, Batch 0, train loss:0.6490575075149536, Elapsed time for epoch : 0.011688204606374104
Epoch 3, Batch 10, train loss:0.750257670879364, Elapsed time for epoch : 0.12803054650624593
Epoch 3, Batch 20, train loss:0.7601352334022522, Elapsed time for epoch : 0.2444261431694031
Epoch 3, Batch 30, train loss:0.5802761912345886, Elapsed time for epoch : 0.3607849438985189
Epoch 3, Batch 40, train loss:0.7409505844116211, Elapsed time for epoch : 0.4772576014200846
Epoch 3, Batch 50, train loss:0.20397578179836273, Elapsed time for epoch : 0.593670646349589
Epoch 3, Batch 60, train loss:0.538579523563385, Elapsed time for epoch : 0.7104159077008565
Epoch 3, Batch 70, train loss:0.51080322265625, Elapsed time for epoch : 0.8271085778872173
Epoch 3, Batch 80, train loss:0.5771481394767761, Elapsed time for epoch : 0.9439541339874268
Epoch 3, Batch 90, train loss:0.46415239572525024, Elapsed time for epoch : 1.0605931202570598
Epoch 3, Batch 100, train loss:0.09808585792779922, Elapsed time for epoch : 1.1777495861053466
Epoch 3, Batch 110, train loss:0.3527704179286957, Elapsed time for epoch : 1.2948627948760987
Batch 0, val loss:5.798018932342529
Batch 10, val loss:14.75664234161377
Batch 20, val loss:15.528691291809082
Batch 30, val loss:2.8383238315582275
Epoch 3, Train Loss:0.5010341617076294, Val loss:6.259986804591285
Epoch 4, Batch 0, train loss:0.45740172266960144, Elapsed time for epoch : 0.011670589447021484
Epoch 4, Batch 10, train loss:0.12530829012393951, Elapsed time for epoch : 0.12784260908762615
Epoch 4, Batch 20, train loss:0.07544941455125809, Elapsed time for epoch : 0.24463460048039753
Epoch 4, Batch 30, train loss:0.4435541033744812, Elapsed time for epoch : 0.36112955808639524
Epoch 4, Batch 40, train loss:0.3920346796512604, Elapsed time for epoch : 0.47747599681218467
Epoch 4, Batch 50, train loss:0.3572840392589569, Elapsed time for epoch : 0.5942453304926555
Epoch 4, Batch 60, train loss:0.3137335479259491, Elapsed time for epoch : 0.7103379964828491
Epoch 4, Batch 70, train loss:0.34199070930480957, Elapsed time for epoch : 0.8270300467809041
Epoch 4, Batch 80, train loss:0.10399054735898972, Elapsed time for epoch : 0.943932036558787
Epoch 4, Batch 90, train loss:0.33058473467826843, Elapsed time for epoch : 1.060176682472229
Epoch 4, Batch 100, train loss:0.33447375893592834, Elapsed time for epoch : 1.1767585595448813
Epoch 4, Batch 110, train loss:0.36815881729125977, Elapsed time for epoch : 1.2937377174695333
Batch 0, val loss:10.521428108215332
Batch 10, val loss:2.1417243480682373
Batch 20, val loss:6.997398376464844
Batch 30, val loss:1.9540059566497803
Epoch 4, Train Loss:0.2943423909985501, Val loss:5.943444619576137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29434
wandb:         Val Loss 5.94344
wandb:      train_batch 110
wandb: train_batch_loss 0.36816
wandb:        val_batch 30
wandb:   val_batch_loss 1.95401
wandb: 
wandb: üöÄ View run winter-deluge-403 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/qetbfogo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_061554-qetbfogo/logs
Seed completed execution! 89 0.7_5
------------------------------------------------------------------
Running for seed 23 of experiment 0.7_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_062342-l0xci4v7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sun-405
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/l0xci4v7
Epoch 0, Batch 0, train loss:7.772118091583252, Elapsed time for epoch : 0.01328871250152588
Epoch 0, Batch 10, train loss:4.110156059265137, Elapsed time for epoch : 0.1292404294013977
Epoch 0, Batch 20, train loss:3.5861294269561768, Elapsed time for epoch : 0.24524413347244262
Epoch 0, Batch 30, train loss:3.3507461547851562, Elapsed time for epoch : 0.3614414095878601
Epoch 0, Batch 40, train loss:3.1592957973480225, Elapsed time for epoch : 0.4777231494585673
Epoch 0, Batch 50, train loss:2.8426802158355713, Elapsed time for epoch : 0.5941643834114074
Epoch 0, Batch 60, train loss:2.7208786010742188, Elapsed time for epoch : 0.7106810212135315
Epoch 0, Batch 70, train loss:2.7141194343566895, Elapsed time for epoch : 0.827046259244283
Epoch 0, Batch 80, train loss:2.5511584281921387, Elapsed time for epoch : 0.9440649191538493
Epoch 0, Batch 90, train loss:2.534867763519287, Elapsed time for epoch : 1.0604509751001994
Epoch 0, Batch 100, train loss:2.195695400238037, Elapsed time for epoch : 1.1770926316579182
Epoch 0, Batch 110, train loss:1.91389000415802, Elapsed time for epoch : 1.2936732014020285
Batch 0, val loss:6.808574676513672
Batch 10, val loss:7.134714603424072
Batch 20, val loss:3.8598008155822754
Batch 30, val loss:7.128749847412109
Epoch 0, Train Loss:3.2517762360365494, Val loss:4.491629593902164
Epoch 1, Batch 0, train loss:1.9461984634399414, Elapsed time for epoch : 0.011735085646311443
Epoch 1, Batch 10, train loss:1.8205089569091797, Elapsed time for epoch : 0.1280814528465271
Epoch 1, Batch 20, train loss:1.4860020875930786, Elapsed time for epoch : 0.2447527050971985
Epoch 1, Batch 30, train loss:1.609519362449646, Elapsed time for epoch : 0.3614258885383606
Epoch 1, Batch 40, train loss:1.7044317722320557, Elapsed time for epoch : 0.47761168479919436
Epoch 1, Batch 50, train loss:1.5855234861373901, Elapsed time for epoch : 0.5939740260442098
Epoch 1, Batch 60, train loss:1.5392639636993408, Elapsed time for epoch : 0.7102993408838908
Epoch 1, Batch 70, train loss:1.7378628253936768, Elapsed time for epoch : 0.8265760143597921
Epoch 1, Batch 80, train loss:1.3756721019744873, Elapsed time for epoch : 0.9433395783106486
Epoch 1, Batch 90, train loss:1.309278130531311, Elapsed time for epoch : 1.0597238103548685
Epoch 1, Batch 100, train loss:0.7931307554244995, Elapsed time for epoch : 1.17598907550176
Epoch 1, Batch 110, train loss:1.0261014699935913, Elapsed time for epoch : 1.2926288644472759
Batch 0, val loss:2.4065041542053223
Batch 10, val loss:2.594111680984497
Batch 20, val loss:3.0769762992858887
Batch 30, val loss:2.633695363998413
Epoch 1, Train Loss:1.4384395988091179, Val loss:3.8756343060069613
Epoch 2, Batch 0, train loss:0.9507532119750977, Elapsed time for epoch : 0.011666945616404216
Epoch 2, Batch 10, train loss:0.9133151173591614, Elapsed time for epoch : 0.12828108072280883
Epoch 2, Batch 20, train loss:0.9675410389900208, Elapsed time for epoch : 0.24468230009078978
Epoch 2, Batch 30, train loss:0.9982491731643677, Elapsed time for epoch : 0.3613745093345642
Epoch 2, Batch 40, train loss:0.8952164053916931, Elapsed time for epoch : 0.4783983747164408
Epoch 2, Batch 50, train loss:1.0990649461746216, Elapsed time for epoch : 0.5952422817548116
Epoch 2, Batch 60, train loss:0.8621593117713928, Elapsed time for epoch : 0.712178361415863
Epoch 2, Batch 70, train loss:0.379912406206131, Elapsed time for epoch : 0.8288781682650248
Epoch 2, Batch 80, train loss:0.7908468842506409, Elapsed time for epoch : 0.9453251004219055
Epoch 2, Batch 90, train loss:0.7981401085853577, Elapsed time for epoch : 1.0618670463562012
Epoch 2, Batch 100, train loss:0.9419729113578796, Elapsed time for epoch : 1.1785888512929281
Epoch 2, Batch 110, train loss:0.5831536650657654, Elapsed time for epoch : 1.295107122262319
Batch 0, val loss:5.503520965576172
Batch 10, val loss:4.2605204582214355
Batch 20, val loss:1.8713349103927612
Batch 30, val loss:2.900514602661133
Epoch 2, Train Loss:0.7871142896621124, Val loss:5.1755169207851095
Epoch 3, Batch 0, train loss:0.6490575075149536, Elapsed time for epoch : 0.011639118194580078
Epoch 3, Batch 10, train loss:0.750257670879364, Elapsed time for epoch : 0.12854820092519123
Epoch 3, Batch 20, train loss:0.7601352334022522, Elapsed time for epoch : 0.24550209840138754
Epoch 3, Batch 30, train loss:0.5802761912345886, Elapsed time for epoch : 0.3623883684476217
Epoch 3, Batch 40, train loss:0.7409505844116211, Elapsed time for epoch : 0.47871190706888833
Epoch 3, Batch 50, train loss:0.20397578179836273, Elapsed time for epoch : 0.5948562860488892
Epoch 3, Batch 60, train loss:0.538579523563385, Elapsed time for epoch : 0.7114262739817302
Epoch 3, Batch 70, train loss:0.51080322265625, Elapsed time for epoch : 0.8283255100250244
Epoch 3, Batch 80, train loss:0.5771481394767761, Elapsed time for epoch : 0.9446196913719177
Epoch 3, Batch 90, train loss:0.46415239572525024, Elapsed time for epoch : 1.0614961703618369
Epoch 3, Batch 100, train loss:0.09808585792779922, Elapsed time for epoch : 1.1779643058776856
Epoch 3, Batch 110, train loss:0.3527704179286957, Elapsed time for epoch : 1.2945844650268554
Batch 0, val loss:5.798018932342529
Batch 10, val loss:14.75664234161377
Batch 20, val loss:15.528691291809082
Batch 30, val loss:2.8383238315582275
Epoch 3, Train Loss:0.5010341617076294, Val loss:6.259986804591285
Epoch 4, Batch 0, train loss:0.45740172266960144, Elapsed time for epoch : 0.011660949389139811
Epoch 4, Batch 10, train loss:0.12530829012393951, Elapsed time for epoch : 0.12802255153656006
Epoch 4, Batch 20, train loss:0.07544941455125809, Elapsed time for epoch : 0.2445688764254252
Epoch 4, Batch 30, train loss:0.4435541033744812, Elapsed time for epoch : 0.36118423541386924
Epoch 4, Batch 40, train loss:0.3920346796512604, Elapsed time for epoch : 0.4774432937304179
Epoch 4, Batch 50, train loss:0.3572840392589569, Elapsed time for epoch : 0.5943669438362121
Epoch 4, Batch 60, train loss:0.3137335479259491, Elapsed time for epoch : 0.7108947316805522
Epoch 4, Batch 70, train loss:0.34199070930480957, Elapsed time for epoch : 0.8275838017463684
Epoch 4, Batch 80, train loss:0.10399054735898972, Elapsed time for epoch : 0.9446379025777181
Epoch 4, Batch 90, train loss:0.33058473467826843, Elapsed time for epoch : 1.062074367205302
Epoch 4, Batch 100, train loss:0.33447375893592834, Elapsed time for epoch : 1.1786534110705058
Epoch 4, Batch 110, train loss:0.36815881729125977, Elapsed time for epoch : 1.2952637871106465
Batch 0, val loss:10.521428108215332
Batch 10, val loss:2.1417243480682373
Batch 20, val loss:6.997398376464844
Batch 30, val loss:1.9540059566497803
Epoch 4, Train Loss:0.2943423909985501, Val loss:5.943444619576137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29434
wandb:         Val Loss 5.94344
wandb:      train_batch 110
wandb: train_batch_loss 0.36816
wandb:        val_batch 30
wandb:   val_batch_loss 1.95401
wandb: 
wandb: üöÄ View run generous-sun-405 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/l0xci4v7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_062342-l0xci4v7/logs
Seed completed execution! 23 0.7_5
------------------------------------------------------------------
Running for seed 113 of experiment 0.7_5
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /oscar/scratch/vsharm44/rl_project/Exploration-For-Shortcut-Learning/contrastive_learning/wandb/run-20240511_063131-hlnjtz8w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-night-407
wandb: ‚≠êÔ∏è View project at https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: üöÄ View run at https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hlnjtz8w
Epoch 0, Batch 0, train loss:7.772118091583252, Elapsed time for epoch : 0.013394689559936524
Epoch 0, Batch 10, train loss:4.110156059265137, Elapsed time for epoch : 0.12989964882532756
Epoch 0, Batch 20, train loss:3.5861294269561768, Elapsed time for epoch : 0.2473223884900411
Epoch 0, Batch 30, train loss:3.3507461547851562, Elapsed time for epoch : 0.36385061343510944
Epoch 0, Batch 40, train loss:3.1592957973480225, Elapsed time for epoch : 0.48199854294459027
Epoch 0, Batch 50, train loss:2.8426802158355713, Elapsed time for epoch : 0.5991676648457845
Epoch 0, Batch 60, train loss:2.7208786010742188, Elapsed time for epoch : 0.716066054503123
Epoch 0, Batch 70, train loss:2.7141194343566895, Elapsed time for epoch : 0.83343106508255
Epoch 0, Batch 80, train loss:2.5511584281921387, Elapsed time for epoch : 0.9506978631019593
Epoch 0, Batch 90, train loss:2.534867763519287, Elapsed time for epoch : 1.0686285654703775
Epoch 0, Batch 100, train loss:2.195695400238037, Elapsed time for epoch : 1.1860390583674112
Epoch 0, Batch 110, train loss:1.91389000415802, Elapsed time for epoch : 1.3025871515274048
Batch 0, val loss:6.808574676513672
Batch 10, val loss:7.134714603424072
Batch 20, val loss:3.8598008155822754
Batch 30, val loss:7.128749847412109
Epoch 0, Train Loss:3.2517762360365494, Val loss:4.491629593902164
Epoch 1, Batch 0, train loss:1.9461984634399414, Elapsed time for epoch : 0.011655461788177491
Epoch 1, Batch 10, train loss:1.8205089569091797, Elapsed time for epoch : 0.1283216635386149
Epoch 1, Batch 20, train loss:1.4860020875930786, Elapsed time for epoch : 0.2444293737411499
Epoch 1, Batch 30, train loss:1.609519362449646, Elapsed time for epoch : 0.3609549641609192
Epoch 1, Batch 40, train loss:1.7044317722320557, Elapsed time for epoch : 0.47747867902119956
Epoch 1, Batch 50, train loss:1.5855234861373901, Elapsed time for epoch : 0.5937406460444132
Epoch 1, Batch 60, train loss:1.5392639636993408, Elapsed time for epoch : 0.7099948525428772
Epoch 1, Batch 70, train loss:1.7378628253936768, Elapsed time for epoch : 0.8266191720962525
Epoch 1, Batch 80, train loss:1.3756721019744873, Elapsed time for epoch : 0.9430404345194499
Epoch 1, Batch 90, train loss:1.309278130531311, Elapsed time for epoch : 1.0592568635940551
Epoch 1, Batch 100, train loss:0.7931307554244995, Elapsed time for epoch : 1.1753871043523152
Epoch 1, Batch 110, train loss:1.0261014699935913, Elapsed time for epoch : 1.2919523437817892
Batch 0, val loss:2.4065041542053223
Batch 10, val loss:2.594111680984497
Batch 20, val loss:3.0769762992858887
Batch 30, val loss:2.633695363998413
Epoch 1, Train Loss:1.4384395988091179, Val loss:3.8756343060069613
Epoch 2, Batch 0, train loss:0.9507532119750977, Elapsed time for epoch : 0.011656320095062256
Epoch 2, Batch 10, train loss:0.9133151173591614, Elapsed time for epoch : 0.12814882198969524
Epoch 2, Batch 20, train loss:0.9675410389900208, Elapsed time for epoch : 0.244452699025472
Epoch 2, Batch 30, train loss:0.9982491731643677, Elapsed time for epoch : 0.3605482180913289
Epoch 2, Batch 40, train loss:0.8952164053916931, Elapsed time for epoch : 0.4770700772603353
Epoch 2, Batch 50, train loss:1.0990649461746216, Elapsed time for epoch : 0.5930960456530253
Epoch 2, Batch 60, train loss:0.8621593117713928, Elapsed time for epoch : 0.7091359376907349
Epoch 2, Batch 70, train loss:0.379912406206131, Elapsed time for epoch : 0.8250788489977519
Epoch 2, Batch 80, train loss:0.7908468842506409, Elapsed time for epoch : 0.9415682435035706
Epoch 2, Batch 90, train loss:0.7981401085853577, Elapsed time for epoch : 1.0574730555216472
Epoch 2, Batch 100, train loss:0.9419729113578796, Elapsed time for epoch : 1.173934030532837
Epoch 2, Batch 110, train loss:0.5831536650657654, Elapsed time for epoch : 1.290218190352122
Batch 0, val loss:5.503520965576172
Batch 10, val loss:4.2605204582214355
Batch 20, val loss:1.8713349103927612
Batch 30, val loss:2.900514602661133
Epoch 2, Train Loss:0.7871142896621124, Val loss:5.1755169207851095
Epoch 3, Batch 0, train loss:0.6490575075149536, Elapsed time for epoch : 0.01184924046198527
Epoch 3, Batch 10, train loss:0.750257670879364, Elapsed time for epoch : 0.12778226534525552
Epoch 3, Batch 20, train loss:0.7601352334022522, Elapsed time for epoch : 0.24412428538004557
Epoch 3, Batch 30, train loss:0.5802761912345886, Elapsed time for epoch : 0.36047540108362836
Epoch 3, Batch 40, train loss:0.7409505844116211, Elapsed time for epoch : 0.47698566516240437
Epoch 3, Batch 50, train loss:0.20397578179836273, Elapsed time for epoch : 0.593033766746521
Epoch 3, Batch 60, train loss:0.538579523563385, Elapsed time for epoch : 0.7097310145696004
Epoch 3, Batch 70, train loss:0.51080322265625, Elapsed time for epoch : 0.8265178839365641
Epoch 3, Batch 80, train loss:0.5771481394767761, Elapsed time for epoch : 0.9432708064715067
Epoch 3, Batch 90, train loss:0.46415239572525024, Elapsed time for epoch : 1.0598870992660523
Epoch 3, Batch 100, train loss:0.09808585792779922, Elapsed time for epoch : 1.176180076599121
Epoch 3, Batch 110, train loss:0.3527704179286957, Elapsed time for epoch : 1.2928876876831055
Batch 0, val loss:5.798018932342529
Batch 10, val loss:14.75664234161377
Batch 20, val loss:15.528691291809082
Batch 30, val loss:2.8383238315582275
Epoch 3, Train Loss:0.5010341617076294, Val loss:6.259986804591285
Epoch 4, Batch 0, train loss:0.45740172266960144, Elapsed time for epoch : 0.011620775858561198
Epoch 4, Batch 10, train loss:0.12530829012393951, Elapsed time for epoch : 0.12800892194112143
Epoch 4, Batch 20, train loss:0.07544941455125809, Elapsed time for epoch : 0.24451053539911907
Epoch 4, Batch 30, train loss:0.4435541033744812, Elapsed time for epoch : 0.3608416676521301
Epoch 4, Batch 40, train loss:0.3920346796512604, Elapsed time for epoch : 0.47734358310699465
Epoch 4, Batch 50, train loss:0.3572840392589569, Elapsed time for epoch : 0.5939407269159953
Epoch 4, Batch 60, train loss:0.3137335479259491, Elapsed time for epoch : 0.7098705689112346
Epoch 4, Batch 70, train loss:0.34199070930480957, Elapsed time for epoch : 0.8262897928555807
Epoch 4, Batch 80, train loss:0.10399054735898972, Elapsed time for epoch : 0.942744787534078
Epoch 4, Batch 90, train loss:0.33058473467826843, Elapsed time for epoch : 1.0590641379356385
Epoch 4, Batch 100, train loss:0.33447375893592834, Elapsed time for epoch : 1.1755652189254762
Epoch 4, Batch 110, train loss:0.36815881729125977, Elapsed time for epoch : 1.291943860054016
Batch 0, val loss:10.521428108215332
Batch 10, val loss:2.1417243480682373
Batch 20, val loss:6.997398376464844
Batch 30, val loss:1.9540059566497803
Epoch 4, Train Loss:0.2943423909985501, Val loss:5.943444619576137
wandb: - 0.157 MB of 0.157 MB uploadedwandb: \ 0.157 MB of 0.171 MB uploadedwandb: | 0.157 MB of 0.171 MB uploadedwandb: / 0.171 MB of 0.171 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Epoch ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà
wandb:       Train Loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ
wandb:         Val Loss ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñá
wandb:      train_batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà
wandb: train_batch_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        val_batch ‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:   val_batch_loss ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÖ‚ñÅ‚ñÑ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            Epoch 4
wandb:       Train Loss 0.29434
wandb:         Val Loss 5.94344
wandb:      train_batch 110
wandb: train_batch_loss 0.36816
wandb:        val_batch 30
wandb:   val_batch_loss 1.95401
wandb: 
wandb: üöÄ View run unique-night-407 at: https://wandb.ai/vipul/RL_Project_CSCI2951F/runs/hlnjtz8w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/vipul/RL_Project_CSCI2951F
wandb: Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20240511_063131-hlnjtz8w/logs
Seed completed execution! 113 0.7_5
------------------------------------------------------------------
Experiment complete 0.7_5
