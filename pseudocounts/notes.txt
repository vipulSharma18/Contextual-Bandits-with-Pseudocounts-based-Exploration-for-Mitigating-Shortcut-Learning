Coin Flipping Network (Supervised Learning): 
State's visitation count: 
everytime a state visited -> Rademacher trial, 
count = sample of the distribution using these trials.
CFN simply does average of the sampling distribution. 

\beta = intrinsic reward (state, action) -> intrinsince reward
reward_prime = reward + \lambda (Intrinsic reward (state, action))

We consider only bonuses conditioned on states, not on action. 
So intrinsinc reward (state) -> intrinsic reward
\beta = 1/(N(s))**0.5
\beta count based exploration bonus = 1/(pseudocount)**0.5
pseudocount = 1/\beta**2

CFN -> argmin Loss(state, label) -> estimates the bonus directly, 
not the pseudocounts
Loss is simply MSE between random vector of d dim and predictions of the model.
regression problem.
Bonus = sqrt(1/num_coins_d * L2_norm(model**2))

label is crucial: Just a d dimensional random vector of -1 or 1
d = num of coins.
even if state repeated in batch -> different random vector, or different label.

average across coins -> square -> divide by total number of coins


Bandit's reward will be (reward + intrinsic bonus)/probability(action | context)

