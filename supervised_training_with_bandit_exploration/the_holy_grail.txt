Combine supervised learning with contrastive self-supervised trained world model and bandits which explore using intrinsic 
rewards which are calculated using psuedocounts as given by coin flipping networks. 

A. Self-supervised/Unsupervised Contrastive Trained "World Model". Technically not a world model in RL sense
as there's no sequential actions. It's more of a hierarchy of the states: 
1. state abstraction for exploration complexity reduction
2. context generation for multi-armed bandits to make better decision
3. state and action space discretizer for reducing dimensionality & complexity of bandits and exploration

B. Bandits/Actor: 
1. Use MLP with ReLU to predict the extrinsic reward given the action of selecting that a particular patch in the image 
based on the context generated by A is "meaningful" or useful for a class label. 
2. Returns a simple probability of taking it's action or not.

C. Exploration/Intrinsic Reward/CFN: 
1. Add an exploration bonus using exponentially decaying exploration factor by using a coin flipping network. 

D. Supervised Learning/Critic: 
1. The parent/global teaching signal generator. This model takes the actions and exploration done by the bandits 
and evaluates/critiques it. 